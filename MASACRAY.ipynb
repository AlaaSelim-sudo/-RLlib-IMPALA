{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ray[rllib] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code prints out the path of the Python executable\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# It might be useful to know which Python version you're using as well\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install py_dss_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea49dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MultiAgentEnvCompatibility(original_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dace131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 16:27:04,806\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-11-12 16:27:06,896\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-11-12 16:27:06,904\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n",
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Testing with partial observability of voltage violatiosn for each agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Tuple, Dict\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.sac import SAC\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "from py_dss_interface import DSSDLL\n",
    "#import stable_baselines3\n",
    "#from stable_baselines3 import SAC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from stable_baselines3 import A2C, DQN, PPO, TD3, SAC\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenDSS\n",
    "dss = DSSDLL(r\"C:\\Program Files\\OpenDSS\")\n",
    "dss_file = r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\"\n",
    "dss.text(f\"compile [{dss_file}]\")\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "TRANSFORMER_TAPS_ACTION_LOW = 0.9\n",
    "TRANSFORMER_TAPS_ACTION_HIGH = 1.1\n",
    "\n",
    "CAPACITOR_ACTION_LOW = 0\n",
    "CAPACITOR_ACTION_HIGH = 1\n",
    "\n",
    "\n",
    "class PowerSystemEnv(MultiAgentEnv):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(PowerSystemEnv, self).__init__()\n",
    "\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses  = [\n",
    "                                '1', '7', '8', '13', '21', '23', '29', '250', '35', '40', '42', '55',\n",
    "                                '56', '65', '76', '78', '66', '79', '81', '83', '91', '95', '100',\n",
    "                                '197', '300', '110', '135', '160', '152', '610'\n",
    "                            ]\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "        self.previous_reward = 0.0\n",
    "        self.alpha = 0.1  #\n",
    "         # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=100 Pmpp=80\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=100 kVAR=0\")\n",
    "        \n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "    \n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)  # Skip the header\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "            \n",
    "            \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array(\n",
    "                [PV_KVAR_ACTION_LOW]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_LOW]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_LOW]*BESS_KW_ACTION_LEN \n",
    "            ),\n",
    "            high=np.array(\n",
    "                [PV_KVAR_ACTION_HIGH]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_HIGH]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_HIGH]*BESS_KW_ACTION_LEN \n",
    "               \n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "            \n",
    "\n",
    "     \n",
    "        # Assuming observation space is the voltage at each bus\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(278,), dtype=np.float32)  # Modified shape\n",
    "            \n",
    "         \n",
    "        # Update the action and observation spaces for each agent\n",
    "        self.action_space_dict = {\n",
    "            f\"agent_{i}\": spaces.Box(\n",
    "                low=np.concatenate(([PV_KW_ACTION_LOW] * 6, [PV_KVAR_ACTION_LOW] * 6, [BESS_KW_ACTION_LOW] * 6)),\n",
    "                high=np.concatenate(([PV_KW_ACTION_HIGH] * 6, [PV_KVAR_ACTION_HIGH] * 6, [BESS_KW_ACTION_HIGH] * 6)),\n",
    "                shape=(18,), dtype=np.float32) for i in range(5)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        self.observation_space_dict = {}\n",
    "\n",
    "        for i in range(4):  # For the first four agents\n",
    "            self.observation_space_dict[f'agent_{i}'] = spaces.Box(low=0, high=2, shape=(50,), dtype=np.float32)\n",
    "\n",
    "        # For the last agent\n",
    "        self.observation_space_dict['agent_4'] = spaces.Box(low=0, high=2, shape=(78,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        self.current_step = 0\n",
    "\n",
    "        \n",
    "         # Initialize control step counter\n",
    "        self.control_steps = 0\n",
    "\n",
    "        # Maximum control steps allowed in one episode\n",
    "        self.max_control_steps =int(2)  # for example\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "          # Define the agent IDs\n",
    "        self._agent_ids = [f'agent_{i}' for i in range(5)]\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "        \n",
    "    def _take_action(self, action_dict):\n",
    "    # Define total actions per agent\n",
    "        TOTAL_ACTIONS_PER_AGENT = 6 + 6 + 6\n",
    "\n",
    "    # Iterate through the action_dict\n",
    "        for agent_id, action in action_dict.items():\n",
    "            agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "        # Determine action segments for this agent\n",
    "            pv_kw_actions = action[:6]\n",
    "            pv_kvar_actions = action[6:12]\n",
    "            bess_kw_actions = action[12:18]\n",
    "\n",
    "        # Index offset based on agent_idx for PV and battery control\n",
    "            pv_idx_offset = agent_idx * 6\n",
    "            battery_idx_offset = agent_idx * 6\n",
    "\n",
    "        # Actions for PVs\n",
    "            for idx, (kw_action_value, kvar_action_value) in enumerate(zip(pv_kw_actions, pv_kvar_actions)):\n",
    "            # handle kW actions\n",
    "                irradiance = self.irradiance_profile[(self.current_step) % 8760]\n",
    "                scaled_pv_kw = kw_action_value * irradiance\n",
    "                if scaled_pv_kw > irradiance * self.KWrated:  # clip to max\n",
    "                    scaled_pv_kw = irradiance * self.KWrated\n",
    "\n",
    "            # handle kVAR actions\n",
    "                pv_kvar = kvar_action_value\n",
    "                S_max = 100  # Maximum apparent power (example value)\n",
    "                q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "                pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "                self.controller.text(f\"edit PVSystem.PV{idx + pv_idx_offset + 1} phases=3 kV=4.16 Pmpp={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "\n",
    "            for idx, action_value in enumerate(bess_kw_actions):\n",
    "                self.controller.text(f\"edit Storage.Battery{idx + battery_idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action_dict):\n",
    "    # Execute the action\n",
    "        self._take_action(action_dict)\n",
    "\n",
    "    # Calculate the rewards\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages if v <= 0.95 or v >= 1.05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate the sum of squared voltage deviations from 1. This penalizes larger deviations more heavily.\n",
    "        voltage_deviations = sum((v - 1)**2 for v in all_bus_voltages)\n",
    "\n",
    "\n",
    "\n",
    "        # Define penalty weights for different components\n",
    "        w_deviation = 1.0\n",
    "        w_violation = 5.0\n",
    "        w_loss = 0.001\n",
    "\n",
    "        # Combine losses, violations, and deviations in the reward\n",
    "        #reward = - w_deviation * voltage_deviations - w_violation * voltage_violations\n",
    "        reward= - voltage_violations \n",
    "        \n",
    "        #-  self.control_steps\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Gather observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # Set the reward for all agents\n",
    "        rewards = {agent_id: reward for agent_id in self._agent_ids}\n",
    "        print(rewards)\n",
    "        print(reward)\n",
    "\n",
    "    # Set termination status for all agents\n",
    "        terminations = {agent_id: (self.control_steps >= self.max_control_steps) or (voltage_violations == 0) for agent_id in self._agent_ids}\n",
    "        terminations['__all__'] = any(terminations.values())\n",
    "\n",
    "    # Assuming no truncation; modify as needed\n",
    "        truncateds = {agent_id: False for agent_id in self._agent_ids}\n",
    "        truncateds['__all__'] = any(truncateds.values())\n",
    "\n",
    "    # Gather additional information if required\n",
    "        infos = {agent_id: {} for agent_id in self._agent_ids}\n",
    "\n",
    "    # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        return observations, rewards, terminations, truncateds, infos\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "    # If a seed is provided, set the random seed for numpy\n",
    "        if seed is None:\n",
    "            seed = 1\n",
    "\n",
    "    # Now set the random seed for numpy\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # You can use options to customize the reset function if needed\n",
    "    # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        \n",
    "        #self.current_step = int(np.clip(np.random.normal(loc=0, scale=8760), 0, 8760))\n",
    "        self.current_step = np.random.randint(0, 8761)\n",
    "       \n",
    "\n",
    " # assuming the profile has 8760 hours  #use gaussian distribution\n",
    "        self.control_steps = 0\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        np.random.seed(seed)  # Set the seed again before generating random numbers\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 8760], scale=0.5, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 10]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 10]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "        \n",
    "        # Get the observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # You can include any additional information here. If there's nothing, just return an empty dictionary.\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self, agent_id):\n",
    "    # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "    # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "\n",
    "    # Extract agent index from the agent_id\n",
    "        agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "    # Determine the start and end index for the observation chunk for this agent\n",
    "        if agent_idx < 4:\n",
    "            start_idx = agent_idx * 50\n",
    "            end_idx = start_idx + 50\n",
    "        else:  # This is agent_4\n",
    "            start_idx = 200\n",
    "            end_idx = 278  # or you could use end_idx = len(all_bus_voltages) if the size is not guaranteed to be 278\n",
    "\n",
    "    # Extract the observation chunk for this agent\n",
    "        agent_observation = all_bus_voltages[start_idx:end_idx]\n",
    "\n",
    "        return agent_observation\n",
    "\n",
    "\n",
    "    def get_agent_observation(self):\n",
    "        agent_obs = {}\n",
    "        for agent_id in self.agents:\n",
    "            agent_obs[agent_id] = self.get_observation()\n",
    "        return agent_obs\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "    \n",
    "\n",
    "irradiance_csv_file = r\"D:\\Alaa_Selim\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "load_profile_file = r\"D:\\Alaa_Selim\\LoadShape1.csv\"\n",
    "\n",
    "    \n",
    "    \n",
    "original_env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "env = MultiAgentEnvCompatibility(original_env)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PowerSystemEnv(\n",
    "                            dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "                            dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "                            irradiance_csv_file=irradiance_csv_file,\n",
    "                            load_profile_file=load_profile_file\n",
    "                            )\n",
    "\n",
    "register_env(\"PowerSystemEnv\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "# Suppose you have N agents\n",
    "N = 5  # adjust this as per your actual number of agents\n",
    "\n",
    "policies = {\n",
    "    f\"policy_{i}\": (None, \n",
    "                    original_env.observation_space_dict[f\"agent_{i}\"], \n",
    "                    original_env.action_space_dict[f\"agent_{i}\"], \n",
    "                    {\"agent_id\": i})  # Set 'agent_id' key here\n",
    "    for i in range(N)\n",
    "}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    if \"agent_\" in agent_id:\n",
    "        # Extract agent number and return the corresponding policy\n",
    "        agent_num = int(agent_id.split(\"_\")[1])\n",
    "        return f\"policy_{agent_num}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23de4681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-11-03 16:45:14</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:09.85        </td></tr>\n",
       "<tr><td>Memory:      </td><td>45.4/127.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 7.0/24 CPUs, 0/1 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>IMPALA_PowerSystemEnv_ddac9_00000</td><td style=\"text-align: right;\">           1</td><td>C:\\Users\\Alaa\\ray_results\\IMPALA\\IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>IMPALA_PowerSystemEnv_ddac9_00000</td><td>ERROR   </td><td>127.0.0.1:20108</td><td style=\"text-align: right;\">1e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=20108)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m 2023-11-03 16:45:08,659\tWARNING algorithm_config.py:656 -- Cannot create ImpalaConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=22968)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m 2023-11-03 16:45:13,918\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <PowerSystemEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m 2023-11-03 16:45:13,918\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <PowerSystemEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m 2023-11-03 16:45:13,918\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <PowerSystemEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m 2023-11-03 16:45:13,918\tWARNING multi_agent_env.py:237 -- action_space_sample() of <PowerSystemEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m 2023-11-03 16:45:13,919\tWARNING multi_agent_env.py:169 -- observation_space_contains() of <PowerSystemEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,883\tWARNING multi_agent_env.py:274 -- observation_space_sample() of <PowerSystemEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,885\tWARNING multi_agent_env.py:199 -- action_space_contains() of <PowerSystemEnv instance> has not been implemented. You can either implement it yourself or bring the observation space into the preferred format of a mapping from agent ids to their individual observation spaces. \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,956\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,957\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,962\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,963\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,967\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,967\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,967\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,968\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,973\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDiagGaussian` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchDiagGaussian` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=36036)\u001b[0m 2023-11-03 16:45:13,973\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m OpenDSS Started successfully! \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m Reset options: {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m {'agent_2': -5, 'agent_0': -5, 'agent_4': -5, 'agent_1': -5, 'agent_3': -5}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=20024)\u001b[0m -5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 16:45:14,169\tERROR tune_controller.py:911 -- Trial task failed for trial IMPALA_PowerSystemEnv_ddac9_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2524, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::Impala.train()\u001b[39m (pid=20108, ip=127.0.0.1, actor_id=015c953a3d4d9c16c7eb358801000000, repr=Impala)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 375, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 372, in train\n",
      "    result = self.step()\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 853, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 2837, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 697, in training_step\n",
      "    unprocessed_sample_batches = self.get_samples_from_workers(\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 907, in get_samples_from_workers\n",
      "    ] = self.workers.fetch_ready_async_reqs(\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 788, in fetch_ready_async_reqs\n",
      "    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 76, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=49024, ip=127.0.0.1, actor_id=54940af9a5a5e45b376fccd501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000024E24650E80>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "    lambda worker: worker.sample(),\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 344, in run\n",
      "    outputs = self.step()\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 370, in step\n",
      "    active_envs, to_eval, outputs = self._process_observations(\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 637, in _process_observations\n",
      "    processed = policy.agent_connectors(acd_list)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "    ret = c(ret)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "    return [self.transform(d) for d in acd_list]\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "    return [self.transform(d) for d in acd_list]\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "    d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 206, in transform\n",
      "    self.check_shape(observation)\n",
      "  File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 74, in check_shape\n",
      "    raise ValueError(\n",
      "ValueError: Observation ([0.99420774 0.9767894  0.9585348  0.9767887  0.9580485  0.9764302\n",
      " 0.9583919  0.99431705 0.9765323  0.957559   0.9763899  0.97603226\n",
      " 0.9557713  0.9584488  0.99434847 0.97618794 0.9586668  0.9943115\n",
      " 0.97580916 0.9586668  0.9943115  0.97580916 0.97593707 0.95814645\n",
      " 0.9920815  0.9785481  0.9570888  0.99202526 0.9572296  0.9913167\n",
      " 0.9782196  0.9561547  0.991639   0.9913267  0.9780807  0.95628923\n",
      " 0.99055856 0.9779442  0.98918927 0.95567274 0.99017245 0.97759974\n",
      " 0.9553952  0.9550749  0.9895914  0.97718364 0.95515263 0.9549791\n",
      " 0.98948956 0.9771054 ] dtype=float32) outside given space (Box(0.0, 2.0, (278,), float32))!\n",
      "2023-11-03 16:45:14,179\tWARNING tune.py:1122 -- Trial Runner checkpointing failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/IMPALA', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\IMPALA'\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m 2023-11-03 16:45:14,122\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.multi_gpu_learner_thread.MultiGPULearnerThread` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m 2023-11-03 16:45:14,123\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.minibatch_buffer.MinibatchBuffer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m 2023-11-03 16:45:14,123\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.learner_thread.LearnerThread` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m 2023-11-03 16:45:14,165\tERROR actor_manager.py:500 -- Ray error, taking actor 3 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=49024, ip=127.0.0.1, actor_id=54940af9a5a5e45b376fccd501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000024E24650E80>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 370, in step\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 637, in _process_observations\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     processed = policy.agent_connectors(acd_list)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     ret = c(ret)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 206, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     self.check_shape(observation)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 74, in check_shape\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m ValueError: Observation ([0.99420774 0.9767894  0.9585348  0.9767887  0.9580485  0.9764302\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9583919  0.99431705 0.9765323  0.957559   0.9763899  0.97603226\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9557713  0.9584488  0.99434847 0.97618794 0.9586668  0.9943115\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.97580916 0.9586668  0.9943115  0.97580916 0.97593707 0.95814645\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9920815  0.9785481  0.9570888  0.99202526 0.9572296  0.9913167\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9782196  0.9561547  0.991639   0.9913267  0.9780807  0.95628923\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.99055856 0.9779442  0.98918927 0.95567274 0.99017245 0.97759974\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9553952  0.9550749  0.9895914  0.97718364 0.95515263 0.9549791\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.98948956 0.9771054 ] dtype=float32) outside given space (Box(0.0, 2.0, (278,), float32))!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Caught sync error: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/IMPALA/IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\IMPALA\\IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04\\'. Retrying after sleeping for 1.0 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Caught sync error: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/IMPALA/IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\IMPALA\\IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04\\'. Retrying after sleeping for 1.0 seconds...\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Caught sync error: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/IMPALA/IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\IMPALA\\IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04\\'. Retrying after sleeping for 1.0 seconds...\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Could not upload checkpoint to c://\\Users\\Alaa\\ray_results\\IMPALA\\IMPALA_PowerSystemEnv_ddac9_00000_0_lr=0.0000_2023-11-03_16-45-04 even after 3 retries.Please check if the credentials expired and that the remote filesystem is supported. For large checkpoints or artifacts, consider increasing `SyncConfig(sync_timeout)` (current value: 1800 seconds).\n",
      "2023-11-03 16:45:17,688\tERROR tune.py:1144 -- Trials did not complete: [IMPALA_PowerSystemEnv_ddac9_00000]\n",
      "2023-11-03 16:45:17,688\tINFO tune.py:1148 -- Total run time: 13.37 seconds (9.84 seconds for the tuning loop).\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=36036, ip=127.0.0.1, actor_id=90f5879e689479febe5f435c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000206917A0DF0>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 370, in step\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 637, in _process_observations\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     processed = policy.agent_connectors(acd_list)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     ret = c(ret)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 206, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     self.check_shape(observation)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 74, in check_shape\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m ValueError: Observation ([0.9418027  0.9871132  0.96512437 0.94320256 0.9909925  0.9653945\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9417069  0.9434859  0.99069506 0.9645829  0.9421855  0.99044997\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.96465814 0.93988144 0.93833125 0.93742555 0.96206886 0.94324887\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9906336  0.96489966 0.9600671  0.9587191  0.9439874  0.9921248\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.96558416 0.94220555 0.9892415  0.9660218  0.9441812  0.9925816\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9657148  0.9438945  0.9927071  0.96567327 0.9457086  0.9944872\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9664246  0.9462899  0.9953613  0.96660334 0.9469202  0.99665123\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9672054  0.9647137  0.9480154  0.9977381  0.9678752  0.96460044\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.94156533 0.9884214 ] dtype=float32) outside given space (Box(0.0, 2.0, (278,), float32))!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=20024, ip=127.0.0.1, actor_id=de26e60cb6e4b733b6d0286a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001814F5C0E20>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 370, in step\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 637, in _process_observations\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     processed = policy.agent_connectors(acd_list)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     ret = c(ret)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 206, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     self.check_shape(observation)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 74, in check_shape\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m ValueError: Observation ([0.95477664 0.98918146 0.97705287 0.9546348  0.989264   0.9767272\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9543585  0.98939776 0.976704   0.9543585  0.9893978  0.976704\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.96089274 0.99648446 0.97895753 0.9580747  0.99617326 0.9774532\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.95643556 0.99591404 0.97652465 0.9563638  0.99587363 0.97655576\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.95278823 0.99402577 0.9740999  0.95635086 0.9957996  0.9765918\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.99322015 0.94519365 0.99122405 0.9682418  0.9929516  0.94519365\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9912241  0.9682419  0.9438204  0.99021626 0.96744454 0.9428023\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.98945004 0.96725863 0.94243956 0.98762447 0.9665854  0.9417494\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9870615  0.96545714] dtype=float32) outside given space (Box(0.0, 2.0, (278,), float32))!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=25844, ip=127.0.0.1, actor_id=74222f66130dea14a8ef7ccd01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000015D010C0E50>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 370, in step\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 637, in _process_observations\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     processed = policy.agent_connectors(acd_list)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     ret = c(ret)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 206, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     self.check_shape(observation)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 74, in check_shape\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m ValueError: Observation ([0.96651953 0.9414491  0.94112974 0.98814154 0.96687126 0.9880294\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.94092065 0.9878185  0.9670419  0.9668669  0.9405831  0.9876553\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.96714145 0.93982947 0.94054216 0.9872585  0.96733356 0.98699194\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.94194233 0.9901921  0.9644837  0.942206   0.9893475  0.96420825\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9423916  0.98928374 0.9638314  0.94239163 0.98928374 0.9638315\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.94218546 0.99044997 0.96465814 0.94139016 0.9901419  0.9640805\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9625394  0.94005734 0.9899611  0.9643775  0.96075773 0.9588374\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9887628  0.9385739  0.99060935 0.9642376  0.9872306  0.93444353\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.938574   0.9906094  0.96423763 0.93250775 0.9317274  0.93187094\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9298741  0.9294343  0.9601403  0.99366343 0.9787368  0.96718925\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.99676234 0.9820286  0.9451908  0.99122703 0.9682402  0.9451935\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.991224   0.96824175 0.94519365 0.9912241  0.9682419  0.9543585\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9893978  0.976704   0.95643556 0.95881546 0.9713391  0.9743376 ] dtype=float32) outside given space (Box(0.0, 2.0, (278,), float32))!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=22968, ip=127.0.0.1, actor_id=9d4d238648ab636e445aeba101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001FA5A550DF0>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 370, in step\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 637, in _process_observations\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     processed = policy.agent_connectors(acd_list)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     ret = c(ret)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 206, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     self.check_shape(observation)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 74, in check_shape\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m ValueError: Observation ([0.95477664 0.98918146 0.97705287 0.9546348  0.989264   0.9767272\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9543585  0.98939776 0.976704   0.9543585  0.9893978  0.976704\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.96089274 0.99648446 0.97895753 0.9580747  0.99617326 0.9774532\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.95643556 0.99591404 0.97652465 0.9563638  0.99587363 0.97655576\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.95278823 0.99402577 0.9740999  0.95635086 0.9957996  0.9765918\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.99322015 0.94519365 0.99122405 0.9682418  0.9929516  0.94519365\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9912241  0.9682419  0.9438204  0.99021626 0.96744454 0.9428023\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.98945004 0.96725863 0.94243956 0.98762447 0.9665854  0.9417494\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9870615  0.96545714] dtype=float32) outside given space (Box(0.0, 2.0, (278,), float32))!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=50704, ip=127.0.0.1, actor_id=a795e91de76b8be340545e1e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000026049C90E20>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 370, in step\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\env_runner_v2.py\", line 637, in _process_observations\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     processed = policy.agent_connectors(acd_list)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\pipeline.py\", line 41, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     ret = c(ret)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in __call__\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\connector.py\", line 254, in <listcomp>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return [self.transform(d) for d in acd_list]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\connectors\\agent\\obs_preproc.py\", line 58, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     d[SampleBatch.NEXT_OBS] = self._preprocessor.transform(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 206, in transform\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     self.check_shape(observation)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 74, in check_shape\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m ValueError: Observation ([0.9418027  0.9871132  0.96512437 0.94320256 0.9909925  0.9653945\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9417069  0.9434859  0.99069506 0.9645829  0.9421855  0.99044997\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.96465814 0.93988144 0.93833125 0.93742555 0.96206886 0.94324887\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9906336  0.96489966 0.9600671  0.9587191  0.9439874  0.9921248\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.96558416 0.94220555 0.9892415  0.9660218  0.9441812  0.9925816\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9657148  0.9438945  0.9927071  0.96567327 0.9457086  0.9944872\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9664246  0.9462899  0.9953613  0.96660334 0.9469202  0.99665123\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.9672054  0.9647137  0.9480154  0.9977381  0.9678752  0.96460044\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m  0.94156533 0.9884214 ] dtype=float32) outside given space (Box(0.0, 2.0, (278,), float32))!\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=36036, ip=127.0.0.1, actor_id=90f5879e689479febe5f435c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000206917A0DF0>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m StopIteration\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=20024, ip=127.0.0.1, actor_id=de26e60cb6e4b733b6d0286a01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001814F5C0E20>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m StopIteration\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=49024, ip=127.0.0.1, actor_id=54940af9a5a5e45b376fccd501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000024E24650E80>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m StopIteration\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=25844, ip=127.0.0.1, actor_id=74222f66130dea14a8ef7ccd01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000015D010C0E50>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m StopIteration\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=22968, ip=127.0.0.1, actor_id=9d4d238648ab636e445aeba101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001FA5A550DF0>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m StopIteration\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=50704, ip=127.0.0.1, actor_id=a795e91de76b8be340545e1e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000026049C90E20>)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\impala\\impala.py\", line 902, in <lambda>\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     lambda worker: worker.sample(),\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m   File \"C:\\Users\\Alaa\\.conda\\envs\\py310\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(Impala pid=20108)\u001b[0m StopIteration\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`C:\\Users\\Alaa\\ray_results\\IMPALA\\experiment_state-2023-11-03_16-45-04.json` must either be a path to an experiment checkpoint file, or a directory containing an experiment checkpoint file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39menvironment(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPowerSystemEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Use to_dict() to get the old-style python config dict\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# when running with tune.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIMPALA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mair\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRunConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m---> 21\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\tuner.py:347\u001b[0m, in \u001b[0;36mTuner.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_ray_client:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TuneError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\n\u001b[0;32m    350\u001b[0m             _TUNER_FAILED_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    351\u001b[0m                 path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_tuner\u001b[38;5;241m.\u001b[39mget_experiment_checkpoint_dir()\n\u001b[0;32m    352\u001b[0m             )\n\u001b[0;32m    353\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:588\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    586\u001b[0m param_space \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_space)\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_restored:\n\u001b[1;32m--> 588\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resume(trainable, param_space)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:703\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[1;34m(self, trainable, param_space)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[0;32m    690\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuner_kwargs,\n\u001b[0;32m    702\u001b[0m }\n\u001b[1;32m--> 703\u001b[0m analysis \u001b[38;5;241m=\u001b[39m run(\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    705\u001b[0m )\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_remote_string_queue()\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\tune.py:1167\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1163\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1164\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment has been interrupted, but the most recent state was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1165\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResume experiment with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrestore_entrypoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1166\u001b[0m         )\n\u001b[1;32m-> 1167\u001b[0m ea \u001b[38;5;241m=\u001b[39m \u001b[43mExperimentAnalysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_storage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ea\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\analysis\\experiment_analysis.py:114\u001b[0m, in \u001b[0;36mExperimentAnalysis.__init__\u001b[1;34m(self, experiment_checkpoint_path, trials, default_metric, default_mode, remote_storage_path, sync_config)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_states \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoints_and_paths: List[Tuple[\u001b[38;5;28mdict\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_checkpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_checkpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoints_and_paths\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials \u001b[38;5;241m=\u001b[39m trials\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\ray\\tune\\analysis\\experiment_analysis.py:196\u001b[0m, in \u001b[0;36mExperimentAnalysis._load_checkpoints\u001b[1;34m(self, experiment_checkpoint_path)\u001b[0m\n\u001b[0;32m    194\u001b[0m latest_checkpoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_latest_checkpoint(experiment_checkpoint_path)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m latest_checkpoints:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_checkpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` must either be a path to an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment checkpoint file, or a directory containing an experiment \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Collect all checkpoints and their directory paths.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# These are used to infer the `local_dir` from the checkpoints\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# in case the experiment folder had been moved from its original\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# location (e.g. from a ray cluster to a GCS/S3 bucket or to local disk).\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_checkpoints_from_latest(latest_checkpoints)\n",
      "\u001b[1;31mValueError\u001b[0m: `C:\\Users\\Alaa\\ray_results\\IMPALA\\experiment_state-2023-11-03_16-45-04.json` must either be a path to an experiment checkpoint file, or a directory containing an experiment checkpoint file."
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ImpalaConfig()\n",
    "# Print out some default values.\n",
    "print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config = config.training(   \n",
    "    lr=tune.grid_search([0.00001])\n",
    ")\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=6) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"IMPALA\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 0}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 300000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"environment\": {\n",
    "            \"disable_env_checking\": True\n",
    "        },\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn\n",
    "        },\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 19,\n",
    "        \"lr\": 3e-5,\n",
    "    },\n",
    "    local_dir=\"SAC-Test\",\n",
    "    # callbacks=[eval_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "class EntropyScheduleCallback(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        # This is called at the beginning of training.\n",
    "        self.config = config\n",
    "\n",
    "    def step(self):\n",
    "        # This is called at each training step.\n",
    "        iteration = self._iteration\n",
    "        start = 0.01\n",
    "        end = 0.001\n",
    "        decay_steps = 10000\n",
    "        decayed_value = (start - end) * ((1 - iteration / decay_steps) ** 2) + end\n",
    "        self.config[\"entropy_coeff\"] = max(decayed_value, end)\n",
    "        # ... rest of your training logic ...\n",
    "        return {\"mean_reward\": ...}  # return your training result here\n",
    "\n",
    "# Define the training configuration\n",
    "training_config = {\n",
    "    \"env\": \"PowerSystemEnv\",\n",
    "    \"num_workers\": 19,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    # SAC-specific hyperparameters\n",
    "    \"timesteps_per_iteration\": 1000,\n",
    "    \"target_entropy\": \"auto\",\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_starts\": 1000,\n",
    "    \"tau\": 0.01,\n",
    "    \"gamma\": 0.99,\n",
    "    \"train_batch_size\": 512,\n",
    "    \"lr\": 1e-4,\n",
    "    \"optimization\": {\n",
    "        \"actor_learning_rate\": 1e-4,\n",
    "        \"critic_learning_rate\": 1e-4,\n",
    "        \"entropy_learning_rate\": 1e-4,\n",
    "    },\n",
    "    \"entropy_coeff\": 0.01,  # initial value\n",
    "    \"clip_actions\": False,\n",
    "    \"normalize_actions\": True,\n",
    "}\n",
    "\n",
    "# Execute the training run\n",
    "tune.run(\n",
    "    EntropyScheduleCallback,\n",
    "    config=training_config,\n",
    "    stop={\"training_iteration\": 20000},  # Adjust as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.sac.sac import SAC\n",
    "from ray.tune import Callback\n",
    "\n",
    "class EntropyScheduleCallback(Callback):\n",
    "    def on_train_result(self, trainer, result):\n",
    "        iteration = result[\"training_iteration\"]\n",
    "        start = 0.01\n",
    "        end = 0.001\n",
    "        decay_steps = 10000\n",
    "        decayed_value = (start - end) * ((1 - iteration / decay_steps) ** 2) + end\n",
    "        trainer.config[\"entropy_coeff\"] = max(decayed_value, end)\n",
    "\n",
    "# Define the training configuration\n",
    "training_config = {\n",
    "    \"env\": \"PowerSystemEnv\",\n",
    "    \"num_workers\": 19,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    # SAC-specific hyperparameters\n",
    "    \"timesteps_per_iteration\": 1000,\n",
    "    \"target_entropy\": \"auto\",\n",
    "    \"buffer_size\": 500000,\n",
    "    \"learning_starts\": 1000,\n",
    "    \"tau\": 0.02,\n",
    "    \"gamma\": 0.95,\n",
    "    \"train_batch_size\": 1024,\n",
    "    \"lr\": 1e-4,\n",
    "    \"optimization\": {\n",
    "        \"actor_learning_rate\": 5e-4,\n",
    "        \"critic_learning_rate\": 5e-4,\n",
    "        \"entropy_learning_rate\": 5e-4,\n",
    "    },\n",
    "    \"entropy_coeff\": 0.01,  # initial value\n",
    "    \"clip_actions\": False,\n",
    "    \"normalize_actions\": True,\n",
    "}\n",
    "\n",
    "# Execute the training run\n",
    "tune.run(\n",
    "    SAC,\n",
    "    config=training_config,\n",
    "    stop={\"training_iteration\": 20000},  # Adjust as needed\n",
    "    callbacks=[EntropyScheduleCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "print(ray.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_decay_schedule(iteration, start=0.01, end=0.001, decay_steps=10000):\n",
    "    decayed_value = (start - end) * ((1 - iteration / decay_steps) ** 2) + end\n",
    "    return max(decayed_value, end)\n",
    "\n",
    "# Define the training configuration\n",
    "training_config = {\n",
    "    \"env\": \"PowerSystemEnv\",\n",
    "    \"num_workers\": 19,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    # SAC-specific hyperparameters\n",
    "    \"timesteps_per_iteration\": 1000,\n",
    "    \"target_entropy\": \"auto\",\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_starts\": 1000,\n",
    "    \"tau\": 0.01,\n",
    "    \"gamma\": 0.99,\n",
    "    \"train_batch_size\": 512,\n",
    "    \"lr\": 1e-4,\n",
    "    \"optimization\": {\n",
    "        \"actor_learning_rate\": 1e-4,\n",
    "        \"critic_learning_rate\": 1e-4,\n",
    "        \"entropy_learning_rate\": 1e-4,\n",
    "    },\n",
    "    \"clip_actions\": False,\n",
    "    \"normalize_actions\": True\n",
    "    #\"entropy_coeff_schedule\": polynomial_decay_schedule,\n",
    "}\n",
    "\n",
    "# Execute the training run\n",
    "tune.run(\n",
    "    \"SAC\",\n",
    "    config=training_config,\n",
    "    stop={\"training_iteration\": 20000},  # Adjust as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75cdb9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dss.circuit_all_node_names(): ['150.1', '150.2', '150.3', '150r.1', '150r.2', '150r.3', '149.1', '149.2', '149.3', '1.1', '1.2', '1.3', '2.2', '3.3', '7.1', '7.2', '7.3', '4.3', '5.3', '6.3', '8.1', '8.2', '8.3', '12.2', '9.1', '13.1', '13.2', '13.3', '9r.1', '14.1', '34.3', '18.1', '18.2', '18.3', '11.1', '10.1', '15.3', '16.3', '17.3', '19.1', '21.1', '21.2', '21.3', '20.1', '22.2', '23.1', '23.2', '23.3', '24.3', '25.1', '25.2', '25.3', '25r.1', '25r.3', '26.1', '26.3', '28.1', '28.2', '28.3', '27.1', '27.3', '31.3', '33.1', '29.1', '29.2', '29.3', '30.1', '30.2', '30.3', '250.1', '250.2', '250.3', '32.3', '35.1', '35.2', '35.3', '36.1', '36.2', '40.1', '40.2', '40.3', '37.1', '38.2', '39.2', '41.3', '42.1', '42.2', '42.3', '43.2', '44.1', '44.2', '44.3', '45.1', '47.1', '47.2', '47.3', '46.1', '48.1', '48.2', '48.3', '49.1', '49.2', '49.3', '50.1', '50.2', '50.3', '51.1', '51.2', '51.3', '151.1', '151.2', '151.3', '52.1', '52.2', '52.3', '53.1', '53.2', '53.3', '54.1', '54.2', '54.3', '55.1', '55.2', '55.3', '57.1', '57.2', '57.3', '56.1', '56.2', '56.3', '58.2', '60.1', '60.2', '60.3', '59.2', '61.1', '61.2', '61.3', '62.1', '62.2', '62.3', '63.1', '63.2', '63.3', '64.1', '64.2', '64.3', '65.1', '65.2', '65.3', '66.1', '66.2', '66.3', '67.1', '67.2', '67.3', '68.1', '72.1', '72.2', '72.3', '97.1', '97.2', '97.3', '69.1', '70.1', '71.1', '73.3', '76.1', '76.2', '76.3', '74.3', '75.3', '77.1', '77.2', '77.3', '86.1', '86.2', '86.3', '78.1', '78.2', '78.3', '79.1', '79.2', '79.3', '80.1', '80.2', '80.3', '81.1', '81.2', '81.3', '82.1', '82.2', '82.3', '84.3', '83.1', '83.2', '83.3', '85.3', '87.1', '87.2', '87.3', '88.1', '89.1', '89.2', '89.3', '90.2', '91.1', '91.2', '91.3', '92.3', '93.1', '93.2', '93.3', '94.1', '95.1', '95.2', '95.3', '96.2', '98.1', '98.2', '98.3', '99.1', '99.2', '99.3', '100.1', '100.2', '100.3', '450.1', '450.2', '450.3', '197.1', '197.2', '197.3', '101.1', '101.2', '101.3', '102.3', '105.1', '105.2', '105.3', '103.3', '104.3', '106.2', '108.1', '108.2', '108.3', '107.2', '109.1', '300.1', '300.2', '300.3', '110.1', '111.1', '112.1', '113.1', '114.1', '135.1', '135.2', '135.3', '152.1', '152.2', '152.3', '160r.1', '160r.2', '160r.3', '160.1', '160.2', '160.3', '61s.1', '61s.2', '61s.3', '300_open.1', '300_open.2', '300_open.3', '94_open.1', '610.1', '610.2', '610.3']\n"
     ]
    }
   ],
   "source": [
    "print(f'dss.circuit_all_node_names(): {dss.circuit_all_node_names()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7c5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n"
     ]
    }
   ],
   "source": [
    "buses = ['150.1', '150.2', '150.3', '150r.1', '150r.2', '150r.3', '149.1', '149.2', '149.3', '1.1', '1.2', '1.3', '2.2', '3.3', '7.1', '7.2', '7.3', '4.3', '5.3', '6.3', '8.1', '8.2', '8.3', '12.2', '9.1', '13.1', '13.2', '13.3', '9r.1', '14.1', '34.3', '18.1', '18.2', '18.3', '11.1', '10.1', '15.3', '16.3', '17.3', '19.1', '21.1', '21.2', '21.3', '20.1', '22.2', '23.1', '23.2', '23.3', '24.3', '25.1', '25.2', '25.3', '25r.1', '25r.3', '26.1', '26.3', '28.1', '28.2', '28.3', '27.1', '27.3', '31.3', '33.1', '29.1', '29.2', '29.3', '30.1', '30.2', '30.3', '250.1', '250.2', '250.3', '32.3', '35.1', '35.2', '35.3', '36.1', '36.2', '40.1', '40.2', '40.3', '37.1', '38.2', '39.2', '41.3', '42.1', '42.2', '42.3', '43.2', '44.1', '44.2', '44.3', '45.1', '47.1', '47.2', '47.3', '46.1', '48.1', '48.2', '48.3', '49.1', '49.2', '49.3', '50.1', '50.2', '50.3', '51.1', '51.2', '51.3', '151.1', '151.2', '151.3', '52.1', '52.2', '52.3', '53.1', '53.2', '53.3', '54.1', '54.2', '54.3', '55.1', '55.2', '55.3', '57.1', '57.2', '57.3', '56.1', '56.2', '56.3', '58.2', '60.1', '60.2', '60.3', '59.2', '61.1', '61.2', '61.3', '62.1', '62.2', '62.3', '63.1', '63.2', '63.3', '64.1', '64.2', '64.3', '65.1', '65.2', '65.3', '66.1', '66.2', '66.3', '67.1', '67.2', '67.3', '68.1', '72.1', '72.2', '72.3', '97.1', '97.2', '97.3', '69.1', '70.1', '71.1', '73.3', '76.1', '76.2', '76.3', '74.3', '75.3', '77.1', '77.2', '77.3', '86.1', '86.2', '86.3', '78.1', '78.2', '78.3', '79.1', '79.2', '79.3', '80.1', '80.2', '80.3', '81.1', '81.2', '81.3', '82.1', '82.2', '82.3', '84.3', '83.1', '83.2', '83.3', '85.3', '87.1', '87.2', '87.3', '88.1', '89.1', '89.2', '89.3', '90.2', '91.1', '91.2', '91.3', '92.3', '93.1', '93.2', '93.3', '94.1', '95.1', '95.2', '95.3', '96.2', '98.1', '98.2', '98.3', '99.1', '99.2', '99.3', '100.1', '100.2', '100.3', '450.1', '450.2', '450.3', '197.1', '197.2', '197.3', '101.1', '101.2', '101.3', '102.3', '105.1', '105.2', '105.3', '103.3', '104.3', '106.2', '108.1', '108.2', '108.3', '107.2', '109.1', '300.1', '300.2', '300.3', '110.1', '111.1', '112.1', '113.1', '114.1', '135.1', '135.2', '135.3', '152.1', '152.2', '152.3', '160r.1', '160r.2', '160r.3', '160.1', '160.2', '160.3', '61s.1', '61s.2', '61s.3', '300_open.1', '300_open.2', '300_open.3', '94_open.1', '610.1', '610.2', '610.3']\n",
    "print(len(buses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bd9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Testing with partial observability of voltage violatiosn for each agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Tuple, Dict\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.sac import SAC\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "from py_dss_interface import DSSDLL\n",
    "#import stable_baselines3\n",
    "#from stable_baselines3 import SAC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from stable_baselines3 import A2C, DQN, PPO, TD3, SAC\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenDSS\n",
    "dss = DSSDLL(r\"C:\\Program Files\\OpenDSS\")\n",
    "dss_file = r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\"\n",
    "dss.text(f\"compile [{dss_file}]\")\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -800\n",
    "PV_KVAR_ACTION_HIGH = 800\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 1000\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 1000\n",
    "\n",
    "TRANSFORMER_TAPS_ACTION_LOW = 0.9\n",
    "TRANSFORMER_TAPS_ACTION_HIGH = 1.1\n",
    "\n",
    "CAPACITOR_ACTION_LOW = 0\n",
    "CAPACITOR_ACTION_HIGH = 1\n",
    "\n",
    "\n",
    "class PowerSystemEnv(MultiAgentEnv):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(PowerSystemEnv, self).__init__()\n",
    "\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses  = [\n",
    "                                '1', '7', '8', '13', '21', '23', '29', '250', '35', '40', '42', '55',\n",
    "                                '56', '65', '76', '78', '66', '79', '81', '83', '91', '95', '100',\n",
    "                                '197', '300', '110', '135', '160', '152', '610'\n",
    "                            ]\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=1000\n",
    "        self.previous_reward = 0.0\n",
    "        self.alpha = 0.1  #\n",
    "        #self.last_actions = {agent_id: np.zeros(original_action_size) for agent_id in self._agent_ids}\n",
    "\n",
    "         # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new Generator.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=1000 kW=800\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=1000 kVAR=0\")\n",
    "        \n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "    \n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)  # Skip the header\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "            \n",
    "            \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array(\n",
    "                [PV_KVAR_ACTION_LOW]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_LOW]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_LOW]*BESS_KW_ACTION_LEN \n",
    "            ),\n",
    "            high=np.array(\n",
    "                [PV_KVAR_ACTION_HIGH]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_HIGH]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_HIGH]*BESS_KW_ACTION_LEN \n",
    "               \n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "            \n",
    "\n",
    "     \n",
    "        # Assuming observation space is the voltage at each bus\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(278,), dtype=np.float32)  # Modified shape\n",
    "            \n",
    "         \n",
    "        # Update the action and observation spaces for each agent\n",
    "        self.action_space_dict = {\n",
    "            f\"agent_{i}\": spaces.Box(\n",
    "                low=np.concatenate(([PV_KW_ACTION_LOW] * 6, [PV_KVAR_ACTION_LOW] * 6, [BESS_KW_ACTION_LOW] * 6)),\n",
    "                high=np.concatenate(([PV_KW_ACTION_HIGH] * 6, [PV_KVAR_ACTION_HIGH] * 6, [BESS_KW_ACTION_HIGH] * 6)),\n",
    "                shape=(18,), dtype=np.float32) for i in range(5)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        self.observation_space_dict = {}\n",
    "        \n",
    "        #self.observation_space_dict = {\n",
    "              # agent_id: spaces.Box(low=0, high=2, shape=(278,), dtype=np.float32) for agent_id in self._agent_ids\n",
    "              #  }\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Initializing the observation_space_dict\n",
    "        #self.observation_space_dict = {}\n",
    "\n",
    "        # Define the sizes for each agent's observations\n",
    "        agent_sizes = [62, 62, 62, 60, 32]\n",
    "\n",
    "        for i in range(5):\n",
    "            agent_obs_size = agent_sizes[i]\n",
    "            self.observation_space_dict[f'agent_{i}'] = spaces.Box(low=0, high=2, shape=(agent_obs_size,), dtype=np.float32)\n",
    "\n",
    "\n",
    "        # For the last agent\n",
    "        #self.observation_space_dict['agent_4'] = spaces.Box(low=0, high=2, shape=(78,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        self.current_step = 0\n",
    "\n",
    "        \n",
    "         # Initialize control step counter\n",
    "        self.control_steps = 0\n",
    "\n",
    "        # Maximum control steps allowed in one episode\n",
    "        self.max_control_steps =int(5)  # for example\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "          # Define the agent IDs\n",
    "        self._agent_ids = [f'agent_{i}' for i in range(5)]\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "        \n",
    "    def _take_action(self, action_dict):\n",
    "    # Define total actions per agent\n",
    "        TOTAL_ACTIONS_PER_AGENT = 6 + 6 + 6\n",
    "\n",
    "    # Iterate through the action_dict\n",
    "        for agent_id, action in action_dict.items():\n",
    "            agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "        # Determine action segments for this agent\n",
    "            pv_kw_actions = action[:6]\n",
    "            pv_kvar_actions = action[6:12]\n",
    "            bess_kw_actions = action[12:18]\n",
    "\n",
    "        # Index offset based on agent_idx for PV and battery control\n",
    "            pv_idx_offset = agent_idx * 6\n",
    "            battery_idx_offset = agent_idx * 6\n",
    "\n",
    "        # Actions for PVs\n",
    "            for idx, (kw_action_value, kvar_action_value) in enumerate(zip(pv_kw_actions, pv_kvar_actions)):\n",
    "            # handle kW actions\n",
    "                irradiance = self.irradiance_profile[(self.current_step) % 24]\n",
    "                scaled_pv_kw = kw_action_value * irradiance\n",
    "                if scaled_pv_kw > irradiance * self.KWrated:  # clip to max\n",
    "                    scaled_pv_kw = irradiance * self.KWrated\n",
    "\n",
    "            # handle kVAR actions\n",
    "                pv_kvar = kvar_action_value\n",
    "                S_max = 1000  # Maximum apparent power (example value)\n",
    "                q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "                pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "                self.controller.text(f\"edit Generator.PV{idx + pv_idx_offset + 1} phases=3 kV=4.16 kW={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "\n",
    "            for idx, action_value in enumerate(bess_kw_actions):\n",
    "                self.controller.text(f\"edit Storage.Battery{idx + battery_idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action_dict):\n",
    "    # Execute the action\n",
    "        self._take_action(action_dict)\n",
    "\n",
    "    # Calculate the rewards\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages if v <= 0.95 or v >= 1.05)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate the sum of squared voltage deviations from 1. This penalizes larger deviations more heavily.\n",
    "        voltage_deviations = sum((v - 1)**2 for v in all_bus_voltages)\n",
    "\n",
    "\n",
    "\n",
    "        # Define penalty weights for different components\n",
    "        w_deviation = 1.0\n",
    "        w_violation = 5.0\n",
    "        w_loss = 0.001\n",
    "\n",
    "        # Combine losses, violations, and deviations in the reward\n",
    "        #reward = - w_deviation * voltage_deviations - w_violation * voltage_violations\n",
    "        #reward = - (2 ** voltage_violations) - 1\n",
    "        #reward = -  voltage_deviations\n",
    "        reward = - voltage_violations\n",
    "        \n",
    "\n",
    "        \n",
    "        #-  self.control_steps\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Gather observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # Set the reward for all agents\n",
    "        rewards = {agent_id: reward for agent_id in self._agent_ids}\n",
    "\n",
    "    # Set termination status for all agents\n",
    "        terminations = {agent_id: (self.control_steps >= self.max_control_steps) or (voltage_violations == 0) for agent_id in self._agent_ids}\n",
    "        terminations['__all__'] = any(terminations.values())\n",
    "\n",
    "    # Assuming no truncation; modify as needed\n",
    "        truncateds = {agent_id: False for agent_id in self._agent_ids}\n",
    "        truncateds['__all__'] = any(truncateds.values())\n",
    "\n",
    "    # Gather additional information if required\n",
    "        infos = {agent_id: {} for agent_id in self._agent_ids}\n",
    "\n",
    "    # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        return observations, rewards, terminations, truncateds, infos\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "    # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    # You can use options to customize the reset function if needed\n",
    "    # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        \n",
    "        #self.current_step = int(np.clip(np.random.normal(loc=0, scale=8760), 0, 8760))\n",
    "        self.current_step = np.random.randint(0, 25)\n",
    "       \n",
    "\n",
    " # assuming the profile has 8760 hours  #use gaussian distribution\n",
    "        self.control_steps = 0\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 24], scale=0.5, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 24]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 10000 * self.irradiance_profile[self.current_step % 24]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 24]*1000  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit Generator.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} kW={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "        \n",
    "        # Get the observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # You can include any additional information here. If there's nothing, just return an empty dictionary.\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self, agent_id):\n",
    "        # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "\n",
    "        agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "        # Determine the start and end indices for slicing the all_bus_voltages\n",
    "        if agent_idx < 3:  # First three agents\n",
    "            start_idx = agent_idx * 62\n",
    "            end_idx = start_idx + 62\n",
    "        elif agent_idx == 3:  # fourth agent\n",
    "            start_idx = agent_idx * 62\n",
    "            end_idx = start_idx + 60\n",
    "        else:  # Fifth and last agent\n",
    "            start_idx =3 * 62 + 60  # Cover all the previous indices\n",
    "            end_idx = start_idx + 32\n",
    "\n",
    "        agent_observations = all_bus_voltages[start_idx:end_idx]\n",
    "\n",
    "        return agent_observations\n",
    "\n",
    "\n",
    "    def get_agent_observation(self):\n",
    "        agent_obs = {}\n",
    "        for agent_id in self.agents:\n",
    "            agent_obs[agent_id] = self.get_observation(agent_id)  # Note: We're still passing agent_id even if it's unused. This keeps the interface consistent.\n",
    "        return agent_obs\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "\n",
    "    \n",
    "\n",
    "irradiance_csv_file = r\"D:\\Alaa_Selim\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "load_profile_file = r\"D:\\Alaa_Selim\\LoadShape1.csv\"\n",
    "\n",
    "    \n",
    "    \n",
    "original_env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "env = MultiAgentEnvCompatibility(original_env)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PowerSystemEnv(\n",
    "                            dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "                            dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "                            irradiance_csv_file=irradiance_csv_file,\n",
    "                            load_profile_file=load_profile_file\n",
    "                            )\n",
    "\n",
    "register_env(\"PowerSystemEnv\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "# Suppose you have N agents\n",
    "N = 5  # adjust this as per your actual number of agents\n",
    "\n",
    "policies = {\n",
    "    f\"policy_{i}\": (None, \n",
    "                    original_env.observation_space_dict[f\"agent_{i}\"], \n",
    "                    original_env.action_space_dict[f\"agent_{i}\"], \n",
    "                    {\"agent_id\": i})  # Set 'agent_id' key here\n",
    "    for i in range(N)\n",
    "}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    if \"agent_\" in agent_id:\n",
    "        # Extract agent number and return the corresponding policy\n",
    "        agent_num = int(agent_id.split(\"_\")[1])\n",
    "        return f\"policy_{agent_num}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12325712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC, TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# First, wrap the custom environment with Monitor\n",
    "monitored_env = Monitor(PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    "), './models/')\n",
    "\n",
    "# Then, wrap the monitored environment with DummyVecEnv\n",
    "env = DummyVecEnv([lambda: monitored_env])\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./models/')\n",
    "\n",
    "# Training configurations\n",
    "num_timesteps = 100000\n",
    "agents = {\n",
    "    \"SAC\": SAC(\"MlpPolicy\", env, verbose=1),\n",
    "}\n",
    "\n",
    "# Train each agent and store results\n",
    "results = {}\n",
    "for name, agent in agents.items():\n",
    "    print(f\"\\nTraining {name}...\\n\")\n",
    "    agent.learn(total_timesteps=num_timesteps, callback=checkpoint_callback)\n",
    "    results[name] = load_results('./models/')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Plot learning curves\n",
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "def plot_results():\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        x, y = ts2xy(result, 'timesteps')\n",
    "        y = moving_average(y, window=50)  # Window of 50 for smoothing\n",
    "        ax.plot(x[:-49], y, label=name)\n",
    "    \n",
    "    ax.set_xlabel(\"Timesteps\")\n",
    "    ax.set_ylabel(\"Rewards\")\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "    plt.savefig('SAC_Learning.pdf', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "plot_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de25702",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 1000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"environment\": {\n",
    "            \"disable_env_checking\": True\n",
    "        },\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn\n",
    "        },\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 19,\n",
    "        \"lr\": 3e-5,\n",
    "        \"tau\": 5e-3,\n",
    "        \"prioritized_replay\": True,\n",
    "        \"actor_learning_rate\": 3e-5,\n",
    "        \"critic_learning_rate\": 3e-5,\n",
    "        \"entropy_learning_rate\": 3e-5,\n",
    "        \n",
    "        \n",
    "        \n",
    "    },\n",
    "    local_dir=\"SAC-Test\",\n",
    "    # callbacks=[eval_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.sac.sac import SACConfig\n",
    "\n",
    "\n",
    "config = SACConfig().training(gamma=0.9, lr=0.01)  \n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=4)  \n",
    "print(config.to_dict())  \n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"PowerSystemEnv\")  \n",
    "algo.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.maddpg.maddpg import MADDPGConfig\n",
    "config = MADDPGConfig()\n",
    "print(config.replay_buffer_config)  \n",
    "replay_config = config.replay_buffer_config.update(  \n",
    "    {\n",
    "        \"capacity\": 100000,\n",
    "        \"prioritized_replay_alpha\": 0.8,\n",
    "        \"prioritized_replay_beta\": 0.45,\n",
    "        \"prioritized_replay_eps\": 2e-6,\n",
    "    }\n",
    ")\n",
    "config.training(replay_buffer_config=replay_config)   \n",
    "config = config.resources(num_gpus=0)   \n",
    "config = config.rollouts(num_rollout_workers=4)   \n",
    "config = config.environment(\"PowerSystemEnv\")   \n",
    "algo = config.build()  \n",
    "algo.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf16b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.maddpg.maddpg import MADDPGConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = MADDPGConfig()\n",
    "config.training(n_step=tune.grid_search([3, 5]))  \n",
    "config.environment(env=\"PowerSystemEnv\")  \n",
    "tune.Tuner(  \n",
    "    \"MADDPG\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\":200}),\n",
    "    param_space=config.to_dict()\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e170d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 30000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "         \"evaluation_interval\": 2,\n",
    "        \"evaluation_num_episodes\": 10,\n",
    "        \"environment\": {\n",
    "            \"disable_env_checking\": True\n",
    "        },\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn\n",
    "        },\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 17,\n",
    "        \"lr\": 3e-4,\n",
    "        \"grad_clip\": 40.0,  # Clip gradients with L2 norm exceeding this value\n",
    "    },\n",
    "    local_dir=\"SAC-Test\",\n",
    "    # callbacks=[eval_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42778ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TestController:\n",
    "    def __init__(self):\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(278,), dtype=np.float32)\n",
    "        self.irradiance_profile = np.ones(8760) * 0.5  # For simplicity, set irradiance to 0.5 for all hours\n",
    "        self.KWrated = 100\n",
    "        self.current_step = 0\n",
    "        self.action_space_dict = {\n",
    "            f\"agent_{i}\": spaces.Box(\n",
    "                low=np.concatenate(([PV_KW_ACTION_LOW] * 6, [PV_KVAR_ACTION_LOW] * 6, [BESS_KW_ACTION_LOW] * 6)),\n",
    "                high=np.concatenate(([PV_KW_ACTION_HIGH] * 6, [PV_KVAR_ACTION_HIGH] * 6, [BESS_KW_ACTION_HIGH] * 6)),\n",
    "                shape=(18,), dtype=np.float32) for i in range(5)\n",
    "        }\n",
    "\n",
    "        self.observation_space_dict = {}\n",
    "        for i in range(4):  # For the first four agents\n",
    "            self.observation_space_dict[f'agent_{i}'] = spaces.Box(low=0, high=2, shape=(50,), dtype=np.float32)\n",
    "        # For the last agent\n",
    "        self.observation_space_dict['agent_4'] = spaces.Box(low=0, high=2, shape=(78,), dtype=np.float32)\n",
    "        \n",
    "        self._agent_ids = [f'agent_{i}' for i in range(5)]\n",
    "    \n",
    "    def _take_action(self, action_dict):\n",
    "    # Define total actions per agent\n",
    "        TOTAL_ACTIONS_PER_AGENT = 6 + 6 + 6\n",
    "\n",
    "    # Iterate through the action_dict\n",
    "        for agent_id, action in action_dict.items():\n",
    "            agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "        # Determine action segments for this agent\n",
    "            pv_kw_actions = action[:6]\n",
    "            pv_kvar_actions = action[6:12]\n",
    "            bess_kw_actions = action[12:18]\n",
    "\n",
    "        # Index offset based on agent_idx for PV and battery control\n",
    "            pv_idx_offset = agent_idx * 6\n",
    "            battery_idx_offset = agent_idx * 6\n",
    "\n",
    "        # Actions for PVs\n",
    "            for idx, (kw_action_value, kvar_action_value) in enumerate(zip(pv_kw_actions, pv_kvar_actions)):\n",
    "            # handle kW actions\n",
    "                irradiance = self.irradiance_profile[(self.current_step) % 8760]\n",
    "                scaled_pv_kw = kw_action_value * irradiance\n",
    "                if scaled_pv_kw > irradiance * self.KWrated:  # clip to max\n",
    "                    scaled_pv_kw = irradiance * self.KWrated\n",
    "\n",
    "            # handle kVAR actions\n",
    "                pv_kvar = kvar_action_value\n",
    "                S_max = 100  # Maximum apparent power (example value)\n",
    "                q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "                pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "                print(f\"edit PVSystem.PV{idx + pv_idx_offset + 1} phases=3 kV=4.16 kW={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "\n",
    "            for idx, action_value in enumerate(bess_kw_actions):\n",
    "                print(f\"edit Storage.Battery{idx + battery_idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "\n",
    "        # Solve the power flow\n",
    "        print(\"set controlmode=off\")\n",
    "        print(\"solve\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def text(self, cmd):\n",
    "        print(cmd)  # Mock function to print the commands\n",
    "\n",
    "# Assuming you've already defined or imported spaces.Box from somewhere (like OpenAI Gym)\n",
    "class spaces:\n",
    "    class Box:\n",
    "        def __init__(self, low, high, shape, dtype):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.shape = shape\n",
    "            self.dtype = dtype\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "# Instantiate the test controller\n",
    "controller = TestController()\n",
    "\n",
    "# Create a mock action_dict\n",
    "action_dict = {\n",
    "    f\"agent_{i}\": np.random.rand(18)*100 for i in range(5)\n",
    "}\n",
    "\n",
    "# Call the function\n",
    "controller._take_action(action_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55af1d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 17:42:52,138\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2024-01-29 17:42:54,253\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2024-01-29 17:42:54,261\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n",
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Testing with partial observability of voltage violatiosn for each agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Tuple, Dict\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.sac import SAC\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "from py_dss_interface import DSSDLL\n",
    "#import stable_baselines3\n",
    "#from stable_baselines3 import SAC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from stable_baselines3 import A2C, DQN, PPO, TD3, SAC\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenDSS\n",
    "dss = DSSDLL(r\"C:\\Program Files\\OpenDSS\")\n",
    "dss_file = r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\"\n",
    "dss.text(f\"compile [{dss_file}]\")\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "TRANSFORMER_TAPS_ACTION_LOW = 0.9\n",
    "TRANSFORMER_TAPS_ACTION_HIGH = 1.1\n",
    "\n",
    "CAPACITOR_ACTION_LOW = 0\n",
    "CAPACITOR_ACTION_HIGH = 1\n",
    "\n",
    "\n",
    "class PowerSystemEnv(gym.Env):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file,load_profile_file):\n",
    "        super(PowerSystemEnv, self).__init__()\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses = [\n",
    "                                '1', '7', '8', '13', '21', '23', '29', '250', '35', '40', '42', '55',\n",
    "                                '56', '65', '76', '78', '66', '79', '81', '83', '91', '95', '100',\n",
    "                                '197', '300', '110', '135', '160', '152', '610'\n",
    "                            ]\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "        \n",
    "        # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=100 Pmpp=80\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=100 kVAR=0\")\n",
    "        \n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "    \n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)  # Skip the header\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "            \n",
    "        \n",
    "\n",
    "        # Define action and observation space\n",
    "        # Actions are continuous values for pv_kvar, pv_kW, battery_kw, transformer_tap and capacitor_states\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array(\n",
    "                [PV_KVAR_ACTION_LOW]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_LOW]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_LOW]*BESS_KW_ACTION_LEN +\n",
    "                [TRANSFORMER_TAPS_ACTION_LOW]*TRANSFORMER_TAPS_ACTION_LEN +\n",
    "                [CAPACITOR_ACTION_LOW]*CAPACITOR_ACTION_LEN\n",
    "            ),\n",
    "            high=np.array(\n",
    "                [PV_KVAR_ACTION_HIGH]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_HIGH]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_HIGH]*BESS_KW_ACTION_LEN +\n",
    "                [TRANSFORMER_TAPS_ACTION_HIGH]*TRANSFORMER_TAPS_ACTION_LEN +\n",
    "                [CAPACITOR_ACTION_HIGH]*CAPACITOR_ACTION_LEN\n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # Assuming observation space is the voltage at each bus\n",
    "        self.observation_space = spaces.Box(low=0, high=100000, shape=(278,), dtype=np.float32)  # Modified shape\n",
    "        \n",
    "        self.current_step = 0\n",
    "\n",
    "        \n",
    "         # Initialize control step counter\n",
    "        self.control_steps = 0\n",
    "\n",
    "        # Maximum control steps allowed in one episode\n",
    "        self.max_control_steps =0  # for example\n",
    "        self.hourly_violations_count = []\n",
    "        \n",
    "        \n",
    "    def update_csv(self, current_step, voltage_violations, pv_kvar, pv_kw, battery_kw, transformer_tap, capacitor_states):\n",
    "        csv_file = 'training_data_all.csv'\n",
    "        file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # If the file doesn't exist, write the header\n",
    "            if not file_exists:\n",
    "                header = ['current_step', 'voltage_violations']\n",
    "                header.extend([f'pv_kvar_{i}' for i in range(30)])\n",
    "                header.extend([f'pv_kw_{i}' for i in range(30)])\n",
    "                header.extend([f'battery_kw_{i}' for i in range(30)])\n",
    "                header.append('transformer_tap')\n",
    "                header.extend([f'capacitor_states_{i}' for i in range(len(capacitor_states))])  # Adjust the range if necessary\n",
    "\n",
    "                writer.writerow(header)\n",
    "\n",
    "            # Write the data row\n",
    "            row = [current_step, voltage_violations]\n",
    "            row.extend(pv_kvar)\n",
    "            row.extend(pv_kw)\n",
    "            row.extend(battery_kw)\n",
    "            row.append(transformer_tap)\n",
    "            row.extend(capacitor_states)\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \n",
    "        pv_kvar = action[:30].copy()\n",
    "        pv_kw = action[30:60].copy()\n",
    "        battery_kw = action[60:90].copy()\n",
    "        transformer_tap = action[90].copy()\n",
    "        capacitor_states = action[91:].copy()\n",
    "        capacitor_states = np.round(capacitor_states)\n",
    "\n",
    "        \n",
    "        \n",
    "        pv_kw_limits = [0, 100]  # Limits for PV kW (example values)\n",
    "        q_max1_limits = [-80, 80]  # Limits for maximum reactive power (example values)\n",
    "    \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        for z in range (0,30):\n",
    "            if  pv_kw[z] <= irradiance* self.KWrated:\n",
    "            # no clipping\n",
    "                pv_kw[z] = pv_kw[z]\n",
    "                #pv_kvar[z]  = np.sqrt(100**2 - np.power(pv_kw[z], 2))\n",
    "            else:\n",
    "                pv_kw[z] = irradiance* self.KWrated\n",
    "     \n",
    "        #pv_kw = np.clip(action[30:60], pv_kw_limits[0], pv_kw_limits[1])\n",
    "    \n",
    "        S_max = 100  # Maximum apparent power (example value)\n",
    "\n",
    "        q_max1 = np.sqrt(S_max**2 - np.power(pv_kw, 2))\n",
    "        \n",
    "        #pv_kvar= 0.8 ** pv_kw\n",
    "        #add checking condition on action space, or you can use PU. system or use clip action (make a constrined actio space)\n",
    "           \n",
    "    # Make pv_kvar controlled based on q_max1\n",
    "        pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "        \n",
    "         \n",
    "        \n",
    "       \n",
    "        # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            scaled_pv_kw = pv_kw[i] \n",
    "            #* irradiance\n",
    "          \n",
    "            \n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar[i]} Pmpp={scaled_pv_kw}\")\n",
    "            # need to add KVA in OpenDSS model\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw[i]} kVAR=0\")\n",
    "\n",
    "        # Apply transformer control\n",
    "        self.controller.text(f\"edit Transformer.reg1a taps={transformer_tap}\")\n",
    "\n",
    "        # Apply capacitor control\n",
    "        for i, cap_name in enumerate(self.capacitor_names):\n",
    "            self.controller.text(f\"edit Capacitor.{cap_name} states={int(capacitor_states[i])}\")\n",
    "        #add control mode = off\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\") \n",
    "        #print(f'dss.circuit_losses(): {dss.circuit_losses()}')\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "    # Execute the action\n",
    "        self._take_action(action)\n",
    "\n",
    "    # Calculate the rewards\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages if v <= 0.95 or v >= 1.05 and v != 0)\n",
    "\n",
    "    # Calculate the sum of squared voltage deviations from 1. This penalizes larger deviations more heavily.\n",
    "        voltage_deviations = sum((v - 1)**2 for v in all_bus_voltages)\n",
    "\n",
    "    # Define penalty weights for different components\n",
    "        w_deviation = 1.0\n",
    "        w_violation = 5.0\n",
    "        w_loss = 0.001\n",
    "\n",
    "    # Combine losses, violations, and deviations in the reward\n",
    "        reward = -voltage_violations \n",
    "        self.hourly_violations_count.append(voltage_violations)\n",
    "       # print(f\"violations count: {voltage_violations}\")\n",
    "        # Extract the action parameters\n",
    "        pv_kvar = action[:30].copy()\n",
    "        pv_kw = action[30:60].copy()\n",
    "        battery_kw = action[60:90].copy()\n",
    "        transformer_tap = action[90].copy()\n",
    "        capacitor_states = action[91:].copy()\n",
    "        capacitor_states = np.round(capacitor_states)\n",
    "\n",
    "        # Call update_csv with all the necessary data\n",
    "        self.update_csv(self.current_step, voltage_violations, pv_kvar, pv_kw, battery_kw, transformer_tap, capacitor_states.tolist())\n",
    "\n",
    "\n",
    "    # Gather observations\n",
    "        observation = self.get_observation()\n",
    "\n",
    "        termination = (self.control_steps >= self.max_control_steps) or (voltage_violations == 0)\n",
    "\n",
    "    # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        return observation, reward, termination, False, {}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "    # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            seed = 1 \n",
    "            np.random.seed(seed)\n",
    "\n",
    "    # You can use options to customize the reset function if needed\n",
    "    # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        \n",
    "        #self.current_step = int(np.clip(np.random.normal(loc=0, scale=8760), 0, 8760))\n",
    "        #self.current_step = np.random.randint(0, 8761)\n",
    "        self.current_step = (self.current_step + 1) % 8760\n",
    "        #print(f\"current_step : {self.current_step}\")\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    " # assuming the profile has 8760 hours  #use gaussian distribution\n",
    "        self.control_steps = 0\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 8760], scale=0.001, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 8760]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 8760]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "        \n",
    "        # Get the observations for all agents\n",
    "        observations = self.get_observation()\n",
    "\n",
    "    # You can include any additional information here. If there's nothing, just return an empty dictionary.\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self):\n",
    "    # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "    \n",
    "    # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "    \n",
    "    # As there's only one agent, you should decide the observation length. \n",
    "    # If you'd like to keep the previous structure, the agent might observe the first 50, or all of them.\n",
    "    # Here, I'm assuming the agent observes all of them:\n",
    "        agent_observation = all_bus_voltages\n",
    "\n",
    "        return agent_observation\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "\n",
    "    \n",
    "\n",
    "irradiance_csv_file = r\"D:\\Alaa_Selim\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "load_profile_file = r\"D:\\Alaa_Selim\\LoadShape1.csv\"\n",
    "\n",
    "    \n",
    "    \n",
    "original_env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "\n",
    "#from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "#env = MultiAgentEnvCompatibility(original_env)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PowerSystemEnv(\n",
    "                            dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "                            dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "                            irradiance_csv_file=irradiance_csv_file,\n",
    "                            load_profile_file=load_profile_file\n",
    "                            )\n",
    "\n",
    "register_env(\"PowerSystemEnv\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f066f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "    0  0.000E+00  -5.060E+01\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   10  0.000E+00  -3.000E-01\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   20  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   30  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   40  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   50  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   60  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   70  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   80  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "   90  0.000E+00  -0.000E+00\n",
      "Iters.  Best Reward  Best(Mean Reward)\n",
      "  100  0.000E+00  -0.000E+00\n",
      "Total optimization time: 36.88770341873169 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from qpso import QDPSO\n",
    "start_time = time.time()\n",
    "# Create an instance of the environment\n",
    "env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "\n",
    "# Constants\n",
    "T = 48  # Time horizon\n",
    "\n",
    "# Lists to capture actions and states for every time step\n",
    "all_actions = []\n",
    "all_states = []\n",
    "\n",
    "# Modify the objective function\n",
    "def f(x):\n",
    "    cumulative_reward = 0\n",
    "    env.reset()\n",
    "    \n",
    "    # Reshape and clip the action sequence according to the environment\n",
    "    actions = x.reshape((T, env.action_space.shape[0]))\n",
    "    actions = np.clip(actions, env.action_space.low, env.action_space.high)\n",
    "    \n",
    "    for t in range(T):\n",
    "        obs, reward, done, truncated, info = env.step(actions[t])\n",
    "       # reward = np.clip(reward, -1e, 1e1)\n",
    "        cumulative_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # This is to gather actions for the last iteration\n",
    "    if len(all_actions) < T:\n",
    "        all_actions.extend(actions)\n",
    "    \n",
    "    return -cumulative_reward\n",
    "\n",
    "# Define the number of particles, iterations, and dimensions\n",
    "NParticle = 10\n",
    "MaxIters = 100\n",
    "NDim = T * env.action_space.shape[0]\n",
    "\n",
    "# Define the bounds for the optimization\n",
    "low_bound = np.tile(env.action_space.low, T)\n",
    "high_bound = np.tile(env.action_space.high, T)\n",
    "bounds = list(zip(low_bound, high_bound))\n",
    "\n",
    "\n",
    "# Quantum behavior parameter\n",
    "g = 0.99\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = QDPSO(f, NParticle, NDim, bounds, MaxIters, g)\n",
    "\n",
    "# Define a callback function for logging\n",
    "rewards = []\n",
    "\n",
    "def log(s):\n",
    "    rewards.append(-s.gbest_value)\n",
    "    if s.iters % 10 == 0:\n",
    "        print(\"{0: >5}  {1: >9}  {2: >9}\".format(\"Iters.\", \"Best Reward\", \"Best(Mean Reward)\"))\n",
    "        best_mean = np.mean([p.best_value for p in optimizer.particles()])\n",
    "        print(\"{0: >5}  {1: >9.3E}  {2: >9.3E}\".format(s.iters, -s.gbest_value, -best_mean))\n",
    "\n",
    "# Perform optimization\n",
    "optimizer.update(callback=log, interval=1)\n",
    "\n",
    "# Plotting the convergence of rewards\n",
    "#plt.figure(figsize=(10,5))\n",
    "#plt.plot(rewards)\n",
    "#plt.xlabel(\"Iteration\")\n",
    "#plt.ylabel(\"Best Reward Value\")\n",
    "#plt.title(\"Convergence of QDPSO's Best Reward\")\n",
    "#plt.grid(True)\n",
    "\n",
    "#plt.savefig('ConvergenceofQDPSO_1.pdf', bbox_inches='tight', dpi=1200)\n",
    "#plt.show()\n",
    "\n",
    "# Get the actions of the last iteration\n",
    "best_actions_last_iter = all_actions[-T:]\n",
    "\n",
    "# Record states and actions of the best solution from the last iteration\n",
    "env.reset()\n",
    "all_states_last_iter = []\n",
    "\n",
    "#for action in best_actions_last_iter:\n",
    " #   if env.current_step >= T:  # Ensure we don't go beyond the time horizon\n",
    "  #      break\n",
    "   # obs, _, _, _ = env.step(action)\n",
    "   # all_states_last_iter.append(obs)\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3221c99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deapNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for deap from https://files.pythonhosted.org/packages/38/6c/e240099dc857d38ec9e0eca7c020c3e18274a873e0f2ac3004e3b8783156/deap-1.4.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading deap-1.4.1-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from deap) (1.22.4)\n",
      "Downloading deap-1.4.1-cp310-cp310-win_amd64.whl (109 kB)\n",
      "   ---------------------------------------- 0.0/109.3 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 81.9/109.3 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 kB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: deap\n",
      "Successfully installed deap-1.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a372cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n",
      "Best individual: [38.394895794168946, 25.57995736819936, -74.88319126747206, -22.59022727033196, -4.624519985558719, -79.56960601181135, -24.34057846498171, -13.659610511253167, 31.195564303490663, -55.145808528696186, 19.77736276202407, 69.4117372419064, -48.74849317632637, -54.72706969458532, 76.09303044641803, -6.637888282064422, -9.744515512657516, 24.80738926688001, -68.7320038270917, -28.97387118850586, 77.88091276383962, -75.34732613121325, -40.12608616682208, -28.15666833219092, 10.79773938180807, -37.146177171992456, 54.37118142253803, -79.92809718979365, 38.16528978326691, 60.91738133433588, -36.54765914119715, 27.37398391327251, -35.072022972517885, 48.68182975462453, 59.10034605730946, -69.08259807413485, 25.878029012269938, -3.0012928877758185, -59.02618529541037, 26.547450311238673, -46.296943963744575, 62.67831035393303, 33.63354001033733, -34.16413147308402, -6.480313371915565, 17.01788081095509, -8.912634825446418, 52.22796335046888, 56.63221651734756, -41.31285040926405, 62.16458568991658, -18.91897133182988, -40.046638043266235, 4.522144131590919, -4.210375264538964, -0.6271273746044908, 29.36590263093333, -26.4015216455822, 60.34057702295398, -63.9427743626021, 40.89243018095277, -81.36768913971196, 40.29318058909458, -76.58852493209744, 1.6298285301711384, 59.019750401614054, 31.91001271644912, 47.38592230593615, 58.93145961885105, -2.4468791693449594, 5.467324963227139, -12.535771276036158, 63.728414142614334, -70.95244340917056, 49.7711276593432, 21.09290011103718, 16.433489079175075, 52.1368686430601, 77.92062076018023, -34.04600109659402, 15.147141418667275, -65.99533343366451, -53.83626784062545, 52.51750980248095, 23.15997955783394, -37.819993223105655, -61.48074627079782, 6.159099287359739, 29.0553716471433, 7.140145629676076, 0.9767683144745242, -54.10649050284896, 48.08051122009027, 53.04073893529623, -7.4279364048629635, 35.90657289849143, 49.02720125489864, -35.09238028810112, 26.051621785611253, -34.61040589868811, -35.94252994009112, 51.59138333132376, 77.87391801775655, -37.120276289769535, 33.84952413891229, 9.085776894585978, 17.5201357818722, -47.80212902278269, -29.737351075781845, -36.9195119605097, -29.10424545306222, 66.4303344330981, 79.91172594252778, -39.742745075475845, 45.397688456843454, -67.5547664391101, -71.61427146693283, 15.161054022037723, 75.99939560518668, -37.343378450667295, -73.26822934661368, 43.955148958671906, 18.71753731017761, 61.18450271130267, 21.280735040114507, 41.71897597631099, 55.745688639624206, -32.43385616942484, -37.46313676188287, 2.1115132694075607, 6.116460893609853, -68.71428741602087, 59.1537105808733, -59.385916439479004, 11.672140656174111, 67.74354483837031, 57.94844436646399, -67.21496338407619, 64.30773847195131, 73.9532828287069, 16.38337412913657, -77.56569894289908, 4.63434949274761, 78.45312600961316, -18.269912469011857, 37.87382917386474, 49.96030369968873, 27.766765446701555, 16.365539334932688, 66.70851633442076, 76.18734480032239, -31.203078413575664, -59.94321628831634, -52.30897627377221, -46.48897567950498, -29.322255123065815, 24.84213855136188, -66.65982972644372, 11.859920582633656, -22.172083530231433, 20.871944410481746, 54.47518797118824, -16.218394413017272, 68.54291302260982, 9.89991496206623, -4.714766414194759, 75.71583700156043, -78.15311345581189, -11.75528012028368, -76.55804523629223, 41.82732298149156, -61.09796344317386, 22.52810842901761, -17.009086164367954, -47.55775678061763, 15.785059036218334, 19.633288020589607, -71.31952393020342, 2.439262341098075, -36.54844697131955, -46.35243294819064, 50.65585589793993, -33.085675877478984, 22.8600702904417, -72.57792264400074, 71.67803304674136, -11.137733967799441, -65.63325594690471, -58.44549619504811, 78.94650716243045, 37.481369350761184, -25.516181976206504, -60.14674336623649, -23.406541752835505, -7.598543524523265, -50.313922827403466, -70.88868242220059, -32.549338203657506, -72.64356037065006, 65.8113980688247, 38.403739737008245, 41.60552988720576, 15.876676868856007, 62.855851248066315, 41.59674311979895, 42.723902174841676, 74.36946969358108, -67.60555544757096, -12.670225326299574, 72.16078043940513, 74.34005024441501, 48.409239772084156, 78.01913448524695, -39.14143079424517, -19.962872001623044, 15.51344102091015, -42.8673829177362, 49.02494613713522, 9.93781794209985, -58.96857020096936, -18.041605887635182, 47.676398573471296, 42.257126386879435, 28.54883068924029, 38.139717646797955, -5.642704659779204, 14.428568158146819, 13.715587391659, 0.7779300509123928, -36.30232769392333, -61.41426843943245, 5.511991295563542, -46.89357811417727, 47.091657397617055, -73.5526279237095, 30.874463627857455, 46.35505438942813, 38.04472631378435, -18.43124242247981, -43.31573104536257, 33.40771353521173, 38.336584183779, 43.07401824563748, 39.62757466638031, -53.95220015464587, 27.172261219307163, 4.2990821349485575, 63.527570949502994, 74.05667513762094, 79.23281743028215, 38.35881476603053, 17.89768973432345, 44.928251262827374, -19.397374999473165, -45.48554719393303, 57.12230417421675, 32.6235220357314, 12.193462332602305, 12.328154633318025, 80.83778853236674, 43.41865432481058, 0.9608067532454725, 41.17859702870505, 55.18452320180234, -16.294055443626775, -51.39956450623605, -71.23837715393664, 23.290597062237083, -20.577115075750772, -38.506454545383164, -71.49250806811742, 37.34231320767032, -3.873818754846895, 66.16285088930674, 7.097349604621549, -56.98192974447998, -47.63926192827507, -18.682704824828576, 43.98669825667743, -44.572223828016895, -28.952936897895707, 63.04901291246779, -78.15636399320641, -69.42799731052001, -62.267812649895156, -75.26106585341385, -32.056723762794086, 13.773529136856988, 55.54686540247101, 36.858167889662695, -11.553858898579774, -47.46017831666009, -70.48263960572389, -66.01683978886902, 17.835941744591608, -6.413101122316098, 65.11465094951511, -42.66539765266432, 3.6567628527199005, -58.776849464746995, -31.77200437577945, -24.456065670579278, -21.470877448853248, -26.476695801112093, 6.181915674385024, -52.018246353600674, 8.497930181119052, 19.53370817205854, -20.648517601474886, -57.35851999930887, -19.48400532859168, -49.66014806572571, 7.65094135836658, 63.19791665774234, 22.10101380863307, 12.404809619315927, -9.947697895715493, -32.5430321425129, 40.226129024272, -18.600666564981672, 53.61491749992707, 28.048324388809103, -44.16023526762942, 49.86552451519021, -48.68190716522431, 2.2999743480764008, 40.86964671984312, 12.664813791692035, -78.8541535194952, 79.64616582772526, -43.57534338224151, 41.19381097675772, 67.06443958423523, 5.660240990148542, -45.10786109735576, -28.212655501562658, 13.86747602363667, -32.53856966394885, -70.11230102057469, 46.24771352157032, -46.466388057894484, 0.7405047391576985, -8.516010306321128, -0.5214448533230325, 7.830273241029914, -21.845654469422456, -70.56925664582526, 8.067128852085323, 39.36117363652701, 50.42288023907602, 57.59761670348723, -18.231051570324432, -62.15573450052089, 79.9667764204162, 49.80821055338255, -64.84023373412475, -10.92176144516435, -26.234324689257996, 47.335471502335615, -25.695515204114404, 71.0666918769707, 1.7528236347326422, 52.45868957358945, -25.48043644678984, 10.75041934568544, 19.35845058453756, 80.14693457990548, -8.877284636119512, 0.139115999104659, -62.37555078325391, 6.646308290239611, -64.98020843509072, 21.146062075123126, 51.06132825417088, 49.66224480483401, 15.231661420267177, -18.42428416119304, 11.699303932165678, 45.011634476125636, 24.9340407455387, -50.98632580402431, 80.88189007817891, 30.42313479088913, 43.19080935311603, -31.787144539232692, -70.09973139008359, -5.171073930561936, 75.06516344686383, -16.768902416859756, 20.776375550474814, -13.341759856275864, 39.30526448419789, -10.879989635114345, 71.86326790360755, -0.12075101997949733, 42.27486454504965, 44.70326504519492, 16.84657244552141, 67.1477640120007, -81.61491213986075, 68.96006447314306, 54.31477344263234, -64.29945778457895, 16.753140787853855, -77.8089317974496, 4.372326641650561, 42.98648855459088, 75.58881969526139, 31.575384788640257, 28.409145270890317, 11.534334266845987, 3.1814783109588305, 36.57335087359833, -80.85378621615656, -55.87862991092104, 3.5709328768923196, -52.44686288233258, 11.16310339548925, 61.71137153529942, 24.678000506072397, 38.87058043045819, -31.354280538430302, -71.77501170704417, 0.09757921182141516, 39.720250398642904, 55.836032031975314, 73.03659004204593, -18.449854395490807, 37.8366938979025, -62.57185144973889, -70.409138294715, 21.386530736709023, -79.31518434142592, 22.26068389334078, 6.122512793562981, 46.68209777159524, -36.655901181308614, 68.00215291654166, -70.99359284763756, -73.1067760457762, -2.711151546422046, 78.86908446894209, 1.5046081273600735, 44.79114459329573, 75.86032354902494, -6.308939118647811, -3.2711074378410223, -44.80886027567176, -36.82138915976256, 21.077061688425832, -67.09704466351306, -12.673956657878653, 32.155645714970994, -17.527348537687132, -70.6853332125036, 8.526516740395913, -5.5512253712247706, 37.95551356157102, -8.623484672949058, -70.47121352337733, 71.89662151393657, -27.824593248306318, 69.02152061055537, 22.15312414250728, 19.56504505711322, -49.94097083398521, 69.46199005160956, -15.221232854694861, -39.83043860763481, -35.62564930279901, 26.385174933810564, 67.94811228846321, 14.687831885932384, 29.089440412017122, 42.236917802190334, 24.59060844626181, 53.95814698390132, 14.063590521663238, -22.833076231608466, -38.413749525663896, -32.242514968381435, -39.968206525609716, 8.452550239119333, 12.035980568274857, -50.63923136171012, 73.74498821292259, 53.93926968524491, 1.0120892143028053, -55.922674763588816, 26.562390682512785, 57.979465081731796, -47.076398539166476, 0.4385545857373769, 55.409817033298886, -72.98698562121707, -42.780833006170944, -47.58464126947853, -69.74789919614923, -11.564160022698221, -20.387734060788933, -76.84101699833553, -12.026419612887805, -66.34153213218892, -28.68709936745192, -46.28074766401621, 10.19868245945222, 45.906044206425854, -62.51897264635696, 45.62486002203994, -30.412272739458857, -45.831904461540944, -77.2019114854261, -16.71090445250686, -41.59372805741026, 42.37667239215994, -0.38876514143770713, 25.16526557268461, -67.46714184495401, 48.950847009326324, 56.91313093196815, 17.950641447481146, -3.0745358134074934, -40.80400957509594, -46.8405166559079, 38.45603035572881, -32.255694827436486, 6.3398896378804634, -68.75750431884957, -56.48156121919457, 51.86514173398933, 52.06736669266314, -73.58000395415372, 16.152256063739134, -53.949933639875454, 29.856231274256622, -78.27178795219166, 27.491464676386123, -50.1007892120645, -53.633659880122906, -5.525394416604235, -47.09819632754085, -78.48120094483671, -2.9380172354021568, -2.3092867312326244, -47.485563719150136, 62.56635136506511, -9.841570412350292, -72.46885659172874, 78.49247947340652, 13.479348111863189, 2.5334536985006926, 8.575425987602191, -32.23690604724763, 5.84446730288637, -35.84241160034378, -58.52355555125913, 8.928752145735281, 27.201709204811454, 24.102260925240945, 41.61028043862566, -76.25078068200934, 72.70542564389909, 62.27047998930942, -73.31339426711993, -69.38684752508367, -49.03401077535457, 70.02636703492837, -51.727543584502406, -56.92909857144711, 37.14903394811736, -64.5663893003596, -12.739943027014546, 61.8077254443689, -14.454032120332444, 28.42034493185144, -64.12860731486155, -9.046869180493431, 7.589836792049894, -53.912360726892665, -53.67601927574555, 6.092219103721508, -50.54306867176056, 10.03859710124702, -27.260769025447672, -26.77079660810097, 74.10153148082003, -18.063530051304394, 12.220355915408529, -63.967001047179814, 27.357747201217478, -25.89014884165466, 28.903615065004995, 47.97055753137768, -14.367256232684818, -34.58432067659204, 57.246069332250016, -74.617394032501, 25.951842838313446, -2.132290293471723, 1.8611522744776667, 62.923876711117856, -24.247063129994174, 46.2448257569328, -14.853757075879836, 34.227969762780326, -65.63490105044787, -54.17679414613245, 66.2539658911916, -36.371030479201494, 20.520709153138682, -58.916154175208305, -70.88624653976937, -19.297645063520257, -75.487282433483, -32.071212391619085, -10.77349043361603, -24.878616894706543, 6.730189012787961, 23.524601772307776, 15.299151248960907, -35.20895718592005, -24.141060514750126, 20.978003189543507, -76.54987476909855, -53.40178351869755, 70.61654311317118, 44.24597843931038, -51.41937153054773, -10.32218537298525, -72.11809483324804, -38.01874785639791, -58.63452296784206, -25.98258986931677, 20.640891384902726, 59.26863823554137, 56.56122508954723, -18.676679051239162, 70.41126424304355, 17.935447985598636, 37.67960907018445, -77.99487531987911, -57.22107649434189, 58.25138575450535, -47.21608296963864, -33.82286014861656, 72.41445067893275, -38.03262564496054, 72.19810853338879, 17.95025158043526, 33.11773928145601, 28.63805427432056, 57.58726452749939, -28.99175226781898, 66.07729434169283, 68.94470009524467, -77.1169362996551, 7.17224999616532, 35.802268623073054, 71.42732270391188, -26.703394197610457, -41.268898708408024, -14.031380362002455, -30.03712968635201, 55.68543826349809, -58.33332284432847, 66.51475736390266, 1.8203201623706486, 63.763022021805625, -19.355684973911988, 5.488812632307151, 75.2624961184473, -47.106402441274874, -20.315183204529443, -20.968754444172887, -0.04315773665665701, -3.5985926623456272, 2.8441077253124387, -34.318870541725595, 24.986921028064952, -53.298161565778614, 63.88180665964177, -49.51333771890588, -65.36954940370433, 14.075835968265318, -68.19950732238513, 20.933360590940026, 67.73421312781713, 78.05131169902674, 16.998526908814863, -4.794472075459176, -23.047675564852586, -71.82550684701485, 55.10092784974385, -57.08417615589506, -1.1917514663663171, -7.8263521704815835, -20.751581317695614, 35.83075270153264, 3.538077153988419, 73.01349217966613, -50.03024727148216, 46.34378017736361, 67.85120485134526, 16.263187822611847, 47.205433981136196, 55.863057066593626, -44.24339070856707, 62.93129282862312, -38.01648542444782, -57.57549926480367, 16.31537486670508, -3.985613716042382, 10.698465189045551, -78.05105074696493, 14.42060908758188, 81.17690692725964, 42.01227872478887, -19.019089753395853, 15.60807216128266, -15.442795619176097, -24.98365857600262, -75.67283674265816, -37.190864318801154, 52.82614688416563, -69.74492571219167, -49.3355531951926, 22.30409650496612, 3.760711711890554, -74.54150921089959, -47.634835531508, 72.16353150561432, 57.679280566443, 58.458158330473935, 51.8040139663358, -35.28657556740125, 63.823248624336436, 59.79420619962778, 61.16695547042443, -62.169755299876584, -63.35760367885418, -11.517613458989617, 30.912476213180618, 13.679087810628085, -0.32088992583051934, 52.2376354543862, 39.47597450744759, 64.52500663850881, 23.875275917367887, -72.0058856853684, -26.560586580705728, -74.86386938246935, -75.79855185320923, 22.595527635864656, 75.59513391646327, -21.466877084544755, -31.436721182600664, 9.607973546465663, 18.66272600483015, 15.757471025658262, 19.332569714373218, -51.69693628319059, -2.370115387452323, 12.411901085879025, 31.573847028931063, 27.31760490795105, 63.99338801805801, -71.40674896222815, 73.45352265506911, 43.81836973752817, 75.07395708508288, 45.064890106121595, 18.897446200441177, -52.10897587167256, -11.611784608476192, 1.4786321854375415, 68.63045089100744, 56.55603638496354, -45.72652821044723, -20.005525283850915, -2.512843337433881, -57.1562623006211, 16.274452429938606, -37.219454379357465, 54.4582013158786, -68.32410242461052, -67.95664980330253, 5.456841567588552, -50.26378631946635, -36.40936037004095, -11.297472542688476, -75.13690441562511, 1.7951960841684769, 50.57800510505527, -33.49906233205086, -43.45797259942348, 9.20577800144617, -45.92575496440047, -12.797381458224988, 37.15565298990508, -52.969271366454, -54.723099746447545, -2.7036631151868233, -69.17363480296075, -49.096334787248075, -9.437306962684975, -75.0412415038885, -63.018636354610805, -8.44457177480962, -1.6048447222744602, -62.751619782430396, -34.864208572056924, -52.25707439317718, 60.75292598878233, 24.867997235874245, 0.7089248364746652, -69.21534919819211, -24.23667013010561, -52.61329435596666, 21.381082270762793, 75.49907750380547, 53.415903221159795, -21.470318215154762, -48.53656337004619, -79.07636205051412, -71.95579588306768, 61.819190886344416, 0.6930281979700084, -58.91960934963786, 12.872289515275195, 28.967538699725516, 76.8351623848263, -29.80043042325093, -46.17091827795104, -54.54279261629489, 50.89578286244351, 65.19471605746627, -12.378069247360687, -23.18300206162641, 75.15951088775759, 55.04531632956049, 64.74951280572, 49.49428008446272, -9.799869547163853, 14.929342657203462, 33.29509791003833, -76.94102028199019, -73.9961210560446, -0.42390948842263343, 13.392199688994829, -58.75775472825869, 62.83970817933661, 13.804825989542664, -71.02246236758558, 36.21501005492698, -6.900951805278261, -9.745240313689012, 61.156455935792096, 62.201527639791465, 1.5333633950985168, 12.919407033555261, 45.99797561350904, 27.002360386369407, -70.399318328503, 73.64478623771646, -62.50460052660443, 56.0262262752965, -45.323015999926525, -75.8064970221828, -12.032958562588666, -75.78856136810465, 37.54928160413599, 77.98381447620432, 62.58156701205989, 16.191963074891873, -46.67793927117258, 58.76841095107634, -77.69156199341063, -56.421153701242396, -51.41422822533214, -69.3328865579705, -68.94516383872094, -41.292634224337895, -63.686985491913155, -30.458694848924356, 71.81722836193673, 8.43950181744077, -43.92780107163175, 24.359945784258954, -29.398357561769462, 41.13938320913276, 53.08831576945049, 5.294223064899358, 11.145937031153409, -46.93836875549033, -6.577011385072165, -1.978224981171067, -51.661445030019294, 3.3419854800459188, -55.45060797124265, 9.38299992250487, 15.194768621217635, -19.14343751899248, 38.54985109836706, 64.01352123327369, 35.352814979382785, 70.45522403969167, 78.74978213486122, 16.781248186375024, -10.436697581506829, 66.81126592038949, -62.368624880878116, -60.33207573269467, 28.539250371966133, -16.740727985794948, -15.255259441640387, 63.00647246278507, -17.03952737575156, 54.39949248015034, 45.80375363362966, 52.230436056640734, -46.18657850156339, -76.14939802413335, 48.937916349064245, 26.895529515953623, 19.602580366673866, 55.87208102824013, -28.438345898501083, 16.49701987647715, 73.73672295645946, -57.82372690901334, 65.75341852641006, -24.07245243428757, 35.287345677926, -17.52595307520609, 41.649833460611866, -36.780061333147316, -17.597243701336623, -75.09402312400545, 27.079511439780504, -46.53803008198966, -79.59900238007512, -40.500911552400744, -16.000923689630774, 58.517272777356865, 36.651692172213764, -35.93646757102914, 10.606562486100975, 36.88959492650063, 20.84669138320846, 29.141599094641553, -2.7520383015053356, 50.428848049329, -57.6604054713943, 70.01982161185029, 70.21135972997759, 58.4933318333133, -67.08557714723216, 40.80232434628779, 9.372149531954312, 20.34324247398373, -42.454489345580996, -11.160733752182036, 1.8736262944210074, 63.81355571010078, 21.098984107575838, -1.1891510922870818, -73.5963397360559, 66.6260396090688, 1.2553817039074542, -36.511629153545485, 76.03097527698175, 30.004139365286175, -49.054926965013905, 76.44763631868261, 25.578467062924823, -0.6382333007711001, 77.82423340686105, -43.64104845770842, 10.331439927337144, 0.04903247631311425, -79.3908597004012, 28.269769311985947, 2.1654067977082345, -30.28596387726936, -20.85318106393049, -32.698297204224055, -53.521792979885255, 1.6887769186521542, -42.53614929051034, -63.84237667161973, 33.22497828523878, -69.15246583725485, 48.556379733301156, 24.79178228024367, 77.83079106549978, -28.906719612796287, -4.331149738693693, -24.69041681494923, 59.721207736328225, -35.38637230509359, 37.563924329062246, -23.156632669387655, -71.74865256693651, 11.724213660889898, 28.951457877595697, 9.712137856183773, 69.13394127858524, -53.59716789253987, -39.33673767129373, -62.65853508385547, 3.1336189572721307, 29.347511803650246, 67.62081017475504, 28.05308507465527, 35.27686850346931, -55.33770566488106, 9.53617793646797, -14.213954384194826, 2.244291344701251, 13.66465932732556, -28.00291402479396, 39.014758929717935, -58.09760961702757, -69.69983142427239, 31.69116091182341, 23.094005041306946, 9.907329520253148, -58.94417438021855, 4.958288917251125, 63.48708186443858, 28.52716278246969, 54.94968523250463, 39.11175997682636, 32.16506317638971, -15.27351949533483, -30.483079109539595, -15.191052383288202, 73.0911582711712, 57.56403154502453, 17.638058385244747, -46.635723515373435, -45.24925480626425, -71.66985046493477, 67.47951052513808, 59.2362146940617, 3.881492442370596, -48.172789832348556, -7.867454071729391, -51.996617477957, 2.9924177598358552, 35.11230768351145, -71.3372483995972, 75.1263880987459, -26.22071878432586, -34.40741967438376, 47.70989894432053, 57.872518860741366, -57.60769689426297, 45.15036100234313, -44.105654002705364, -75.5542145914187, -39.12363910107547, -16.595902811401235, 7.546259902121562, 39.50928184147574, -59.45688202310436, 43.1797358801825, 45.68614878908065, -71.84121539540311, -76.86457456032824, -73.26904707159326, 53.09082544144614, 22.85057996562824, -43.874370719570514, 56.87881893321595, -73.66242807854199, 25.03884561474838, 63.0121329365429, 76.00601206677906, 40.85765360399846, -43.894883866322836, 59.19447577567475, -17.000351159061786, 13.51919525920175, 30.97971333846178, -44.771273793373915, 49.90773245340203, -22.47977460054961, -42.764774004527524, -75.70039678064693, 74.9739912698216, -2.90286585611094, 16.46870576509187, 63.12154223041081, -78.15595078681318, -33.65442110116805, -45.24544458726053, 52.02889482999266, -59.778768976202045, -6.355259892638658, -43.29984026871284, 73.59241829392664, 63.75584469715649, -39.73880115947387, 70.68502213794383, -31.016235267043367, 57.57479231783328, 76.64856631620718, 22.082734409567337, -22.035821650041644, 27.315399195621108, -10.69489587128261, -50.864482236685134, 16.05545735461446, 55.75984730199952, -14.58842164639538, 19.082615806493568, -5.708476603723767, 40.51959652595833, 16.12801817515964, -29.847065258946095, -10.022917787111751, 26.178814788275538, 63.51484580393521, 67.61614907029809, 5.72537759600989, 61.357123434987145, 4.970173270253312, -59.28144599782783, -72.11750337214818, 69.71357950871007, 8.675272960567405, -70.02255545319181, -12.528812493269685, -27.64864951363472, -38.55135273569173, -22.374920996366566, 6.508755017256981, -23.73426539798386, -19.027953319327786, 61.27877127597812, -28.187655192993734, 45.736425151325285, 2.9306923848837183, -70.50450906261933, -15.048252717530078, 44.726884717628, 47.73545313423233, 16.865570632331067, -43.31009963975025, -11.366157270176062, -2.6075331846505, -65.42965262154769, 23.974766336182142, 10.394021802307064, 20.938466606224708, 70.89443088412685, 38.00750129989021, 69.50051487492992, -68.00695157003855, -26.490171767664062, 43.34835430509289, 4.505945758717877, 57.195580390020886, 38.98336308315959, 5.244704977029073, 12.57156968774644, 3.3640133688715554, 1.013678081619952, 40.99171208905449, -37.94160645056351, 51.770287794467585, 24.75626371500148, -2.1055153910256252, -54.37102745701743, -67.16378796143158, -0.339802084675475, 67.6537181844165, -66.39418106044529, -35.99436962906811, 3.4462414865755733, -55.64170500229106, 40.16402410230588, 8.40837126999926, -67.05442057475186, -35.73876677707447, -53.28083597215075, 63.22578932890484, 58.390983787533855, 0.5991482314590836, -42.10087624583951, -30.659908889351343, 30.71962501675507, 64.6835096266199, -76.62105945120135, 30.87132287263486, 21.223780004534877, 66.71626730610241, 53.17496141621526, 38.57148023238804, -17.745314641248473, -20.86344465759154, 41.634614351394404, 73.11235184471147, 72.97691467372724, -67.84514237301265, -74.5254213749384, 45.25289948968196, -26.186500740017316, -28.55977616610132, -57.20869701209694, -66.85833496633002, -77.20032476026564, -0.5376826235152379, 14.184794385771795, -0.737944278267108, 0.3189975771443172, 3.6710161887003516, -36.34035560012009, -55.38127183226853, -9.864287468489774, 3.339260387095077, -48.34291463488159, 54.47851722128513, -65.20625514212276, 7.97361731074912, -62.60479535759913, -7.7543725436993896, -32.65301150803243, -80.14904723847819, 35.03440222243615, -58.388478690276905, -18.39246862156231, 75.68397610023871, 7.337526656778394, 4.55663296211975, -36.29983766402431, 38.364400666397735, 44.77164400725452, 41.32417284565478, -67.39469209315222, 29.012792199046125, -71.58611052126444, 10.779412801772459, -24.956323404505984, -69.47233441681709, 65.95236500728069, -27.289706556857475, 42.17540172853042, 75.67876210466937, -3.677599468049419, 76.1382658189174, 53.192556526350984, -43.467006068357044, 13.585230174704016, -70.07451839011814, -30.79863822438439, 76.48164873033457, -51.04366156165278, -71.47170411676339, -7.557352154477526, -15.797813639402122, -73.01413630716024, -52.08823076129813, 25.23666319327827, 22.724440957573933, 65.84904560974302, 74.45970133412214, 18.540316157406696, -64.62474592508744, -32.40604789125581, 40.46201915436735, -46.028224474110495, -25.977684974974668, -0.6556801480953214, -22.959624657671448, -73.11858719454415, -37.998150948477736, 4.316465632675624, 73.19275155566442, -78.21552532396026, -61.15957783268646, -41.57431965879938, 51.41891705429143, -67.32722875455286, -23.92971408019283, 70.32822563329029, 13.5045038540059, -9.852065801327246, -56.3529545452277, -51.18815208934218, 62.21413059191072, 53.79878118439339, 36.772405447877674, 10.414554240246177, -37.29611300292936, 6.846414564326267, 73.9219994711855, -58.28028301686174, 39.44775166125812, -77.89662005587925, -60.6231763832574, -3.107629294596153, -70.68382850951002, 55.66968026706883, 76.93083421661741, -40.73452643192387, -19.76051260472087, -68.98486841170532, 76.19394587220843, 67.46052864405027, -61.27217832564571, -9.79208805068805, 63.981074057960306, 9.245229635079484, 45.585964358733264, 19.64367283805379, 19.871221726307738, -16.061567693393624, 2.5899087734657087, 51.19482962547873, -2.0203339205473583, -10.18011911030714, 75.1083239680615, -28.40694817666289, 43.625451304219766, -7.470564744325714, -41.810368925784175, 39.199339028686396, -55.85414969420653, 27.43719709152606, 21.18748055641791, -29.02279862350867, -14.946650094485683, 37.50082019293255, -80.28927903444105, -45.82658959712983, -71.39822003516674, -75.48843635981595, 34.62288678509033, 48.82450662734425, 47.33936834069816, -54.17740375663296, -12.201555503262641, 25.703752680708106, 52.99337159482502, 70.79062079167164, 63.61549122699853, -54.99019925862695, -3.690909595863239, 40.89665044399163, 70.2025871960315, 0.4315541385550633, -14.60306124102371, 35.336911743340664, -78.00676221921174, 24.305254152052186, -63.9846429699495, -42.26030589011812, -43.18054151964232, 70.94048682293544, 58.247591499046734, -2.7398937027654156, -53.573694918469116, -25.159704058678862, 26.773004892525297, -65.26249816499238, -10.556464978863648, -21.650596247505582, -78.894330404035, -55.522528800355744, -65.48485845598174, -27.7751573662541, -23.162414017945522, 46.5690833280658, -18.832431793722844, -1.0628651397086486, -57.50518348256963, -72.03319226256478, -76.78302501461866, -68.49580762906577, 57.58879971462824, -8.503874274480435, -26.379586634380974, 30.695420351661802, 17.03862609533249, -47.732482524494536, 38.12788621163371, -11.4743430322374, -22.805125934498385, 30.780145764566495, 60.391987718700726, 0.5624852294138684, 18.984840638539357, -14.274483264019327, 73.85729651761926, -53.765247446533, 56.70225920775398, 77.51953068128617, 41.3079185287391, -22.31678528841872, -9.835168228930854, 37.53419885417937, 68.42466898991823, -6.8808245337421425, -58.25257591122942, 31.068058061059805, 74.57318504785792, -36.16280298659832, -43.076134987520994, -39.634325343333515, 7.283003622362194, 45.14269078862705, 54.034742200883954, 58.06269550309753, 42.64586861065352, -8.057908207110742, -49.68903689858375, 64.20509388170274, -7.558609532996632, 6.342766995753581, 30.413264671167106, -26.027533998016633, 26.784342090229813, 16.76399978918428, 9.460005659255774, -58.308814007043445, -18.02511513883259, 60.253758853970425, 3.1926937719355797, 44.96735084715839, -72.97901492644247, -0.4966687456535653, 68.50589200810428, -50.08762741724466, 48.89691947504261, -6.502805055707413, 60.50539466045212, 50.85673274617304, 29.153126951077773, -15.768529960891149, 31.71507908958452, -30.74898443804652, 4.230194337862868, 21.36118476660407, 62.07799298364276, 79.96600908572586, -31.471041487686993, 17.75404217206628, 12.672543286582616, 36.106248480761316, 76.18113204666635, -2.1338601081421302, -81.15606493271065, 35.83240493851778, -40.72262687600081, 2.954750691786105, 7.706481550415561, 25.555139482630423, 34.54522127462598, -49.163471919746954, -79.16083806399362, -70.90194690090158, 54.74498767415734, 16.227316770284578, 10.210744727023837, 19.497630833815442, 75.40170510719274, -8.921126996378744, -28.398348610176278, -44.11460868216711, 58.628991258204714, -17.682887210832355, -53.53414467339164, 10.84746179118321, 58.24412676337914, -49.79437617444528, -13.88248217688114, 20.506039528551554, 67.04251856121893, -8.734929578295992, -27.553078060666767, 75.87790427392892, 50.99163081116261, 42.35131705433009, -27.428479921246662, -14.616275163800617, -38.354670489948816, 8.765745834498528, -7.138931984051046, -58.52293019021343, -77.929728869621, 4.686402140260796, -74.12671542659724, -68.89173381767895, -15.976223891967072, -44.34271035308568, -19.732054565311795, 58.98388041193598, -37.27005548381636, -26.407749166819357, 66.60590399684982, -79.13425160015319, -60.01148332656045, 58.142753440818616, -66.05216102589, 43.941284650520004, 60.02155151376843, -71.0382944336568, -11.620520707945573, 24.88272967791604, -11.434015797768367, 42.42739019635195, -77.67162409233647, -46.00595727470434, 66.67416029903369, -28.522186905425627, 39.31066268130161, 7.96066923728452, -39.37520896641949, 20.748559569005145, 46.765447329702916, 21.528621577650977, -59.26837399963755, -30.705577614182882, 53.242491793895795, -68.94301335548471, 11.434869300079779, -20.15382244431615, -27.034265399562436, 66.6373590340823, -67.45132983095726, 66.86389240619387, 15.277448189873702, 9.578043612113065, 3.6712440731826392, 33.72042397934385, -52.75592342880504, -73.95918582080432, -43.645789194257055, 36.225038889430856, -44.675021485858686, 20.06669250575232, -16.6096597353226, 38.15523769914907, -31.070378597887636, -34.637609570694224, -52.67887688318899, 4.132694825711853, -54.26315604950504, 41.025618231888885, -48.25743203639425, -22.099828524159662, -17.221986857225883, 0.18069877789015654, 60.53540263685342, -18.20276370104773, 47.219789956293084, 56.48417757071874, 72.22739860227671, -63.59520518662773, -14.093191121417721, 46.0576245825577, -63.32607253287414, 11.787556326439013, 61.09592176522287, -9.014456619672261, -10.568387389978422, -54.14191310554337, 38.86528330095447, 18.211085829538927, 18.53947759769944, -41.56836733717147, 12.118876887382656, -6.150154250412669, -37.62475413861285, -62.41950525894626, -72.24307774264744, 81.75264138896338, 77.59385420260963, 7.886225195810345, 20.1243240707702, -35.22803456105994, 3.0726986485613805, 14.578640906400633, -3.8880156777770507, -74.99314770423457, -58.53757610698307, 59.79290464849159, -11.808721171440366, 44.24070999892744, -48.50185710364392, 37.568156470544416, 53.17346010123892, -8.288403203831383, 42.80846951565354, 20.214777484073853, -74.61361729896528, -38.62127949938838, 25.403119434361678, -24.58239059163905, 32.14926165160901, 81.20245805884844, 36.22687068918568, 66.63163327089276, 3.2018735145009343, 43.89371056832745, -59.66507652179596, -34.29781713093325, 12.672788886907966, -0.3437528701806088, 64.37084106825938, -16.75124688727803, -58.53390941281434, 5.170907384977647, 42.24016702112117, 23.06714996774936, -19.803449372287545, -77.69158029845141, -61.13960336113032, -31.28799657847802, -50.15167698672223, -48.641888192276845, -20.14676214830415, -62.71413269831304, 3.7132182003256418, -48.076794420592186, -2.4905642965480497, -11.356844578142338, -17.050898293807023, -20.82111938669549, 68.62449087903549, 5.739004472457152, 50.68834099194848, 49.38204614286212, -59.39430137234001, -25.4884865588256, 19.677258842249472, -14.70511900493306, 70.5720250222136, 79.40220532251334, 2.764246320199305, 39.23048969184386, -59.06674223697477, -76.92049494511042, -4.1377474114578385, 37.11053650282681, -54.991729144847994, -6.318655638335233, 0.81261387622527, 26.025920448946366, -71.65616154676735, 57.908211058538605, -63.24855743685544, 52.91991473878687, -14.516104912563156, 10.89385074514452, -17.045649661026008, 32.658482428835484, 63.27602630289777, 14.352007559378775, 58.86338926626882, 27.075570894378238, -14.516690644697468, 13.138485671456934, 50.2957848672591, -51.70236758537124, -39.911596678537954, -25.73542099380521, -12.840151354753083, -61.84366223476466, -57.09895943881888, 0.7067411728239452, -40.78549967447886, 26.845217376899893, 32.12616959148275, 21.346029942637184, -68.73559073248653, -5.204811461357094, 18.12383767591114, -7.768461325745884, 66.84837276516392, 40.987514316412586, 44.57154134362773, 9.734925909454091, -42.5250363263944, -56.35215562126933, 5.280754011732114, 40.857362872369684, 70.96457623846334, -63.93459423805784, 44.74452077608202, 34.212639204541155, 37.73642593951416, -61.68466398421528, 66.61768416872992, -32.48232324384876, -58.12723678872434, -0.33631416017363275, -10.023885462780939, 2.896060390148449, -4.559636444389216, -46.557313599143, -77.6968594698985, -38.60427922958206, -5.197323580448625, 71.75359201298578, 47.50714547642306, -27.115285252983202, 64.61078731026558, -53.88943969861728, 34.56023505551484, 35.94028166421735, -26.914416954948244, 76.47247095068724, -49.80729560831146, -25.809707375169317, -40.652500006087266, -20.731153449366282, 58.149414126580936, 19.35246705899192, 64.3542152968859, -22.799150184271475, -29.96551369378221, 56.456082462421065, -56.39883178775412, 43.86520186960855, -34.484406106500444, -66.6297306422243, 68.45177911083123, 11.4731822713557, 46.027201927613035, 20.25699317047524, -14.65152722228517, -38.713571562772636, 21.625712305933185, 32.30761135441078, -64.01002936106391, 27.16557000955494, 5.48996934435123, 56.97491567766984, 69.89980789764336, -21.251479551825906, -5.264015321187304, 78.52410780888965, 34.39573055122772, -47.92194093131526, -4.383070825391044, -37.64319350595274, -26.130445252352136, 12.28998355425397, 50.31741927247732, -41.43897405991294, 25.749176008125982, 72.40032887306597, 79.68272134544549, 55.125686607990225, -25.629979004066875, 22.170422753979167, -40.72405892024389, -16.277196436253725, -57.66100250044419, -68.09804636644365, 7.436348536876999, -61.286347998467065, 78.96689172318165, 62.23353381122228, 63.43739220209443, -12.287799897715967, -78.89109237749977, 23.130069082484027, -5.0017103372124545, 67.02365235999201, -48.19811649338542, -63.8796645407405, -6.0397312685738305, 46.740979594761235, -33.125238792136, 73.4528313205045, 53.260119018040534, 62.14638106220754, -60.1517894655764, -47.389618101227086, -61.35927114536358, -77.22297413279102, -65.50967326152771, 62.54982598004682, -9.80081854512054, 29.89984494876548, -60.08889318658887, -14.531785335964935, 18.78666212627097, 69.1123524267399, -0.7682922991737733, 70.24072960709381, 23.492459596008302, 18.582231674174338, 34.42958300508941, 16.793574853354333, -16.90070123823258, -28.059161350826415, 34.356393044980955, -72.86577382574502, -6.136704929689615, -43.99719873764025, -36.61831263382412, 57.875161838355616, -27.849458154058468, -0.9905293948280018, -67.45181759172515, -16.047936895599953, -23.07720017486856, 79.86643641107497, -42.331145023974415, 2.7738714319946043, 65.14415920557984, 14.674954924318715, 6.2718166648426035, -11.640096811541671, 25.89944206171618, -15.078739102588205, 17.454349599326676, 50.496504744705646, 67.7488682692974, 78.82037061527735, 62.13684254857682, 50.73979432971913, 16.616089639207914, -40.383616772376804, 35.28446628770949, 41.422299234288445, 13.454552888505905, -0.7786083232829312, 40.64462525944438, -33.110662273598166, -4.03313989501927, -45.2653941681635, -42.73179494664854, -59.899291944819154, 63.698140946958006, 51.70731538398839, 2.807815868978634, -49.753105228769975, -8.268256956596092, -27.066014470527413, -73.57500181716262, 10.406296214694029, 34.55744881568221, -6.383043789596879, 4.176524384368415, -62.66811363080009, 64.81698906016258, 69.07899824995617, 46.31145612195843, 18.218398571101265, 55.28767193535063, 32.53810528143185, -8.135505469405498, -20.757261555424964, -68.94644600733868, -79.02211726229596, -30.903403083118796, 35.95456167817007, 64.77959448408055, -45.36339063020428, -49.33776054548113, -28.3640595171734, 68.31652659217855, -43.61146127989776, -24.412557097144465, 24.478475905838458, -62.12949952719422, 26.864451333603935, -6.14994025172358, 18.601900801809418, 63.27863263968069, 70.91737234092825, 49.49155632965143, -74.61033920619192, -50.38557637599504, -10.090282360316941, -17.492317773622897, 4.365962085730023, -33.01026634184125, -56.78006393621464, -47.14939252328317, -4.0744998842842115, -17.44821339526605, -43.525155029586195, -34.08857939962829, 1.693841182143433, 55.30716872112317, 54.6941454906498, -25.101823798198613, 40.82765815460906, 61.5841485319074, -11.658454513801992, 20.957895298435343, -31.229070596742563, -66.64604049305638, -15.897397616225408, 46.490820824879144, -49.933289731055375, -41.37310817479081, 79.17699238255926, -76.22403082176784, 9.488287503765989, 4.286546528330005, -69.22262433630964, -13.236067653399516, 75.36856547405924, -64.53453954077695, 21.84616098979504, 62.59648669623869, 44.382959305356124, 30.15571225605208, -63.68013390104417, 67.62739034563758, -5.3865362358115565, -65.66852299960738, -58.012137815026826, 40.55846218144224, 1.4738086757131676, -49.039074605640145, -23.308024413572976, 4.959982152085924, 75.10440891817885, -77.83715114455255, 50.689289842083895, -69.47795944310296, 77.24323826999719, 40.35368993221121, 66.80674101772982, 15.602921081749347, 18.50113507367764, -3.9361820384226593, 38.42355566602433, 75.99587728624455, -39.33796008861742, 67.8546837932722, 37.044184869268214, -3.171954943125674, -9.857854409787606, -64.62089017020848, -60.89172002750272, 6.268624378210147, -79.34167073235658, 43.23121344692322, 72.53746486497431, -14.204570707687274, 75.40792439941835, 74.87031949693406, -64.26465754124303, -71.2724157059774, 50.72742043680124, -41.96886680287929, 16.878741509219935, 40.228579313052805, 65.63921019095302, -21.794917225386754, 59.980934528273906, 39.67778131943683, -3.4462910026326057, -35.82267962681189, -25.610802531472384, -69.46449377173471, -22.906622767967637, 76.14382444259714, -60.168042165202365, -59.599446208551356, -17.772752575419595, -10.289844305349298, 8.765819259072401, 62.059249059457606, -53.685386823435266, -55.51328124618104, -65.32082516401614, -39.405609180963225, 43.24782397896883, 35.72256286419994, -22.283208910797242, -61.11470729182521, 6.342058137373714, 19.426316097568808, -77.97563340318419, -36.3205590726957, 55.84583955450241, -64.4721390159384, 41.86339466969773, -34.18748323458031, -18.555318794743055, 21.86925954765622, 57.01328646968999, -44.52217348579831, -77.98105063293382, 61.478197469616646, 1.9160855536468213, -19.581012221503915, 43.75295840427101, 53.817005090593895, 45.20295278347462, -80.32802113343898, -51.46707770085847, -22.465040277643975, 32.996716927675976, 8.32381320459357, -54.90687742735192, -43.58901779323537, -47.61591903130851, 71.01152874150324, 49.488966855867986, -59.08710231190707, 75.4559351527953, 64.40033389794597, 22.061887724966635, 56.281384771451634, -68.10841833194381, 7.312874084474429, -17.45897793683645, -36.76235064985774, -73.84855341290627, -29.16206613688613, 16.57154849699019, 39.95821573371284, 11.513836437208758, -37.82501241089383, -76.8343481135076, 21.659243928814334, 34.71216451567427, -31.240012909320797, 21.428089989340016, 38.807878370709496, 35.31671685684858, -10.31510321162236, 34.84002396497039, -32.429092439195344, 68.04901486962126, -42.50699149028692, 47.20296301878942, -54.13170621304022, -30.635980004148678, -10.53366332418955, -46.53606482215264, 24.122926433094012, 9.6677370727611, -48.12334929288957, -47.3196932755643, -31.947365143907916, 71.88664961296483, -12.263387208773374, -31.46221530470195, 73.09820743619127, -64.99243899536926, 15.832432093864918, 1.165767288276379, 47.32894813007486, 54.284084009015395, 49.32743692271767, 42.04957142792018, 25.322541891521485, 36.552521389063365, -72.84001977866048, 45.97605078063569, -5.990470004001971, -2.454895257368715, -72.12377656883059, -36.21445633562524, 50.6808540843901, -77.65585860260059, -40.12921824089374, -12.647961386578334, 50.674001286843115, 7.512671933182168, 20.47540387482177, 54.03323308096099, -41.6597628934547, -75.58767993371883, -5.257317003699521, 2.9168187556754503, -23.687937121526677, 19.181509129580135, -43.982507860504334, -21.45000867828438, -14.42604138510642, 57.893315289789285, 17.0496495935328, 40.13079976958052, 63.72782353128818, 68.74203406801702, -56.212038453672456, -12.004989072924172, 44.25369035409405, -60.20812714251487, 2.712588354122544, -21.7938865405552, 34.33747480739248, 14.21620529483563, -70.06000665535203, 74.59899306553233, 65.42414528787049, -21.615142792564427, 25.539872352698797, 5.225881794427297, 19.684784873183176, -21.63035839466095, 46.59903150371134, 70.32091024592029, 78.84724157224211, -9.988773430686523, 66.99984850611864, -34.109780280827415, -40.91446857546469, 48.69782708282787, 61.63733579366951, -50.09620514416201, 77.09784385491018, 5.9748482660762505, 47.473675955562356, 78.54670648060022, -41.30555548912203, -73.42733100874591, 79.92893182753255, -3.416813181219612, 25.03751159081957, 1.1417382307674724, 34.64949001772737, 78.15479480324241, -42.65395003248783, 39.74131855969066, -26.468547781180906, 74.44974091289579, 42.02892649693252, 49.5608424558011, -39.98325390948239, -41.692467779359724, 28.669800458092784, -30.40930487393208, 40.22810590051259, -60.35423376775223, -69.6276683684174, 26.88908001514079, -62.8105818314485, 3.1895178203053085, -34.40777497833868, -44.6177768255452, -24.221213769496025, -61.34581031486921, -19.45251794698153, -0.07656671430961592, -22.747340399703358, -47.082195820980274, 76.06188171483959, -31.065682075126233, -38.16429793717281, 65.28215252298259, -35.45395471824673, -4.340246726657654, 43.588227973637046, -76.81108762640174, 15.853720613147715, -24.69514189917741, 29.370052952415566, -37.95306797502318, -18.59527874184228, -18.024804673310047, 32.18370068693079, -71.01224904455627, -6.125286830571085, -63.84542871135928, -22.779883413410936, 31.686282038069855, 75.73278028559305, 55.06477816303556, -49.661735479423044, -57.35010370598512, -36.67550144790092, -3.8376583137838622, -44.543159667544906, 50.205621823317465, -18.56896196001783, 59.01243002909906, -20.48994878383711, 26.74416023366665, -12.592066913573397, 52.93955521369775, 37.063993422060754, -8.761760188567475, 35.058421529743065, 27.496336744844054, 67.6701379766194, -71.81703463482489, 45.381415281059404, 52.88568717847238, 66.9187495029885, 56.49625063767985, 40.87677092260152, -58.56836536896205, 47.112002333443115, -75.761571308448, -44.04294851288782, 16.584375719508728, -30.442473040883637, 45.21497417136642, 6.538759564999864, -26.531271762416065, 3.2171569157467497, -9.508765510533182, 37.47283610113169, 52.356875539114846, 36.95022357765577, -70.83506796909253, -71.77083395832712, -22.24129951070522, -37.1653050611032, -72.36273154874034, -33.391893818593275, 41.98146239028469, 79.73249595919246, 56.264328036460526, -52.207679023675766, 38.30785203969918, -40.03903446476203, 27.314259694650055, -32.440400226698856, 46.71786365690052, 42.156305654001265, -52.360050687147165, -12.835949606359918, 58.466743691748945, -56.81793598365374, -46.12431147611445, 40.41066823690464, 23.45997482550304, -52.24624379362691, -40.024614968554474, 7.727047423800741, 8.462569434144996, 69.81466786771551, -43.485822337458714, -31.989056961048995, -66.16304481468266, 0.17992508442022362, -59.629829681706575, -47.17589059590955, 11.06200899564484, 4.908483120998213, 71.45341272078036, 74.33169620141125, 23.18324972176225, 66.94753544409875, 34.78728817767808, -13.054923639168281, -25.898045353118174, -77.46757012174652, -6.7826682788552555, -77.97025606710298, -27.795426757849803, -54.96374403510871, 1.6901372626953457, 30.96476020718885, -21.754283111848917, -63.99724585655889, -62.82140720832586, -24.354485220680985, 25.120590633794492, 38.547325638921315, -25.630370636167743, -71.2871363518938, -27.287098271088166, -43.5215091902735, -8.55544077329003, -54.178515182013264, -68.1843071300126, -66.23320838322387, 74.2517121503132, -24.9856706027488, 70.33620708985596, 41.17814377632066, 59.01726818560363, -52.016102067936, -55.15589181709125, -5.987084184254878, 62.16682678685104, 48.08324627040002, 42.76630119431315, 47.169991495376365, -78.27540413462953, 41.65527025489166, 20.661268922894706, 38.97522089785288, -69.49184487788577, -2.8052311933405747, -7.286510946831826, -46.20990894895846, -75.82736083236934, -30.088548605526167, -54.12157977240522, -5.669553800484767, -75.25055001961537, -72.17210346218236, -25.589480385573314, 35.06737232983125, -55.095116431550395, 47.537529565073044, -66.87291094968079, -18.996373076440378, 57.24673951563562, -43.03860172456561, 62.04396786968532, 67.03995178316308, -35.13769282761845, 30.92701977041456, 28.82297659688758, 50.57347912655017, -23.50965241308231, 18.98587438414983, 75.76829268001455, -71.95940701188873, 32.48825350359537, -45.01053111471183, 12.692593188170207, 6.991118449089675, 18.331258723169316, 25.117749534970635, 58.38951460380093, 20.591887553634816, -78.27518301023795, -73.11282134212539, -52.01413757288616, 42.16804210158747, -18.984696056440207, -16.993081954439067, 61.14982720834841, 18.29195404475181, -58.246547349222084, -15.091661150472497, -19.464408542338656, 27.432725949566848, 73.7513771163132, -72.64456124011778, 32.12789800531966, 22.100250867142204, -33.788379142841436, 55.035029738329335, 42.65112287760905, -43.150479405210525, -74.79546748020621, -74.92267410583266, -22.946780670446266, -9.182425801888774, 34.87669428527423, 3.0743703655323062, 2.531147074005166, -51.9526127469161, 31.780806013727783, -13.265693835034375, -20.311030389846923, -57.74230879562481, -11.072470694442199, -4.528120367055812, -31.92356494701302, 17.60006826459632, -43.323777700133434, -71.94006058509274, 23.094235158735216, -41.523759386200595, 60.187017928275424, -63.439331386281374, -51.871438593788106, 17.19714057243819, 75.7092421473904, -3.7123507309848716, 58.78815085361196, 75.17409368373049, -58.19370795519525, 78.04164988539735, 28.221617184082675, -41.872205455386826, -43.85938199768861, -2.4506775376724343, -49.03759485549304, 4.879334629826332, -6.663555239688961, -37.58115076810673, -2.1041891737378084, -30.08284856350571, -8.856651712352122, -12.604078062000319, -51.223547460255695, 63.11545787466878, 40.60524536692549, 63.446855401818326, -35.991223879975315, -47.8830934659623, 30.98386355221033, -28.920049841231325, 26.682072929894915, -3.460794135672294, 58.66520868326951, 64.2106193301166, -65.30602871507035, -9.336582067311646, -61.29481959812803, 69.28057000427795, -11.14405771560943, 8.793803638967258, 64.64474950176728, -36.4006036056861, 56.75758530379522, 35.57071425004385, 34.31551449234279, 47.60905208344339, 60.069034632962584, 31.77149795910273, -14.432749497091178, -9.360627754557873, 44.91301929294512, 30.91364121856527, 57.18952592546907, -55.268497384271384, -57.54578500539761, -33.84133838898997, 2.311702547392143, -78.91509062098073, -67.55374984733984, 48.50582802846705, -19.637660211257867, 48.79335994908632, 77.25217339807232, 4.9759352999028295, -64.58866875655379, 73.44583938441936, -47.62759593350111, -50.89368866446726, 42.624561333933464, 4.132316824587946, 35.87924140601315, 58.765749519710184, -50.78771949811063, 61.138141474927714, -43.18710968183655, 67.48492655713532, -35.44757257713687, -48.853258389893625, 77.59464972634983, -64.51748586880258, 17.615936787327257, 30.90123398544415, -22.26711394526477, 13.008430178263723, -27.32194883883325, 76.29676022833331, -20.167430123292007, 52.63619828305617, 71.59044874184309, 35.07299415679726, -70.538744127419, 19.990065387696735, -54.68118262231635, -53.95719482131234, 48.16648081494439, 28.67337642156808, -14.20335122523528, 77.97016675987025, 75.18048547849173, 77.9694550776201, 4.834347941783962, -74.2238546318192, 18.922048079950056, 67.61795184646495, -8.44254637831267, 62.11632226523802, -52.32145627360673, -54.99351329699095, -76.15831903474879, 33.120474442195984, -44.78695767310512, 39.98960479752017, 18.34753469310255, 52.35465382803703, 5.8783117162546645, 54.8535748250474, 52.8817916063367, 54.528133804760124, -79.21418811145236, -9.267900824005762, 75.7149587493004, -44.35395455884228, 42.05991830831774, -6.714689156719215, 14.649862415647604, 45.688117086387535, -3.242472580224329, 30.840221130822684, -4.159945691565769, 16.4543965898078, -10.240396852529447, -49.98963368720587, 78.64301280064876, 62.666183070928795, 74.42337387564265, 51.963915364656074, -10.283993914274864, 47.07469533392485, -0.29966713483432483, -66.80215167224021, 48.87597746026764, 36.40309600418999, 47.87374042264476, -29.374062263756436, 72.21162690499932, 4.433382066334289, -15.16878894037983, -6.143034280407164, 15.285127030018797, -57.515015918246526, -16.206954635792837, -38.91544722682886, -26.175392786578186, 67.03806216085192, -41.82325156727959, -79.06224366318334, -15.526521213654842, -65.2097010933641, 27.979944557710287, 57.93811589367513, -28.94879731185971, 42.204709918635295, -53.16099565546777, 74.02716444370498, -55.319118676415684, -53.972508321912315, 62.07905352864361, 70.6080524351716, 54.19922663331253, -53.76713207308361, -26.87570952588189, -9.480364269871648, -45.0948995839372, -14.402002587002148, 37.94478535797972, 29.98988554922478, -16.111520716499246, 70.95515460135111, 53.42393579430447, 57.92566451278597, -46.46136121465791, -29.37841271693613, -29.889295655993458, 26.66179419881185, -49.10832836536692, -43.37010737540439, -12.666268092843948, 35.73409710232138, -23.004869006504176, 29.762201903443408, 38.520892602298304, -61.432155275187306, -17.692389372447966, 30.238034441411905, 48.47290142925698, -16.68727198018715, -5.86995447947495, -64.63897236953446, 9.315920072827975, -28.549632922377484, -30.458664111668348, -69.88198082291856, 4.547844202764681, -15.016000093768898, 26.422322610273458, -79.2316770340774, -31.348301474497497, 34.101279622888754, 6.086230524458353, 1.1827612839965767, 9.271281141347346, 55.8055289843099, -35.9844401448765, 34.13125904232919, -63.778715440611855, 0.38484837596326804, -67.05218239427512, -9.954918393246842, -10.507775128431287, 55.6831442889662, -6.882670281634487, 47.25246253966241, -26.433853606434063, 0.40834745907664693, 41.57006700391871, -72.71128454190978, 55.19111939384847, -17.888290082083874, -33.259759395894534, -16.794076787175705, -69.11165662303273, 3.7470937542121114, -20.221173980570086, 19.856284259307806, -34.38616943318146, -36.528006086300984, -52.79125574511745, -31.953884926535597, 70.22494733292318, 55.125947544413464, 1.5571376749352432, 15.131638296577053, -37.753544414714305, -32.717576576288145, 16.370906662531038, -29.008293875013376, 14.721772189932379, -74.59786043571972, 55.67566814639518, -33.926411711364054, 10.356949419009462, 46.93184499731276, -61.60916461527764, -62.68018381742524, -37.50445766347768, 61.52930842159231, 78.68322358606794, 1.7520824999783802, -5.4877710800550386, 42.55399964205701, 2.8353721598560035, 71.17748842693811, 41.819536945761584, -66.04488678524427, 35.6263173821364, -53.34615063743425, 0.09522919233913685, -1.9902479076969444, -23.443263639083877, -77.85572134658108, -78.5297023608739, -12.940030162889375, 35.84013684363472, -20.621617671004856, 79.72336744591856, 25.916034723100008, -14.70822395387646, 35.20328399486061, -17.72501238205673, -55.82780000122492, 80.4462684271016, -63.34794230349365, 50.384957044578, -71.80395352078908, 40.98073108492707, 3.6493500581566525, 66.87973812167932, -70.20366346144895, 24.829438575354573, -68.92095545253177, 41.73607251423304, -15.604895944775922, -7.477653765228743, 24.256061821522792, -13.493010892541017, -64.38076467397859, -42.64497727456611, -11.00215577266129, -71.2719745427545, -76.43452156220322, -7.051553397853058, 23.538799514820575, -72.39405159831689, 79.50932175004331, -5.943515164778075, 0.7521188209912708, -44.16684577232904, -40.981891142256686, 4.271499306640876, -1.384196820598543, 65.88596699241033, -6.8019227103378626, -33.154119017759925, 45.38082732072926, 77.74913342769494, 45.16734508078983, 9.018405581237952, -31.77964141880393, -10.956365516722435, 7.257922847078167, 4.079921302997355, -80.61515954357056, 43.83293659915415, 74.85453281306054, 40.79839033891311, -46.08476701583123, -77.48270352881812, 46.59344977513187, 58.199492641756706, 17.44592949373281, -68.80814016504281, 78.31748183026858, -8.178840217037788, 33.37939583405826, 63.511858011237074, 1.2196897616870501, -65.22493931163545, -70.38534201294858, 22.241036433257833, 3.3278332851660144, -58.882585557336554, -12.988751089123252, 10.587847400346805, 22.051226731218712, -74.15032965822509, 10.618757253462418, -0.12011129041420388, 8.8057114210825, 36.76672437324742, 59.57468547105367, -36.34284782372619, 56.393468794031975, -35.237710000231274, 15.185201499560701, -32.03242095224613, -47.26490207200527, 2.0105147059369464, 66.56370848960114, -8.938695154610286, 11.786041681925113, -31.803401497624787, -45.559958424021914, -45.694486889927, -77.60587524827264, -49.1909067272202, -68.94109579034237, -46.50231017058997, -27.165219167786045, 17.075784396109288, -48.466406197402485, -52.3503863098732, -0.10208159982292453, 15.661716086752488, -42.873009569258706, 29.291610326672142, 51.97712055197276, 62.83049210677211, -47.221934533787106, 77.93245008098869, 57.14414229322557, 52.35372883434358, 51.27596393401763, 21.6497706665009, -78.12017658424182, -1.7453177086265441, 68.59270540837511, 7.944896199045274, 5.766137921937411, 5.609037625177753, 57.92563346251588, -31.138758188234995, 30.881106992569045, 51.73388514730081, -51.37035363514396, -9.226061655517459, 11.623270036825776, 16.645807706687513, 44.33853520661567, -70.37522094752842, 13.760451188777381, -68.26287928763298, 32.092595599442674, -9.436161548255058, 58.036118947632744, -53.15473325389721, -42.0009474354227, -65.59083057045129, -32.98994210921796, 55.12119883290755, 6.900896723453045, 40.79818448184783, 52.38181692091087, 33.958531147418554, -52.679774612859966, 52.28062403726549, 0.9036229579232322, -20.747866135144186, 36.06004872931817, 29.681322824443168, -49.54543255244054, -45.89125993316649, 53.7317865620688, -53.67684693259408, 20.44358758085334, -36.68170614791804, -17.765334023046314, -67.27337346837211, -50.650423411075174, 66.942414812359, -62.29015622984498, -22.419395569810785, -64.54784388358777, -32.35993170319713, -73.9910778789463, -56.296589346557774, 58.71974353740389, -52.77634900504611, -46.9433018227259, 26.491359164339205, 48.88332401274673, 20.814942419104128, -23.026945327191502, 31.55016945023499, -63.86707959327444, -61.236682898797184, -29.69398193700851, -71.96891441062344, -13.50945212829158, 25.00607463676883, -69.51974972909369, -13.243893633469243, -51.5449803299287, 61.27838686757682, 59.35881760013732, 0.6552376008140685, -75.31073718868164, 44.718382774692806, -17.955207710272592, 6.349741526623529, -28.91003275439806, 30.547320325286133, 33.69540259651727, -16.236656078921726, 77.55416226520796, -63.82770719923078, -37.49880876650801, -66.83015392769491, -25.690489562723386, 4.7301324659616295, 63.186184067086394, 67.45774721610645, -22.095796473905324, -10.234834387360387, -73.75436931998516, 71.76414012690739, -70.34122939975538, -8.070340552862746, 5.127224414560985, -4.237142891446396, 13.543564658546597, -16.776325269814514, -30.56549435668359, 10.332141814934985, -41.74948057911146, -68.46368340328213, -6.128449832291925, -79.03984339218468, -52.428772387248515, 25.846715543505283, 19.621850163883767, -39.05700130835817, -8.562476809571884, 55.33368337197393, 52.62715457822557, 17.484062871826346, 29.044931791513275, -51.10015436169092, 70.70088281170419, 10.255513732978423, 20.67086715285308, -28.472194978710952, 11.869843590500478, -55.44591046956798, -52.45679576416922, 80.47963832243771, -65.57644267259417, 39.89279957647036, 62.03793776433539, 8.946763725085745, 12.797867308219379, 57.3112590657246, -50.10273417575153, -18.557642000517784, -1.865932441109643, 40.729318361794256, -77.47677997145819, -75.59813984336657, 26.586622870539884, 0.24016489325327028, 3.7337157163917833, -43.34334530136813, 22.54989571019695, 67.35249938191396, 70.28324413454214, 68.75265675956182, 60.16029053340068, 54.64294954614362, -12.95181494736812, -5.376701539117586, 28.847947003733207, -63.123561630804296, -40.36231936368879, 43.2001631802659, -32.0809858524044, 14.72594302069225, 65.8566593635907, -24.936157125306693, 28.681655554051773, -25.344848467591206, -48.07682693747387, -61.381235676487556, -72.92246112218787, -69.35264867222392, 54.69257208263366, 6.95376443856742, 4.448129296471292, 66.26938726944091, -54.44871571076783, -52.26104174599545, -52.577740462867176, -4.220414781993413, -30.1634514195621, 28.265127284820107, 37.67970274126693, 24.850501385278264, -0.3263277729312404, -71.3272937718863, 10.52651228398215, 68.82424660164804, 57.473307616963886, 2.749240636334382, -14.36810430748468, 19.91861641387375, 26.08521766311759, -14.96865543980087, 47.720335742296605, 49.99269610716528, 4.986912614278006, -51.15237948929083, 68.63399801066177, -43.134108250382354, 33.124834167025696, 20.098972044243204, 71.78406673294525, -56.73785116301088, 49.76787858257257, 57.84090786687682, -79.30890110227035, -32.2454981467376, -15.611551810220726, -10.609400103273668, -40.884900847734016, -35.13810036639052, -68.31822577926579, -51.01879358986307, -9.627026156505082, 22.20121996415931, -17.128837688874757, 32.85203183360781, -33.2711707994813, -60.00647282523675, -74.73221067422304, -55.17460456101334, -17.4979128046591, 55.94729907614228, 72.46491848254986, 45.438942194118304, -29.875191350820273, -33.46265493513192, -44.37194229931009, 8.432457918368211, 58.636037111190454, -39.07306636170661, -44.61618454413061, -64.6198497335878, -30.631657432937363, 58.45659619815174, -53.06507772358291, 64.96884239782717, 65.59389992874212, -50.88423664930556, -65.24149492016898, -26.828490779288373, -54.38808566916876, 32.34796587696096, -55.25401445260618, -51.37094269742353, -28.234829698772188, -10.673598722539793, 48.833226355532695, -66.59062514535823, -69.33424225184554, -46.01496999039046, -38.286124153239065, 7.582170480870349, -12.921633053877835, 66.40621445699495, -19.682104447063136, -43.24821583741283, -69.31630558261865, 44.80375564475071, -5.656808043387987, -61.614947066728696, 46.846065616143186, 50.083981810200086, 49.8893290456154, 37.868187905197665, -29.34193819353642, -22.509189193748995, 59.81713103004847, -8.777736441423748, 30.39004556693807, 12.822146889781704, 48.48085317615045, -14.537996119718477, 36.68540016200524, 72.15811421527508, 28.259980153228767, -21.260877139463723, 38.52844688753848, 23.110315845061702, 78.91429930696378, -32.886548849193176, -29.916070096294543, 27.796729748504028, -27.91963896636305, 54.77764211162606, 29.692320890182405, -39.30055220159437, -76.56492805637657, -12.151521856417023, 16.60545934239263, -44.76395061230361, 21.96367869115351, -11.08797552100732, 16.66605706202616, 26.408110000529426, -50.919339631470635, 43.33630798340148, -33.75165760577133, 39.05982054806624, 42.983312810327334, -1.9106703170591572, 29.98304364134662, -52.53544951144172, 29.864253183975098, -35.59652477513433, 68.20761245513901, -71.45432163486777, 18.301138675752963, 8.591308873421719, -1.0474967265424289, 68.28720899698891, -55.67288747058689, -69.21068810634826, -59.81471076208065, 20.783373085334475, 28.69440397284332, -0.4828921226292897, -45.70738380076518, -29.061952260657506, 34.096503369300336, -16.636950517809527, -17.938891965527677, 77.90524076637267, -3.144752198706147, -78.12650249844069, 59.27473133297727, 67.50772645865568, -7.207610517381387, 61.67265004336664, -63.09943623569811, 24.370225334027992, -6.316344037730893, 35.04510866712439, 78.44945596545428, -65.1152482244881, -70.75417158550493, -36.15446738288314, 71.30178525439283, -73.27337300299261, -55.40895500248794, 48.219337070877906, 76.86232832749087, -55.16375812319157, 73.53326088939635, 3.7695640520873184, 54.95693304989295, -40.34207406120551, 20.971617756440708, -36.40507104150086, 17.520449620432796, 73.10111882096494, 63.40027934214631, -36.210754299315, -44.75899534686933, 70.86093111935666, 20.24412679216171, -22.88473309475988, 46.5207350866651, 10.707453868684144, 48.01794625346728, 30.04812734046257, -51.012690735054576, -75.20747736052337, -71.09597405283395, -67.95672745807201, -43.35511210074449, 21.08763502429742, 31.143350988659066, -67.84156809441113, 46.96912837472861, 45.41402444392617, 44.49446678352955, 0.4557865277300306, 41.003475598753795, -11.787343805538514, 20.403501134141557, 14.460853340102567, 61.08551418170462, 31.242000853971852, 60.7035509775274, 54.01031834278629, 58.9445331715917, 21.524519858001927, -42.0440199520219, 68.3707075457687, -42.71357733599419, 66.15751192547195, 79.10881523823117, -53.554826338356214, 64.35373007038928, -42.368451936328654, 27.877586024165826, -72.32639352735424, -23.188377669897104, 14.325946287684047, 19.064335611632913, -18.79567475277103, 1.7109630136089597, -18.14308268512854, -18.57042463177324, -78.00420240298492, -21.23152564525856, -46.480605697704604, 48.35314645076271, 79.63784211009849, 16.882828442566385, 3.7315547579210473, 59.877857312747786, -21.181569059774255, -50.315549854947776, -7.128617547906839, -12.992286905055328, 46.30066852559124, -48.871753487354226, 80.85048293580482, 77.39068387843619, 74.56242088307977, -25.122602853344564, -5.694592782179228, -27.373711518979388, -4.94037146883544, 45.778002779887295, -39.38262070234983, 63.49869069846487, -25.626472392183203, 68.43478573560526, 19.607462596313574, -8.16323978110466, -55.08984804980111, 47.563170234791414, 22.46844419795906, -54.55295403167272, 64.61555549234431, 5.321588233767844, -70.7415321174217, -71.74448977039229, 76.93055762322611, -19.21420798983507, 43.4091220507044, -72.54081569908911, 67.65728839490136, -1.4736345235731703, 59.820130375190814, -29.587294689316888, -71.0917363186721, -57.34827158170497, -5.9122372970406785, -67.74952883976363, 20.134277988503726, -56.81082046772286, -15.440282544072165, 30.32166853243473, -76.81853978928841, 72.34457420889846, 42.682583652830516, 0.864135719301325, -29.786148736572464, 1.4794578815913724, 38.81877182970956, 26.342125342210405, -41.542016459686344, 13.612714177159177, 63.276941346423264, 9.859830415283433, 55.547484761112464, 77.26830137387917, 1.3318571451227568, -47.21512003629099, 63.04323572927594, -45.20085948372815, 47.216498523513536, 55.59038859065498, 40.0943907845601, 26.331219495100722, -26.08984274231368, -62.515658105372815, -64.9757431072621, -23.210482817268176, -47.50491745387608, 10.254689578145431, -71.14732117825014, 35.586297964269534, 64.88605529130302, -22.19752762983208, 42.84580105015968, -39.77487927752612, 64.3297768170315, 61.771863394000874, 15.927002556560241, -63.52711112892998, -77.94364946139152, -37.74444783476215, -69.19483051673362, -14.717013853222804, -9.47075463077893, -62.375952642505354, 41.10125590985389, 58.132459724462365, 58.59348100447643, 6.4316603932258225, -59.32397447214708, 40.269429073487316, -61.47927695984744, -41.581204446673226, -32.35189542849354, 36.43695468431052, -65.90100642014994, 78.00489515772104, -33.35372723210723, 42.36610831604165, -65.05897972260858, -1.9371380090613264, 23.025739377321134, -58.239530002824736, 1.2888354775081154, 80.24296288578691, 19.140885883952112, 54.14649174432729, -15.693374348841985, -73.26261125273516, -31.695689151582215, -66.75366353041555, -6.987204801398576, 31.36771383459621, -13.134922206583239, -76.61638476573862, 57.412898203557326, 66.24758624132419, -4.782680798904744, 51.02382239540654, -18.493528703010412, 7.195990235380293, 55.5393654509405, 24.599597713801188, -68.60204151803939, -36.957607123569026, -75.67510112002309, 41.81523661222389, -68.70215680003633, -1.6438040200477297, 67.6137962093893, 11.17387790422548, -41.56754855970416, 38.12150025857552, -18.420504547707885, 58.615012621588626, 45.007972017670724, -19.50599981823937, 34.399219100817035, 18.640634056317765, 36.38923595669036, 72.15605534855476, -69.75719273436202, 77.45640368035914, -28.469617718559917, -50.6221223975754, -51.379195717301315, 45.16401791728016, -60.04799619762807, 25.60878349218351, -13.798508511483227, -14.501730024601425, 64.88146843662844, -39.12551318171145, 24.717685041776885, -33.535274864509276, -70.55472150377167, -10.103708547003412, 76.28919462157134, -27.995754942679078, 44.83542107941705, 26.033494960645985, 54.00443550332295, -9.077900754348402, -23.196998991845607, -63.506120608134104, 47.06289716284528, -56.592700203250644, -27.636532328024213, -57.44304091060489, -54.53126842668891, 45.829778911880155, -38.550946052086694, 23.17611287087113, 4.513465537609222, -22.93370054479291, -58.68427897032547, 46.086763287134275, -60.54333944046618, 76.19987131910095, -46.297039518955444, -22.045596546975194, -16.355085407876416, 24.155743723421246, -26.37121351935103, -17.897582994625793, -24.570110037815024, -41.94335199346397, 56.52455594068769, 55.56558380463876, -3.7913749294123136, 26.378661961793572, -46.242162652667204, 17.661211348539492, -23.74050268776893, -44.122160438186604, 75.21999965410232, -33.99892086361705, -50.12012226120548, 61.91965420519069, -69.46102570500169, 44.36461755872205, 74.09493907266457, -1.589543957673039, 42.450815038869855, -7.07237289776478, 30.934550010917878, 0.8032872899775412, 5.732760533977461, 42.938003301388065, -56.68273231379347, 9.691816467467957, 30.210837196336687, 14.283941136194196, -72.8284707335279, 44.91422306063705, 3.8331751682820503, 49.750778422707455, 68.38331339025602, 40.73535010155058, 60.63114416994559, 75.63962639960207, -42.77109870061387, -38.64572332570167, 4.126314319046145, -11.458848859234443, -47.016559466340425, -45.832283341056126, 37.90651575383053, 52.708600442625595, 33.27990964156006, -49.929450974621254, 60.349391034062464, 5.8556205186195065, -59.36626327816218, -50.10091547854161, 41.18665028233937, 23.911707053012165, -74.20650698205866, 74.4971835032693, -18.723811542692278, -40.994354636753954, -47.61756838052831, -65.65402388501857, -69.49888991765, -59.6087206691709, 57.520625839823026, 4.915850792466592, 65.55498251048373, -3.4440651136787532, -31.592471377233466, 45.024888379116234, 75.44792138346189, -69.31820514405494, 63.31087095845264, 21.152240144131085, 78.09302992524273, -81.1753376100583, 74.54632532519564, 79.93710587568553, -53.67361751685386, 36.94426123498488, -66.27939601026472, -41.186441236674376, -50.87387325858775, -62.226017966903754, -41.842873222469606, -10.648975667829035, -14.738688783021944, 14.07609583485763, 7.3924209377871914, 7.2449321400249485, 54.23098424984725, 0.8713018599785002, 74.01217716138667, -18.890706807846872, -19.064146333228265, -32.252638345964264, -13.917974821292606, -40.58410536856709, 68.94928111149424, -57.16439060697287, -2.1321278709586835, -27.267785390373756, -79.04528904659688, 57.54706194494061, 8.065442842226073, -54.72023484856859, -33.016841916265406, -3.1177978358578553, 34.92562654869812, 55.019469608981254, 44.44933283435864, -63.14796300626671, 1.168719655457458, -59.045762381226105, 73.87451572919385, 61.32285263530886, -14.442367185188203, 5.727144606350725, 56.165711006840525, -35.0672451459935, -52.600431482629304, 5.923206305189446, -30.319397427404862, 36.7594705817979, 70.55630369129872, -8.67586656963717, 40.68296264946653, -29.165661591019067, 37.34606895276054, -69.1835675287606, -51.17606177962904, 63.77522224806584, -27.940244960609537, 57.080974882985004, -53.111829451897755, -62.742802463814265, 1.9790913054051078, 25.06597485001887, -13.345506468695195, 9.154203805163082, 49.333276452827725, -8.311223562481038, -63.228461814870386, -44.96398313962825, 28.7572325589761, 44.044143358406, -49.03879914739666, 27.43618720150352, 3.580300577652281, -52.449995371956504, 46.970918668739564, 12.08696467406521, 3.843451003718867, 3.898586299823787, -77.46164585161016, 81.02739775018435, -73.51408719568195, 35.90608630121405, -18.003951907819154, 28.514093680001288, 48.15163277597596, -28.85878905611267, -50.10835691416131, 54.83824540453437, -57.94440488519421, -10.562373217048819, -34.94064256319371, -10.41616654568359, -49.46536369815502, -43.700703747676314, 44.560652833246735, 46.74491168203286, 47.68547805310222, -30.3955475001213, -41.045882598941226, -18.100776067760272, -64.73962320988387, 34.40122157486769, 6.130162859802051, 57.49372521904919, -62.196313621718595, -18.04939974352347, 20.171211918524932, 76.03509583190448, -30.698535465017017, -76.18989167601595, -14.9693445140132, 23.63104784179798, 43.9596898393716, 22.27407162484529, 20.600915054486308, 67.27976884748192, -64.42745754145967, -17.14445043784685, -52.2534348136875, 17.00093407922762, -41.91590388446055, 27.684806777756897, -39.71267448895178, -37.77596015557337, -11.001526942699515, -49.43701185798082, 1.4337986028301066, 46.46430269552433, 57.47063663362692, -33.28081920965602, -52.22577670249667, -58.51905809391613, -11.76389834519599, 13.980642533613661, -46.65483799672895, -41.99899650551092, 7.330499784455543, 31.530108180359157, 21.94459898493762, -49.18697695909524, 53.98248489718946, 53.91260690903017, -58.2509170046489, -54.63231299140743, -72.99286170623459, -27.542694178533274, 9.286185270284022, 68.24777928993679, -39.36391786388867, 63.08204085368107, -59.429544465069455, 27.871450121067987, 5.080424545318907, -13.736325280082383, 8.867347011051525, -35.09081876282143, 53.72051529395001, 24.997222466045734, -67.94459170394404, 14.427525423707523, 36.945478584613596, 42.202072929360924, -12.501553416779322, 60.549156956541026, 13.497556656929014, 18.019146585222213, -71.44732913013632, -21.34657572130778, 44.56447872508431, 51.33843079946762, -23.871635479071, -38.83577912749635, -70.45726722150592, -1.1562137769561784, -35.68674340719123, 0.3660966254757597, -5.6033653442346365, 73.98762533726429, 28.35639905685228, -66.63285183154859, 5.57208816993587, 75.24293421009972, -58.03838964755617, -42.44340951001536, 52.770527703061376, 52.52902319597252, 27.919935548073887, 73.81121073027137, -2.1623562881104594, -55.027476409274364, -28.33526223650267, -73.9154423384052, -77.45268296665589, -39.59485783098269, -27.875047574863565, 31.7920235461725, -31.71780870385698, -59.193710376259745, -62.71358221952555, 5.56577425333572, 32.07271184834647, 75.4544033233952, 73.54761951582036, 15.16169406954198, -40.95011768523706, -12.537252734963712, 39.993629726516055, 43.93281389574586, 77.45753199450884, 57.61624794578794, 27.98940316123678, 48.793464296980325, 6.960885963274747, 0.3278194581944498, 3.0810313071527013, -11.789542037003015, -79.87761925187857, -72.1143832291738, -10.33504831097443, 75.32941849040627, -41.80526736234584, -19.21600845326704, 34.44949097937993, -6.192377145498745, -70.28560641713733, 62.26527772877631, 43.56038502495079, 19.215419662274194, -76.77104620325929, 75.10221809960066, 51.75019409371206, 78.91408577388856, -42.393121469173984, 53.749379534826815, 7.451916968588734, 29.12162421657758, -27.339998740126227, 13.823171410749781, -1.623964047862042, 50.97766983782296, 43.46686446107848, -40.919390039084746, -9.10938082609323, -31.84215523019498, -26.157804559246898, -32.23720593717658, -20.309760388194498, 62.278241760221654, 72.31623798521859, 15.837557510030193, -26.15341414432067, -7.76081441850518, 56.44026042159805, -35.78933484308462, 75.54942863674849, -24.46135088562657, 61.07680247568815, 12.26231946799438, -0.80758610368869, -56.95757828438843, 59.55392195522577, -55.33850202691301, 51.210674543985924, -9.352791024941325, 63.90564970418835, 5.433351879830894, -14.20630185105628, 27.702167370750228, -17.45461376856646, -36.55818862858093, -32.051141530925285, -29.399879367024305, 56.57090560859975, -59.720014033607804, 9.021966555256853, -39.06031879970268, 41.18265610013461, -71.69930990633218, -23.821002936079786, 30.822860063115346, 11.606653600248407, 28.124086930128712, 40.975638908767145, -56.65898456312196, 29.08566451385638, -31.546148620596533, 15.878265892209372, -77.68314730342388, 2.7229992843140627, 11.282305939357965, 12.213993685009129, -7.7076634812304, 61.914543777916904, -53.03827628800122, 40.09213873817965, -30.251000258503897, 15.013331392724433, 79.43179601060604, 17.45373055234454, 52.8626586931656, -33.22859353645248, 68.4662469209979, -80.01162568196993, 22.647970963845715, -18.813346352007425, 59.69034249257999, 55.920189137945734, 51.835577503240124, -5.722119130517661, -26.496294045035985, 64.42738995854768, -79.50155833496503, -54.70800339088635, -26.458410405272573, 22.044225341671726, 33.324806955875935, -33.770366116600336, 76.50981716755805, -5.646561762820221, 49.323261797333856, -47.860715397542016, 41.13559944777142, -10.843058402506319, -11.763557412294292, 56.7541718115516, 67.4543274401575, -79.0262655796965, 63.912314571776065, -69.1723401995839, -9.733658779709067, -14.150715927358727, -5.7756273586809845, 53.16523839878721, -35.868742133732574, -64.73414073620421, 17.274979920723666, -38.10004131406458, 47.55813358722604, 7.732519070202816, -44.703249727607236, 60.14060311448955, 21.188096991830548, 11.17582769120376, 32.94209708532551, -54.8403374377648, 49.89669795743548, 16.043108768626425, -64.60564969660733, -13.368659228319563, 74.35835682867717, 37.13608334326268, 24.26288464005126, 78.41194976247905, -79.64213416460551, -35.58204075574493, -74.32919156134074, 51.99505242263949, 8.417787247791532, 35.837866185861195, 45.510905120561745, 26.46665202074285, -69.37443617224909, 3.979433072925506, 13.54396331703474, -72.698846816661, -64.1382799614737, -60.18129467253374, -25.607883345661925, -38.93757908545812, -26.62957726638013, 53.17388532544829, 45.73274039555748, 17.337549176881197, -7.981361644648167, 26.061933442371537, -15.006048001946773, 66.84444842416183, -1.9449132475708013, 73.5026144195355, 2.767648280343806, 47.885277393423536, -33.91350503245091, -37.518154082461876, -1.0589047858254048, 27.36729480041899, 53.94503072451044, -5.7493846556829205, 21.51249835740169, 14.409809779501385, 42.53122272181417, -14.200417018391432, 12.044397337977795, 26.239502233701923, -15.78013606839877, -17.62482644016065, -64.19684535805489, -80.26991177184202, -6.766656900370123, -14.115255516167027, -4.282773227590303, 56.67824893446599, -35.25473638057983, -9.94342579393882, 41.977398965299514, 35.94670986898427, -20.697627032343583, -44.336902614469494, 30.436561242030777, -34.85793061309251, -48.460276127408534, -67.02090914432036, 46.57829481670871, -37.82404232124721, -7.6178132655971025, -27.441670547615985, -54.06517190447703, 76.65501156102873, -21.903353448071766, 39.10431953970834, 65.10329470196328, -71.38477933406669, -14.052947464186644, -6.025094235763443, 8.461100482372782, 66.75802216581422, -12.051999972847895, -66.16172849176355, 71.12658434341964, -81.82915821807204, 55.1226561309293, -15.396082622950445, -72.0358409843098, -0.9821618180042455, 4.623168100215551, 76.39701543973686, -15.829024002069257, 7.685189744041119, 46.509140635813196, 50.97457152394091, -8.078361626076815, -79.42829125274166, 76.58246937054233, 42.544753901539565, 35.005203661119985, 25.465665189995395, 46.64862964828728, 45.69273789514174, -40.90978691793671, 52.73976576028307, 65.88408704682816, 62.099924759695064, -30.3801438183567, -43.57411952362242, -29.81392541456029, 9.17854384269807, 41.32035365466315, 50.89837314376225, -80.16720659846915, -78.13729770103637, 76.71373775879479, -13.817702588786307, -28.380051355001104, 66.32228749076862, 44.6889177146811, -26.803482405328815, 74.70704142167097, -59.82878197086108, 47.50022305121785, -1.9831526405429796, 53.951186976300576, 1.2008765090188531, 10.247121827969345, -5.965714113606215, 35.43111622833047, 59.171284526105545, -55.498514578507724, -74.32848812255811, 57.72861079959706, -28.82135047097013, -68.30144421805238, -78.52815425471847, 4.9867062592839915, 36.54398723404535, 35.1318785023054, -15.766751337299551, -73.35140584193498, -64.15698039229993, 51.26752394932709, -5.363097333740871, -54.78023440151726, -56.025668104529686, -25.088153272263924, -61.220345226329336, -0.07386751880203093, 72.32601984910178, 0.5112880031988232, -9.928314646142212, 65.94364084327766, 64.36127556363932, -65.74327893021173, -36.91193625680225, 10.462850563565837, -65.85219142426834, -25.362286950896014, 2.5644502825096804, 26.78053936254866, 59.532550463915875, 60.41293747796601, 32.093421757790864, -63.88024020179501, 76.54611354685501, -42.78819892468912, 53.01135743646622, -48.48743859524925, -61.987564561900946, 68.29795961929317, -62.3640284738846, 31.965739568766843, 66.29065672155775, 51.98471150895209, -51.574438691369515, -13.357688236242165, -37.4655243155146, -54.716613873040295, 63.09416631441667, 46.51092116586664, -60.79981363547043, 60.4923852564772, 1.2202101957923943, 10.113058299006, -41.91748245106884, -38.79596082501738, -56.718250816091626, -3.6922397932786453, 13.744607124666999, 48.13685843277652, -0.3611809899207729, 20.989877540046212, -77.57989863652212, 51.62198823433196, -25.534734067355704, 5.369142462936123, -53.56586795879559, -30.44049375185078, -51.6949105905788, -52.79290756023646, -37.953738841295404, 41.66016477817998, 62.32737822089606, 26.726802001050746, 40.25481762357832, -62.24555324633936, 55.399143034652184, 39.276111436539416, 56.39437826370024, 10.90926201096298, 62.967151074631005, 23.491181819097747, -49.815254435967326, -60.93066421271359, -27.29279762719351, -65.25855013592779, -45.6028029171613, -57.911415169197674, -21.444563684087584, -24.1009145490736, -58.61240335672147, -75.78777115599847, 7.907389377193773, -71.05411031138821, -19.94846925106298, -27.714711414549377, 75.29107467247762, -21.342869229483824, -29.46443178800449, 56.381118936461604, 59.48955504242029, -52.81227829458225, -58.59369030691634, -18.609827232825996, -50.958396030618566, -30.542455840012877, -51.08617976063118, -59.801090641542174, 29.363246215216172, -54.59869293257808, -14.830712308217262, 18.67967619093124, -31.757872448304376, 59.18727959777356, 74.8091498676722, 39.637281039111784, -47.22476374455474, 26.578944102200317, -4.947543606932846, -49.548811526694664, -44.18976296599935, 39.31953509031629, 15.372234241538903, -19.250065888638503, -25.996393778777975, -47.40041075573806, 59.073550499842256, -7.850559650469851, -55.2756640781369, -77.32861698170399, -57.32181427977311, -46.106514454997026, 65.63691368808294, 56.37417445565594, 67.91062994959991, -58.08885268210574, 76.85168355562746, -46.6048425831103, -8.103719351730675, 55.291615400570024, 50.90516131008286, -46.11886410284084, 26.904259613976, -74.80008185291335, 23.481551769983632, 63.63880431226961, -1.5282806978870411, -15.373064981671545, 24.05705020934401, 56.28891177521098, 23.535974983307554, -40.126195960677826, -55.274543395297705, -70.16201805078515, -37.64105689289747, -44.782393999333, 9.691520714422753, -65.66634725717192, 14.140284581807535, 70.54833376195126, 18.287671441038427, -14.038932737571287, -25.41137388119525, -52.336608808030945, -4.541446460514033, -32.60143638198187, 31.813808633502706, -35.86196631536589, 72.29348825132868, -4.2063075161466585, -22.095102122951538, -62.57963627253828, 34.15183982929772, 4.575415575865331, 33.38423973044844, 6.857702131664121, -16.178969617766946, -50.77520017863705, -1.2230545940337438, -9.229035155019703, -16.96990705828647, -20.400321627076764, 23.99716116879925, -46.21072805023031, -5.428865126964629, 80.8170683902611, 18.584540167883343, 29.538429919644575, -17.969734061150493, -59.78946946511505, 49.41335202830854, 16.438968648816882, -41.61724127632388, 9.653520416497644, -64.26099380070588, -3.3547631306475725, 65.69263768102249, 53.55981742948905, -0.24886578273938764, 73.36812242599322, -23.211943989223265, 17.641625768313165, 48.18603202118772, -10.850694448634277, -55.166180476783076, -55.50533293709874, -69.12723306913837, -18.833859897958728, -71.97933644121773, 7.59842476485111, 3.212921634718805, -74.41291217639018, -54.26763104238453, -29.23527517926516, -9.4246579673019, -51.062376438989006, -32.402412206990824, 6.554925095213576, 71.18899048052941, 55.73697518805545, -60.222533040701656, 62.76541198621412, -34.83722076488704, -6.397189469447822, -7.735589281144891, 26.519534626735275, 63.697606686939025, 70.01316941413472, -6.366734706461635, 12.51047727062774, 37.12999637086965, -5.238310639013491, 44.6759047017183, -51.866945904091764, 61.3205379222336, 54.503736986585594, 24.855877596679164, 22.28230681604839, -51.60550125858824, 43.325776470250965, 3.459513584354653, 21.74598736571074, -51.62581800747816, 29.018732556697486, -28.442662186632116, 64.2560157207672, 25.03993516511125, 47.767212398539954, -60.42180878123843, -35.0309089009928, -68.37273832058204, 56.17891993445953, -59.15713286968831, 42.25325437412105, 56.80842110132606, 55.53054950267411, -9.177608495945805, -77.81921121589357, 65.21308646219038, 62.49108356842254, 12.741858307028297, -24.85757588230601, -18.649611629070684, 56.52385199815108, -75.42249826145677, -54.31274572493209, -5.702880840898423, -14.845003339853676, 62.139949033217064, -76.94909912560863, -53.34716095003923, 23.047397680723993, -2.064564876342241, -79.3899091782446, -43.84424753910914, -70.69427065088561, 19.184789462427165, 22.535470265030263, -59.6889204569689, 27.533650167186124, -69.53549690609026, -20.724424123736437, 23.039800134933284, 37.270556668612706, 3.7244960956477287, 74.25237198758562, -77.38375068727784, -17.41660469040239, 39.887778141902224, -62.01905377083551, 27.31428656564015, -0.5739090651518001, 65.66822731177001, -55.91856697093791, 71.76785004690414, 7.859025079419774, 52.23517029698031, -23.90415412327315, -17.099980278469516, -24.0502802730283, -55.42742268369731, -28.719469711835107, -20.52597913604415, 45.43350736166251, -50.83611751339575, 13.17757377336463, -46.13113592905156, 70.73486305509799, 63.250319480920126, -6.112316827673773, 63.55329307934058, -43.18945824187807, -44.472451104533334, -70.35015426723604, -67.90270194944512, -16.009688094477692, 27.705970461872422, -51.34328442101025, 63.533954528093815, -43.335532774936034, 14.582347096384893, 10.262304214310241, 38.048468775416616, -32.95241405013405, 0.08857997014854324, 41.14102951102395, 61.42589067854003, -24.543057166362207, 68.17747689185381, -57.54492615324598, -65.32510543447147, -14.49428821352906, 49.461645549933564, -20.980230019157162, 73.21474583099143, 20.05430846315648, 23.217598196250577, 70.63049615664409, -18.763097623708436, -8.140337749541825, 3.409317994940256, 70.07910068980772, 36.18128697849056, 10.93435809845026, -73.5712318897667, -22.993891779846635, -55.79350181415773, -74.6785288002736, -13.280155795406086, -4.116607696152617, 36.9751835626544, 5.498619992811967, 45.22039917828877, 48.630141325746536, 47.009303173788865, -70.79694414286081, 0.7703509700976618, -33.77686450451795, 20.55697785521576, -18.985667454597415, 67.548781585708, -72.17374735999877, -5.9297427424158595, 76.7988940732271, -20.646110000575494, 31.98654584024554, 46.723289597599404, -60.99041670481695, 48.661616364266024, 2.507637862981946, 31.858235111826687, 44.17365726072251, 49.225646212255675, 30.57524776858007, -67.49732858580354, -40.45260317109873, -51.56445686939825, 15.084356796050074, -61.06488583917624, -24.860448916430954, -38.971326250733156, -10.77788560291002, 65.19004450104796, -17.933037276478366, 48.74613505704338, -16.512853722496743, -26.466967856053998, -16.354566949263614, 5.643463873740834, 36.79781106863564, -45.94745310618842, 61.74906244151409, 19.197924594536588, 33.33941797300575, 51.71480689497904, -74.8687194109773, -80.9588091678278, 12.663116826807249, -19.59793325650537, -63.37789938601465, 37.16580139049759, 63.26720096395629, 3.484628833476095, -20.198627441269288, 50.15230922534401, 73.44217542157605, -34.95906050285074, 35.898424586914444, -79.72121763186297, -31.515460513003845, -71.59771577927779, -37.95324757787687, 76.87398490683215, 25.634677963834484, 27.58655198917177, 2.6476545453513722, 76.8932846687849, 32.76029690630864, 24.401832108357837, 77.31906754572582, 54.17970736682746, 21.531290903053435, -38.13658822408581, -18.499556837155176, -14.196231705310167, -62.67500815883237, -76.79033519144772, -3.6436489097663434, -28.886664127503607, -59.45562755244482, -30.99331897310827, -45.14670132445652, -37.23438974830381, 53.900008416333755, -21.139596325417145, 29.11570214958149, 53.751500601902784, -1.4958511518647564, 78.18914768565183, -28.913107718562046, -70.26764425210371, 51.57857078901283, -3.2882335832520084, 3.6953210226599884, -57.840096506113895, -50.53522361380188, -79.6304317852493, 34.24967655668856, 30.738025062786985, 53.39342607091456, 38.82395642333028, -36.35587860510555, -27.168793028875424, 8.57740011950094, 24.50799107002731, 52.6786757576023, 40.830527891922884, -42.79383595294746, 29.642799270587947, 56.50958319030771, -61.913334291035355, -49.07761834837158, -63.14547646432972, -73.59467725142254, 15.308349588728019, 61.783385798402236, -71.84176577870272, -24.20758771231678, -47.24859354233125, -17.22350599966674, 14.889967170993694, 79.74779597170748, 3.7443622998860926, 38.958529736042735, -67.87011849906543, -64.52166822755883, 68.21283823525317, -53.378822729227814, 33.579803119601344, -3.411638985684256, -48.80240395763695, -11.893389640681386, -27.75520083011947, -46.341309066531586, 3.937684868074688, -33.20145711263068, -28.811473902865654, 78.86391921192403, 37.06761299096005, -55.38650627229583, 46.10632959130118, -8.302586980958479, -59.81293260761952, -12.508742059752748, 28.08499610243297, -65.64877608764596, -22.808293800060863, 17.759869355659514, -38.13378728721236, -78.68374401133045, -8.31968459889333, 14.213770287013464, 20.035041891228097, 8.06172117468654, 40.60739079493747, -62.54601282464398, -13.94238638401906, -32.38695664375088, 18.655317178741843, 20.17501129552794, -22.36421783277388, 13.523376750114707, 66.31838110751173, -23.04526679247096, 24.519201169888664, -50.637105642333495, 49.47842389325933, -47.086387930807064, 55.61303038170645, -10.420081414333518, 50.10655702149808, 72.83572519798673, -43.004974896868966, 18.943993279149858, 69.2297024821104, -15.200907742870825, -60.99399151373454, 64.73549182885954, -76.06201431434293, 23.11336247858624, -55.67589186402138, -70.95644955622743, -48.934197460897664, -60.17271207852024, 30.667596918174652, -15.411060005338424, 28.751680536486305, 22.92710587446319, -26.65747787855355, 60.431736556405326, -20.05144667979933, -1.950993552757063, 60.92015613647496, -22.18568059575963, -38.09448122480918, 55.17276526058485, -77.77432536817831, -76.27675673295346, -36.6133124096198, -26.35734471618566, 42.76638696669606, 48.102266488119, -49.718162619500795, 13.30493347014252, 59.46463786504638, -58.148710848120096, -57.756797511410824, 60.13915946845509, 32.20608377660124, 29.83410060663515, -49.8290583184013, -35.436244718173526, 4.159393501283766, -80.85055991605797, -3.6823734694265027, -71.8152561925458, 21.61404741968409, -12.546437768335295, -28.573891503236013, -4.8252816168728545, -8.544532784776884, -15.956221201802073, -4.238002987099948, -18.50894438072679, -77.02577821330011, 36.900821299684395, 20.651431185953218, -31.815602611251823, -7.892622394790828, -49.36793053826495, 78.60232125772451, 44.892338143825334, -77.08248753067284, -66.12749412077001, 54.39829410231671, -62.69718831457336, 69.9706911695693, -51.18410043755646, 2.88260682379726, -32.01552024871244, 38.612846867676446, 47.10065786775252, 54.35232319889596, -76.99404337271734, -57.317028904576134, 71.39431724279527, 7.491413942342408, -51.926069016846036, 16.793291493096305, -11.399028942955574, 47.04682703369081, 72.10178422358153, 11.836681566870945, 23.62939822195924, -9.452058909965992, 30.116959377723166, 21.12384054058441, -15.38850581849945, -24.77150471896578, -2.634032553125338, -58.517282520302956, 26.372546804129342, -7.826866865669345, 30.543311392281538, -73.08655261661293, -55.3269838313834, 43.84568279760918, -43.06106786907455, -4.610537463549946, 54.627569723074096, 43.02774282311498, 43.957437227495944, 7.647678226470255, -34.6821656057775, -4.1162772366993226, 67.58326802608323, -14.591966960012615, 26.62739839666699, -28.017638783148158, 76.30927806702154, 51.514886401261954, 70.65870867318155, -30.18687125002857, -12.506886166136214, -25.07591838795517, 72.94970784712069, -0.794293080828154, -17.746755700060632, -20.3398232673725, 1.5667146308949351, -15.903794899436443, -50.96409149073151, -10.936435255507053, -6.095511584910358, -41.364748143684814, -47.229688224445944, -74.00986680078069, 35.68873237553772, -46.15818430883783, -33.14097969050712, 77.02835098616701, 8.175769088761363, -33.10571227250835, -5.373656017237824, -35.112947198495036, 22.041662565006938, -59.17005015837339, 60.45421767245787, 68.99137560596601, 72.03159251644149, 25.497284427396085, -31.040668779996853, -1.817038064261916, 72.07474778680991, -71.3767029477868, 49.43173885540921, -42.87304361417778, -44.82836380200943, 41.30756325733414, 72.8048748260606, -45.475200830847506, -58.003897368550156, -30.006395135594236, 12.53580557828999, -76.09542803673638, 72.57081756537107, 43.830083802325454, 32.91578652664056]\n",
      "Best fitness: 0.0\n",
      "Total optimization time: 196.46507740020752 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from deap import base, creator, tools, algorithms\n",
    "#from your_environment import PowerSystemEnv  # Make sure to import your environment correctly\n",
    "\n",
    "# Initialize the environment\n",
    "env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "# Constants\n",
    "T = 48  # Time horizon\n",
    "NDim = T * env.action_space.shape[0]\n",
    "NParticle = 50  # Population size\n",
    "NGEN = 200  # Number of generations\n",
    "\n",
    "# Define bounds\n",
    "low_bound = np.tile(env.action_space.low, T)\n",
    "high_bound = np.tile(env.action_space.high, T)\n",
    "\n",
    "# Fitness function\n",
    "def f(individual):\n",
    "    cumulative_reward = 0\n",
    "    env.reset()\n",
    "    actions = np.clip(np.array(individual).reshape(T, env.action_space.shape[0]), env.action_space.low, env.action_space.high)\n",
    "    for t in range(T):\n",
    "        obs, reward, done, truncated, info = env.step(actions[t])\n",
    "        cumulative_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return -cumulative_reward,\n",
    "\n",
    "# Set up GA\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", random.uniform, low_bound[0], high_bound[0])\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, NDim)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", f)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Optimization\n",
    "start_time = time.time()\n",
    "population = toolbox.population(n=NParticle)\n",
    "final_population = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=NGEN, verbose=False)[0]\n",
    "best_ind = tools.selBest(final_population, 1)[0]\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Best individual: {best_ind}\")\n",
    "print(f\"Best fitness: {best_ind.fitness.values[0]}\")\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13991e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyswarms\n",
      "  Using cached pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from pyswarms) (1.11.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from pyswarms) (1.22.4)\n",
      "Requirement already satisfied: matplotlib>=1.3.1 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from pyswarms) (3.7.2)\n",
      "Requirement already satisfied: attrs in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from pyswarms) (22.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from pyswarms) (4.66.1)\n",
      "Requirement already satisfied: future in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from pyswarms) (0.18.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from pyswarms) (6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from matplotlib>=1.3.1->pyswarms) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from tqdm->pyswarms) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.16.0)\n",
      "Installing collected packages: pyswarms\n",
      "Successfully installed pyswarms-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\alaa\\.conda\\envs\\py310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36ad840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 20:50:02,388 - pyswarms.single.global_best - INFO - Optimize for 2 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%||2/2, best_cost=0\n",
      "2024-01-29 20:50:02,624 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.0, best pos: [-29.84088979  -5.66003959 -59.33461352 ...   0.19653358   0.24299219\n",
      "   0.54092495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best position: [-29.84088979  -5.66003959 -59.33461352 ...   0.19653358   0.24299219\n",
      "   0.54092495]\n",
      "Best cost: 0.0\n",
      "Total optimization time: 0.23698687553405762 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyswarms as ps\n",
    "import time\n",
    "\n",
    "# Initialize the environment\n",
    "env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "# Constants\n",
    "T = 48  # Time horizon\n",
    "NDim = T * env.action_space.shape[0]\n",
    "NParticle = 10 # Number of particles\n",
    "MaxIters = 2  # Number of iterations\n",
    "\n",
    "# Define bounds\n",
    "bounds = (np.tile(env.action_space.low, T), np.tile(env.action_space.high, T))\n",
    "\n",
    "# Fitness function\n",
    "def f(x):\n",
    "    n_particles = x.shape[0]\n",
    "    j = [0] * n_particles\n",
    "    for i in range(n_particles):\n",
    "        cumulative_reward = 0\n",
    "        env.reset()\n",
    "        actions = np.clip(x[i].reshape(T, env.action_space.shape[0]), env.action_space.low, env.action_space.high)\n",
    "        for t in range(T):\n",
    "            obs, reward, done, truncated, info = env.step(actions[t])\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        j[i] = -cumulative_reward\n",
    "    return np.array(j)\n",
    "\n",
    "# Initialize swarm\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=NParticle, dimensions=NDim, options=options, bounds=bounds)\n",
    "\n",
    "# Perform optimization\n",
    "start_time = time.time()\n",
    "cost, pos = optimizer.optimize(f, iters=MaxIters)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Best position: {pos}\")\n",
    "print(f\"Best cost: {cost}\")\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39cc0dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 20:58:47,527 - pyswarms.single.global_best - INFO - Optimize for 10 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%||10/10, best_cost=0\n",
      "2024-01-29 21:01:41,632 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.0, best pos: [ 53.72559027  49.81846992 -13.19211692 ...   0.41939655   0.73399681\n",
      "   0.77626116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best position: [ 53.72559027  49.81846992 -13.19211692 ...   0.41939655   0.73399681\n",
      "   0.77626116]\n",
      "Size of best position vector: (4560,)\n",
      "Best cost: 0.0\n",
      "Average cost: 0.0\n",
      "Total optimization time: 174.1072781085968 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyswarms as ps\n",
    "import time\n",
    "\n",
    "# Initialize the environment\n",
    "env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "# Constants\n",
    "T = 48  # Time horizon\n",
    "NDim = T * env.action_space.shape[0]\n",
    "NParticle = 1000  # Number of particles\n",
    "MaxIters = 10  # Number of iterations\n",
    "\n",
    "# Define bounds\n",
    "bounds = (np.tile(env.action_space.low, T), np.tile(env.action_space.high, T))\n",
    "\n",
    "# Fitness function\n",
    "def f(x):\n",
    "    n_particles = x.shape[0]\n",
    "    j = [0] * n_particles\n",
    "    for i in range(n_particles):\n",
    "        cumulative_reward = 0\n",
    "        env.reset()\n",
    "        actions = np.clip(x[i].reshape(T, env.action_space.shape[0]), env.action_space.low, env.action_space.high)\n",
    "        for t in range(T):\n",
    "            obs, reward, done, truncated, info = env.step(actions[t])\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        j[i] = -cumulative_reward\n",
    "    return np.array(j)\n",
    "\n",
    "# Initialize swarm\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=NParticle, dimensions=NDim, options=options, bounds=bounds)\n",
    "\n",
    "# Perform optimization\n",
    "start_time = time.time()\n",
    "cost, pos = optimizer.optimize(f, iters=MaxIters, verbose=True)\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute average cost\n",
    "average_cost = np.mean(optimizer.cost_history)\n",
    "\n",
    "print(f\"Best position: {pos}\")\n",
    "print(f\"Size of best position vector: {pos.shape}\")\n",
    "print(f\"Best cost: {cost}\")\n",
    "print(f\"Average cost: {average_cost}\")\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d60b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n",
      "Iteration 1, Average Reward: -144.864\n",
      "Iteration 2, Average Reward: -144.167\n",
      "Iteration 3, Average Reward: -155.552\n",
      "Iteration 4, Average Reward: -151.402\n",
      "Iteration 5, Average Reward: -149.658\n",
      "Iteration 6, Average Reward: -144.266\n",
      "Iteration 7, Average Reward: -142.753\n",
      "Iteration 8, Average Reward: -147.425\n",
      "Iteration 9, Average Reward: -146.578\n",
      "Iteration 10, Average Reward: -146.834\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GlobalBestPSO' object has no attribute 'get_cost_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Final results\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m best_cost \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cost_history\u001b[49m()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     61\u001b[0m best_position \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mget_pos_history()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Best Position: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_position\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalBestPSO' object has no attribute 'get_cost_history'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyswarms as ps\n",
    "import time\n",
    "\n",
    "# Assuming PowerSystemEnv and other necessary imports are correctly set up\n",
    "# from your_environment import PowerSystemEnv\n",
    "\n",
    "# Initialize the environment\n",
    "env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "# Constants\n",
    "T = 48  # Time horizon\n",
    "NDim = T * env.action_space.shape[0]\n",
    "NParticle = 100  # Number of particles\n",
    "MaxIters = 1000  # Number of iterations\n",
    "\n",
    "# Define bounds\n",
    "bounds = (np.tile(env.action_space.low, T), np.tile(env.action_space.high, T))\n",
    "\n",
    "# Fitness function\n",
    "def f(x):\n",
    "    n_particles = x.shape[0]\n",
    "    j = np.zeros(n_particles)\n",
    "    for i in range(n_particles):\n",
    "        cumulative_reward = 0\n",
    "        env.reset()\n",
    "        actions = np.clip(x[i].reshape(T, env.action_space.shape[0]), env.action_space.low, env.action_space.high)\n",
    "        for t in range(T):\n",
    "            obs, reward, done, truncated, info = env.step(actions[t])\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        j[i] = cumulative_reward\n",
    "    return j\n",
    "\n",
    "# Initialize swarm with adjusted hyperparameters for more exploration\n",
    "options = {'c1': 1.0, 'c2': 0.5, 'w': 0.9}\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=NParticle, dimensions=NDim, options=options, bounds=bounds)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Manually iterate through the optimization process\n",
    "for i in range(MaxIters):\n",
    "    optimizer.optimize(f, iters=1, verbose=False)  # Run one iteration at a time\n",
    "\n",
    "    # Calculate the average reward for this iteration\n",
    "    current_rewards = f(optimizer.swarm.position)\n",
    "    average_reward = np.mean(current_rewards)\n",
    "    \n",
    "    print(f\"Iteration {i+1}, Average Reward: {average_reward}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Final results\n",
    "best_cost = optimizer.get_cost_history()[-1]\n",
    "best_position = optimizer.get_pos_history()[-1]\n",
    "\n",
    "print(f\"Final Best Position: {best_position}\")\n",
    "print(f\"Final Best Cost: {best_cost}\")\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7b413ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenDSS Started successfully! \n",
      "OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\n",
      "\n",
      "Iteration 1, Average Reward: -169.27\n",
      "Iteration 2, Average Reward: -131.73\n",
      "Iteration 3, Average Reward: -149.74\n",
      "Iteration 4, Average Reward: -143.47\n",
      "Iteration 5, Average Reward: -150.48\n",
      "Iteration 6, Average Reward: -148.01\n",
      "Iteration 7, Average Reward: -130.86\n",
      "Iteration 8, Average Reward: -125.29\n",
      "Iteration 9, Average Reward: -144.15\n",
      "Iteration 10, Average Reward: -136.0\n",
      "Iteration 11, Average Reward: -137.75\n",
      "Iteration 12, Average Reward: -142.56\n",
      "Iteration 13, Average Reward: -146.86\n",
      "Iteration 14, Average Reward: -150.69\n",
      "Iteration 15, Average Reward: -146.98\n",
      "Iteration 16, Average Reward: -158.22\n",
      "Iteration 17, Average Reward: -139.64\n",
      "Iteration 18, Average Reward: -152.73\n",
      "Iteration 19, Average Reward: -137.17\n",
      "Iteration 20, Average Reward: -149.03\n",
      "Iteration 21, Average Reward: -136.13\n",
      "Iteration 22, Average Reward: -144.28\n",
      "Iteration 23, Average Reward: -146.88\n",
      "Iteration 24, Average Reward: -135.25\n",
      "Iteration 25, Average Reward: -104.24\n",
      "Iteration 26, Average Reward: -130.29\n",
      "Iteration 27, Average Reward: -149.1\n",
      "Iteration 28, Average Reward: -132.68\n",
      "Iteration 29, Average Reward: -101.22\n",
      "Iteration 30, Average Reward: -108.39\n",
      "Iteration 31, Average Reward: -113.41\n",
      "Iteration 32, Average Reward: -122.03\n",
      "Iteration 33, Average Reward: -127.7\n",
      "Iteration 34, Average Reward: -139.85\n",
      "Iteration 35, Average Reward: -129.96\n",
      "Iteration 36, Average Reward: -126.81\n",
      "Iteration 37, Average Reward: -109.51\n",
      "Iteration 38, Average Reward: -123.63\n",
      "Iteration 39, Average Reward: -118.38\n",
      "Iteration 40, Average Reward: -119.89\n",
      "Iteration 41, Average Reward: -127.99\n",
      "Iteration 42, Average Reward: -119.06\n",
      "Iteration 43, Average Reward: -124.93\n",
      "Iteration 44, Average Reward: -104.85\n",
      "Iteration 45, Average Reward: -126.62\n",
      "Iteration 46, Average Reward: -135.53\n",
      "Iteration 47, Average Reward: -116.69\n",
      "Iteration 48, Average Reward: -129.38\n",
      "Iteration 49, Average Reward: -127.12\n",
      "Iteration 50, Average Reward: -128.02\n",
      "Iteration 51, Average Reward: -126.65\n",
      "Iteration 52, Average Reward: -120.7\n",
      "Iteration 53, Average Reward: -113.39\n",
      "Iteration 54, Average Reward: -102.83\n",
      "Iteration 55, Average Reward: -116.67\n",
      "Iteration 56, Average Reward: -109.95\n",
      "Iteration 57, Average Reward: -103.98\n",
      "Iteration 58, Average Reward: -113.03\n",
      "Iteration 59, Average Reward: -99.8\n",
      "Iteration 60, Average Reward: -113.93\n",
      "Iteration 61, Average Reward: -125.62\n",
      "Iteration 62, Average Reward: -123.83\n",
      "Iteration 63, Average Reward: -114.42\n",
      "Iteration 64, Average Reward: -97.82\n",
      "Iteration 65, Average Reward: -91.29\n",
      "Iteration 66, Average Reward: -86.79\n",
      "Iteration 67, Average Reward: -96.82\n",
      "Iteration 68, Average Reward: -108.65\n",
      "Iteration 69, Average Reward: -130.69\n",
      "Iteration 70, Average Reward: -142.59\n",
      "Iteration 71, Average Reward: -146.76\n",
      "Iteration 72, Average Reward: -115.91\n",
      "Iteration 73, Average Reward: -98.87\n",
      "Iteration 74, Average Reward: -121.5\n",
      "Iteration 75, Average Reward: -120.94\n",
      "Iteration 76, Average Reward: -114.69\n",
      "Iteration 77, Average Reward: -135.61\n",
      "Iteration 78, Average Reward: -125.1\n",
      "Iteration 79, Average Reward: -105.65\n",
      "Iteration 80, Average Reward: -107.39\n",
      "Iteration 81, Average Reward: -127.47\n",
      "Iteration 82, Average Reward: -127.25\n",
      "Iteration 83, Average Reward: -117.69\n",
      "Iteration 84, Average Reward: -142.53\n",
      "Iteration 85, Average Reward: -144.44\n",
      "Iteration 86, Average Reward: -115.94\n",
      "Iteration 87, Average Reward: -110.04\n",
      "Iteration 88, Average Reward: -126.85\n",
      "Iteration 89, Average Reward: -116.79\n",
      "Iteration 90, Average Reward: -127.14\n",
      "Iteration 91, Average Reward: -124.66\n",
      "Iteration 92, Average Reward: -124.23\n",
      "Iteration 93, Average Reward: -129.62\n",
      "Iteration 94, Average Reward: -138.11\n",
      "Iteration 95, Average Reward: -137.08\n",
      "Iteration 96, Average Reward: -123.64\n",
      "Iteration 97, Average Reward: -113.17\n",
      "Iteration 98, Average Reward: -106.23\n",
      "Iteration 99, Average Reward: -110.69\n",
      "Iteration 100, Average Reward: -118.53\n",
      "Iteration 101, Average Reward: -123.66\n",
      "Iteration 102, Average Reward: -118.39\n",
      "Iteration 103, Average Reward: -123.54\n",
      "Iteration 104, Average Reward: -134.82\n",
      "Iteration 105, Average Reward: -135.77\n",
      "Iteration 106, Average Reward: -124.68\n",
      "Iteration 107, Average Reward: -117.64\n",
      "Iteration 108, Average Reward: -111.09\n",
      "Iteration 109, Average Reward: -125.73\n",
      "Iteration 110, Average Reward: -132.53\n",
      "Iteration 111, Average Reward: -124.18\n",
      "Iteration 112, Average Reward: -121.47\n",
      "Iteration 113, Average Reward: -112.17\n",
      "Iteration 114, Average Reward: -125.3\n",
      "Iteration 115, Average Reward: -127.86\n",
      "Iteration 116, Average Reward: -128.25\n",
      "Iteration 117, Average Reward: -116.7\n",
      "Iteration 118, Average Reward: -116.32\n",
      "Iteration 119, Average Reward: -127.44\n",
      "Iteration 120, Average Reward: -136.23\n",
      "Iteration 121, Average Reward: -128.32\n",
      "Iteration 122, Average Reward: -138.87\n",
      "Iteration 123, Average Reward: -113.91\n",
      "Iteration 124, Average Reward: -108.1\n",
      "Iteration 125, Average Reward: -89.14\n",
      "Iteration 126, Average Reward: -108.64\n",
      "Iteration 127, Average Reward: -122.3\n",
      "Iteration 128, Average Reward: -116.87\n",
      "Iteration 129, Average Reward: -127.86\n",
      "Iteration 130, Average Reward: -150.35\n",
      "Iteration 131, Average Reward: -146.61\n",
      "Iteration 132, Average Reward: -136.14\n",
      "Iteration 133, Average Reward: -131.75\n",
      "Iteration 134, Average Reward: -115.14\n",
      "Iteration 135, Average Reward: -90.12\n",
      "Iteration 136, Average Reward: -110.94\n",
      "Iteration 137, Average Reward: -116.29\n",
      "Iteration 138, Average Reward: -130.96\n",
      "Iteration 139, Average Reward: -129.19\n",
      "Iteration 140, Average Reward: -118.71\n",
      "Iteration 141, Average Reward: -111.55\n",
      "Iteration 142, Average Reward: -120.62\n",
      "Iteration 143, Average Reward: -119.86\n",
      "Iteration 144, Average Reward: -119.04\n",
      "Iteration 145, Average Reward: -135.1\n",
      "Iteration 146, Average Reward: -119.01\n",
      "Iteration 147, Average Reward: -110.32\n",
      "Iteration 148, Average Reward: -116.69\n",
      "Iteration 149, Average Reward: -122.22\n",
      "Iteration 150, Average Reward: -115.8\n",
      "Iteration 151, Average Reward: -119.19\n",
      "Iteration 152, Average Reward: -140.08\n",
      "Iteration 153, Average Reward: -121.13\n",
      "Iteration 154, Average Reward: -109.61\n",
      "Iteration 155, Average Reward: -124.53\n",
      "Iteration 156, Average Reward: -117.36\n",
      "Iteration 157, Average Reward: -119.87\n",
      "Iteration 158, Average Reward: -106.11\n",
      "Iteration 159, Average Reward: -115.99\n",
      "Iteration 160, Average Reward: -122.73\n",
      "Iteration 161, Average Reward: -105.5\n",
      "Iteration 162, Average Reward: -92.84\n",
      "Iteration 163, Average Reward: -111.91\n",
      "Iteration 164, Average Reward: -134.6\n",
      "Iteration 165, Average Reward: -128.81\n",
      "Iteration 166, Average Reward: -137.7\n",
      "Iteration 167, Average Reward: -147.75\n",
      "Iteration 168, Average Reward: -129.61\n",
      "Iteration 169, Average Reward: -127.69\n",
      "Iteration 170, Average Reward: -129.55\n",
      "Iteration 171, Average Reward: -128.27\n",
      "Iteration 172, Average Reward: -117.41\n",
      "Iteration 173, Average Reward: -118.08\n",
      "Iteration 174, Average Reward: -135.81\n",
      "Iteration 175, Average Reward: -136.16\n",
      "Iteration 176, Average Reward: -123.03\n",
      "Iteration 177, Average Reward: -116.42\n",
      "Iteration 178, Average Reward: -134.92\n",
      "Iteration 179, Average Reward: -118.43\n",
      "Iteration 180, Average Reward: -123.17\n",
      "Iteration 181, Average Reward: -135.33\n",
      "Iteration 182, Average Reward: -144.94\n",
      "Iteration 183, Average Reward: -142.13\n",
      "Iteration 184, Average Reward: -113.21\n",
      "Iteration 185, Average Reward: -119.21\n",
      "Iteration 186, Average Reward: -107.64\n",
      "Iteration 187, Average Reward: -108.43\n",
      "Iteration 188, Average Reward: -104.02\n",
      "Iteration 189, Average Reward: -116.24\n",
      "Iteration 190, Average Reward: -118.77\n",
      "Iteration 191, Average Reward: -128.8\n",
      "Iteration 192, Average Reward: -116.88\n",
      "Iteration 193, Average Reward: -120.46\n",
      "Iteration 194, Average Reward: -123.43\n",
      "Iteration 195, Average Reward: -126.44\n",
      "Iteration 196, Average Reward: -130.37\n",
      "Iteration 197, Average Reward: -127.58\n",
      "Iteration 198, Average Reward: -124.18\n",
      "Iteration 199, Average Reward: -110.83\n",
      "Iteration 200, Average Reward: -106.1\n",
      "Iteration 201, Average Reward: -115.27\n",
      "Iteration 202, Average Reward: -122.98\n",
      "Iteration 203, Average Reward: -114.15\n",
      "Iteration 204, Average Reward: -113.17\n",
      "Iteration 205, Average Reward: -137.68\n",
      "Iteration 206, Average Reward: -122.44\n",
      "Iteration 207, Average Reward: -110.92\n",
      "Iteration 208, Average Reward: -102.66\n",
      "Iteration 209, Average Reward: -110.92\n",
      "Iteration 210, Average Reward: -112.65\n",
      "Iteration 211, Average Reward: -121.27\n",
      "Iteration 212, Average Reward: -130.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 213, Average Reward: -132.37\n",
      "Iteration 214, Average Reward: -129.2\n",
      "Iteration 215, Average Reward: -129.07\n",
      "Iteration 216, Average Reward: -123.35\n",
      "Iteration 217, Average Reward: -122.4\n",
      "Iteration 218, Average Reward: -138.61\n",
      "Iteration 219, Average Reward: -131.0\n",
      "Iteration 220, Average Reward: -114.39\n",
      "Iteration 221, Average Reward: -139.63\n",
      "Iteration 222, Average Reward: -149.89\n",
      "Iteration 223, Average Reward: -142.58\n",
      "Iteration 224, Average Reward: -135.78\n",
      "Iteration 225, Average Reward: -119.53\n",
      "Iteration 226, Average Reward: -124.95\n",
      "Iteration 227, Average Reward: -125.94\n",
      "Iteration 228, Average Reward: -119.54\n",
      "Iteration 229, Average Reward: -115.85\n",
      "Iteration 230, Average Reward: -126.96\n",
      "Iteration 231, Average Reward: -132.44\n",
      "Iteration 232, Average Reward: -125.68\n",
      "Iteration 233, Average Reward: -122.17\n",
      "Iteration 234, Average Reward: -130.71\n",
      "Iteration 235, Average Reward: -120.3\n",
      "Iteration 236, Average Reward: -125.53\n",
      "Iteration 237, Average Reward: -131.76\n",
      "Iteration 238, Average Reward: -112.99\n",
      "Iteration 239, Average Reward: -108.74\n",
      "Iteration 240, Average Reward: -119.27\n",
      "Iteration 241, Average Reward: -131.8\n",
      "Iteration 242, Average Reward: -130.68\n",
      "Iteration 243, Average Reward: -120.76\n",
      "Iteration 244, Average Reward: -125.99\n",
      "Iteration 245, Average Reward: -141.37\n",
      "Iteration 246, Average Reward: -127.65\n",
      "Iteration 247, Average Reward: -114.43\n",
      "Iteration 248, Average Reward: -130.73\n",
      "Iteration 249, Average Reward: -121.99\n",
      "Iteration 250, Average Reward: -110.51\n",
      "Iteration 251, Average Reward: -114.66\n",
      "Iteration 252, Average Reward: -133.11\n",
      "Iteration 253, Average Reward: -141.03\n",
      "Iteration 254, Average Reward: -142.29\n",
      "Iteration 255, Average Reward: -133.64\n",
      "Iteration 256, Average Reward: -109.84\n",
      "Iteration 257, Average Reward: -114.37\n",
      "Iteration 258, Average Reward: -124.1\n",
      "Iteration 259, Average Reward: -117.45\n",
      "Iteration 260, Average Reward: -126.76\n",
      "Iteration 261, Average Reward: -146.86\n",
      "Iteration 262, Average Reward: -149.9\n",
      "Iteration 263, Average Reward: -123.07\n",
      "Iteration 264, Average Reward: -133.11\n",
      "Iteration 265, Average Reward: -137.03\n",
      "Iteration 266, Average Reward: -124.9\n",
      "Iteration 267, Average Reward: -110.85\n",
      "Iteration 268, Average Reward: -118.24\n",
      "Iteration 269, Average Reward: -129.16\n",
      "Iteration 270, Average Reward: -126.16\n",
      "Iteration 271, Average Reward: -144.04\n",
      "Iteration 272, Average Reward: -150.5\n",
      "Iteration 273, Average Reward: -135.45\n",
      "Iteration 274, Average Reward: -118.95\n",
      "Iteration 275, Average Reward: -115.97\n",
      "Iteration 276, Average Reward: -137.33\n",
      "Iteration 277, Average Reward: -124.57\n",
      "Iteration 278, Average Reward: -135.58\n",
      "Iteration 279, Average Reward: -149.31\n",
      "Iteration 280, Average Reward: -129.39\n",
      "Iteration 281, Average Reward: -138.23\n",
      "Iteration 282, Average Reward: -144.86\n",
      "Iteration 283, Average Reward: -129.94\n",
      "Iteration 284, Average Reward: -109.93\n",
      "Iteration 285, Average Reward: -117.08\n",
      "Iteration 286, Average Reward: -115.83\n",
      "Iteration 287, Average Reward: -119.24\n",
      "Iteration 288, Average Reward: -144.15\n",
      "Iteration 289, Average Reward: -141.99\n",
      "Iteration 290, Average Reward: -130.76\n",
      "Iteration 291, Average Reward: -137.88\n",
      "Iteration 292, Average Reward: -132.71\n",
      "Iteration 293, Average Reward: -108.93\n",
      "Iteration 294, Average Reward: -100.28\n",
      "Iteration 295, Average Reward: -117.5\n",
      "Iteration 296, Average Reward: -116.21\n",
      "Iteration 297, Average Reward: -119.12\n",
      "Iteration 298, Average Reward: -113.5\n",
      "Iteration 299, Average Reward: -108.88\n",
      "Iteration 300, Average Reward: -133.76\n",
      "Iteration 301, Average Reward: -121.0\n",
      "Iteration 302, Average Reward: -109.47\n",
      "Iteration 303, Average Reward: -107.42\n",
      "Iteration 304, Average Reward: -118.98\n",
      "Iteration 305, Average Reward: -125.51\n",
      "Iteration 306, Average Reward: -115.92\n",
      "Iteration 307, Average Reward: -135.79\n",
      "Iteration 308, Average Reward: -140.23\n",
      "Iteration 309, Average Reward: -137.93\n",
      "Iteration 310, Average Reward: -124.57\n",
      "Iteration 311, Average Reward: -128.35\n",
      "Iteration 312, Average Reward: -128.81\n",
      "Iteration 313, Average Reward: -120.2\n",
      "Iteration 314, Average Reward: -130.57\n",
      "Iteration 315, Average Reward: -138.74\n",
      "Iteration 316, Average Reward: -118.16\n",
      "Iteration 317, Average Reward: -120.29\n",
      "Iteration 318, Average Reward: -136.11\n",
      "Iteration 319, Average Reward: -119.27\n",
      "Iteration 320, Average Reward: -121.14\n",
      "Iteration 321, Average Reward: -129.97\n",
      "Iteration 322, Average Reward: -109.56\n",
      "Iteration 323, Average Reward: -121.23\n",
      "Iteration 324, Average Reward: -144.66\n",
      "Iteration 325, Average Reward: -138.82\n",
      "Iteration 326, Average Reward: -121.57\n",
      "Iteration 327, Average Reward: -115.13\n",
      "Iteration 328, Average Reward: -112.23\n",
      "Iteration 329, Average Reward: -108.27\n",
      "Iteration 330, Average Reward: -116.75\n",
      "Iteration 331, Average Reward: -146.22\n",
      "Iteration 332, Average Reward: -131.91\n",
      "Iteration 333, Average Reward: -112.77\n",
      "Iteration 334, Average Reward: -105.37\n",
      "Iteration 335, Average Reward: -106.38\n",
      "Iteration 336, Average Reward: -99.85\n",
      "Iteration 337, Average Reward: -117.88\n",
      "Iteration 338, Average Reward: -137.77\n",
      "Iteration 339, Average Reward: -146.76\n",
      "Iteration 340, Average Reward: -129.61\n",
      "Iteration 341, Average Reward: -140.17\n",
      "Iteration 342, Average Reward: -118.8\n",
      "Iteration 343, Average Reward: -120.81\n",
      "Iteration 344, Average Reward: -125.07\n",
      "Iteration 345, Average Reward: -128.58\n",
      "Iteration 346, Average Reward: -122.1\n",
      "Iteration 347, Average Reward: -137.4\n",
      "Iteration 348, Average Reward: -129.86\n",
      "Iteration 349, Average Reward: -118.8\n",
      "Iteration 350, Average Reward: -126.64\n",
      "Iteration 351, Average Reward: -127.29\n",
      "Iteration 352, Average Reward: -121.48\n",
      "Iteration 353, Average Reward: -136.38\n",
      "Iteration 354, Average Reward: -128.05\n",
      "Iteration 355, Average Reward: -145.34\n",
      "Iteration 356, Average Reward: -131.2\n",
      "Iteration 357, Average Reward: -132.44\n",
      "Iteration 358, Average Reward: -130.49\n",
      "Iteration 359, Average Reward: -123.1\n",
      "Iteration 360, Average Reward: -119.16\n",
      "Iteration 361, Average Reward: -125.66\n",
      "Iteration 362, Average Reward: -131.22\n",
      "Iteration 363, Average Reward: -129.17\n",
      "Iteration 364, Average Reward: -123.07\n",
      "Iteration 365, Average Reward: -109.62\n",
      "Iteration 366, Average Reward: -100.34\n",
      "Iteration 367, Average Reward: -94.14\n",
      "Iteration 368, Average Reward: -113.29\n",
      "Iteration 369, Average Reward: -125.38\n",
      "Iteration 370, Average Reward: -124.86\n",
      "Iteration 371, Average Reward: -121.1\n",
      "Iteration 372, Average Reward: -121.15\n",
      "Iteration 373, Average Reward: -122.69\n",
      "Iteration 374, Average Reward: -116.57\n",
      "Iteration 375, Average Reward: -121.26\n",
      "Iteration 376, Average Reward: -107.03\n",
      "Iteration 377, Average Reward: -91.99\n",
      "Iteration 378, Average Reward: -113.19\n",
      "Iteration 379, Average Reward: -125.35\n",
      "Iteration 380, Average Reward: -115.56\n",
      "Iteration 381, Average Reward: -129.04\n",
      "Iteration 382, Average Reward: -129.87\n",
      "Iteration 383, Average Reward: -132.45\n",
      "Iteration 384, Average Reward: -127.64\n",
      "Iteration 385, Average Reward: -124.18\n",
      "Iteration 386, Average Reward: -119.04\n",
      "Iteration 387, Average Reward: -104.29\n",
      "Iteration 388, Average Reward: -112.11\n",
      "Iteration 389, Average Reward: -106.95\n",
      "Iteration 390, Average Reward: -110.43\n",
      "Iteration 391, Average Reward: -126.14\n",
      "Iteration 392, Average Reward: -107.41\n",
      "Iteration 393, Average Reward: -127.42\n",
      "Iteration 394, Average Reward: -143.32\n",
      "Iteration 395, Average Reward: -140.86\n",
      "Iteration 396, Average Reward: -135.5\n",
      "Iteration 397, Average Reward: -132.92\n",
      "Iteration 398, Average Reward: -110.44\n",
      "Iteration 399, Average Reward: -109.84\n",
      "Iteration 400, Average Reward: -114.31\n",
      "Iteration 401, Average Reward: -121.58\n",
      "Iteration 402, Average Reward: -106.69\n",
      "Iteration 403, Average Reward: -117.05\n",
      "Iteration 404, Average Reward: -125.49\n",
      "Iteration 405, Average Reward: -128.26\n",
      "Iteration 406, Average Reward: -144.72\n",
      "Iteration 407, Average Reward: -151.66\n",
      "Iteration 408, Average Reward: -132.72\n",
      "Iteration 409, Average Reward: -125.3\n",
      "Iteration 410, Average Reward: -133.22\n",
      "Iteration 411, Average Reward: -121.87\n",
      "Iteration 412, Average Reward: -108.78\n",
      "Iteration 413, Average Reward: -122.02\n",
      "Iteration 414, Average Reward: -133.35\n",
      "Iteration 415, Average Reward: -129.28\n",
      "Iteration 416, Average Reward: -129.86\n",
      "Iteration 417, Average Reward: -133.7\n",
      "Iteration 418, Average Reward: -118.12\n",
      "Iteration 419, Average Reward: -96.43\n",
      "Iteration 420, Average Reward: -112.81\n",
      "Iteration 421, Average Reward: -129.1\n",
      "Iteration 422, Average Reward: -117.31\n",
      "Iteration 423, Average Reward: -128.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 424, Average Reward: -136.81\n",
      "Iteration 425, Average Reward: -129.54\n",
      "Iteration 426, Average Reward: -99.37\n",
      "Iteration 427, Average Reward: -119.93\n",
      "Iteration 428, Average Reward: -120.89\n",
      "Iteration 429, Average Reward: -124.65\n",
      "Iteration 430, Average Reward: -139.43\n",
      "Iteration 431, Average Reward: -137.23\n",
      "Iteration 432, Average Reward: -145.49\n",
      "Iteration 433, Average Reward: -144.44\n",
      "Iteration 434, Average Reward: -121.1\n",
      "Iteration 435, Average Reward: -131.03\n",
      "Iteration 436, Average Reward: -131.17\n",
      "Iteration 437, Average Reward: -126.83\n",
      "Iteration 438, Average Reward: -124.9\n",
      "Iteration 439, Average Reward: -121.66\n",
      "Iteration 440, Average Reward: -112.21\n",
      "Iteration 441, Average Reward: -113.58\n",
      "Iteration 442, Average Reward: -125.52\n",
      "Iteration 443, Average Reward: -130.09\n",
      "Iteration 444, Average Reward: -139.74\n",
      "Iteration 445, Average Reward: -132.1\n",
      "Iteration 446, Average Reward: -128.92\n",
      "Iteration 447, Average Reward: -140.99\n",
      "Iteration 448, Average Reward: -125.52\n",
      "Iteration 449, Average Reward: -126.95\n",
      "Iteration 450, Average Reward: -114.6\n",
      "Iteration 451, Average Reward: -110.32\n",
      "Iteration 452, Average Reward: -112.11\n",
      "Iteration 453, Average Reward: -97.8\n",
      "Iteration 454, Average Reward: -119.6\n",
      "Iteration 455, Average Reward: -128.22\n",
      "Iteration 456, Average Reward: -135.1\n",
      "Iteration 457, Average Reward: -121.36\n",
      "Iteration 458, Average Reward: -112.34\n",
      "Iteration 459, Average Reward: -106.25\n",
      "Iteration 460, Average Reward: -126.87\n",
      "Iteration 461, Average Reward: -125.02\n",
      "Iteration 462, Average Reward: -124.84\n",
      "Iteration 463, Average Reward: -141.48\n",
      "Iteration 464, Average Reward: -138.58\n",
      "Iteration 465, Average Reward: -114.19\n",
      "Iteration 466, Average Reward: -113.55\n",
      "Iteration 467, Average Reward: -127.14\n",
      "Iteration 468, Average Reward: -122.17\n",
      "Iteration 469, Average Reward: -129.86\n",
      "Iteration 470, Average Reward: -135.43\n",
      "Iteration 471, Average Reward: -124.98\n",
      "Iteration 472, Average Reward: -135.41\n",
      "Iteration 473, Average Reward: -140.39\n",
      "Iteration 474, Average Reward: -134.4\n",
      "Iteration 475, Average Reward: -130.61\n",
      "Iteration 476, Average Reward: -135.75\n",
      "Iteration 477, Average Reward: -133.21\n",
      "Iteration 478, Average Reward: -139.96\n",
      "Iteration 479, Average Reward: -129.14\n",
      "Iteration 480, Average Reward: -129.83\n",
      "Iteration 481, Average Reward: -130.03\n",
      "Iteration 482, Average Reward: -124.97\n",
      "Iteration 483, Average Reward: -130.94\n",
      "Iteration 484, Average Reward: -109.33\n",
      "Iteration 485, Average Reward: -120.98\n",
      "Iteration 486, Average Reward: -136.35\n",
      "Iteration 487, Average Reward: -128.25\n",
      "Iteration 488, Average Reward: -110.21\n",
      "Iteration 489, Average Reward: -120.24\n",
      "Iteration 490, Average Reward: -133.27\n",
      "Iteration 491, Average Reward: -130.52\n",
      "Iteration 492, Average Reward: -129.73\n",
      "Iteration 493, Average Reward: -128.63\n",
      "Iteration 494, Average Reward: -119.09\n",
      "Iteration 495, Average Reward: -120.8\n",
      "Iteration 496, Average Reward: -127.9\n",
      "Iteration 497, Average Reward: -127.45\n",
      "Iteration 498, Average Reward: -132.99\n",
      "Iteration 499, Average Reward: -122.93\n",
      "Iteration 500, Average Reward: -132.79\n",
      "Iteration 501, Average Reward: -122.04\n",
      "Iteration 502, Average Reward: -135.93\n",
      "Iteration 503, Average Reward: -132.1\n",
      "Iteration 504, Average Reward: -121.2\n",
      "Iteration 505, Average Reward: -118.36\n",
      "Iteration 506, Average Reward: -128.02\n",
      "Iteration 507, Average Reward: -120.42\n",
      "Iteration 508, Average Reward: -111.45\n",
      "Iteration 509, Average Reward: -113.23\n",
      "Iteration 510, Average Reward: -112.24\n",
      "Iteration 511, Average Reward: -108.86\n",
      "Iteration 512, Average Reward: -125.13\n",
      "Iteration 513, Average Reward: -116.88\n",
      "Iteration 514, Average Reward: -132.29\n",
      "Iteration 515, Average Reward: -135.76\n",
      "Iteration 516, Average Reward: -138.97\n",
      "Iteration 517, Average Reward: -130.62\n",
      "Iteration 518, Average Reward: -129.1\n",
      "Iteration 519, Average Reward: -130.19\n",
      "Iteration 520, Average Reward: -123.86\n",
      "Iteration 521, Average Reward: -121.54\n",
      "Iteration 522, Average Reward: -124.43\n",
      "Iteration 523, Average Reward: -135.4\n",
      "Iteration 524, Average Reward: -133.36\n",
      "Iteration 525, Average Reward: -147.61\n",
      "Iteration 526, Average Reward: -123.1\n",
      "Iteration 527, Average Reward: -120.67\n",
      "Iteration 528, Average Reward: -127.27\n",
      "Iteration 529, Average Reward: -108.06\n",
      "Iteration 530, Average Reward: -113.95\n",
      "Iteration 531, Average Reward: -114.94\n",
      "Iteration 532, Average Reward: -115.99\n",
      "Iteration 533, Average Reward: -121.53\n",
      "Iteration 534, Average Reward: -126.13\n",
      "Iteration 535, Average Reward: -119.24\n",
      "Iteration 536, Average Reward: -129.03\n",
      "Iteration 537, Average Reward: -126.38\n",
      "Iteration 538, Average Reward: -123.9\n",
      "Iteration 539, Average Reward: -120.28\n",
      "Iteration 540, Average Reward: -122.36\n",
      "Iteration 541, Average Reward: -117.64\n",
      "Iteration 542, Average Reward: -108.83\n",
      "Iteration 543, Average Reward: -103.93\n",
      "Iteration 544, Average Reward: -121.12\n",
      "Iteration 545, Average Reward: -117.77\n",
      "Iteration 546, Average Reward: -122.24\n",
      "Iteration 547, Average Reward: -112.84\n",
      "Iteration 548, Average Reward: -129.15\n",
      "Iteration 549, Average Reward: -132.04\n",
      "Iteration 550, Average Reward: -141.59\n",
      "Iteration 551, Average Reward: -130.86\n",
      "Iteration 552, Average Reward: -128.12\n",
      "Iteration 553, Average Reward: -128.65\n",
      "Iteration 554, Average Reward: -111.46\n",
      "Iteration 555, Average Reward: -112.47\n",
      "Iteration 556, Average Reward: -113.25\n",
      "Iteration 557, Average Reward: -117.77\n",
      "Iteration 558, Average Reward: -133.95\n",
      "Iteration 559, Average Reward: -133.64\n",
      "Iteration 560, Average Reward: -131.92\n",
      "Iteration 561, Average Reward: -120.56\n",
      "Iteration 562, Average Reward: -132.26\n",
      "Iteration 563, Average Reward: -142.03\n",
      "Iteration 564, Average Reward: -121.64\n",
      "Iteration 565, Average Reward: -115.1\n",
      "Iteration 566, Average Reward: -136.78\n",
      "Iteration 567, Average Reward: -125.81\n",
      "Iteration 568, Average Reward: -123.15\n",
      "Iteration 569, Average Reward: -121.45\n",
      "Iteration 570, Average Reward: -125.12\n",
      "Iteration 571, Average Reward: -115.35\n",
      "Iteration 572, Average Reward: -125.3\n",
      "Iteration 573, Average Reward: -123.41\n",
      "Iteration 574, Average Reward: -143.67\n",
      "Iteration 575, Average Reward: -137.26\n",
      "Iteration 576, Average Reward: -137.08\n",
      "Iteration 577, Average Reward: -136.61\n",
      "Iteration 578, Average Reward: -138.62\n",
      "Iteration 579, Average Reward: -120.18\n",
      "Iteration 580, Average Reward: -102.95\n",
      "Iteration 581, Average Reward: -121.72\n",
      "Iteration 582, Average Reward: -120.0\n",
      "Iteration 583, Average Reward: -115.21\n",
      "Iteration 584, Average Reward: -116.14\n",
      "Iteration 585, Average Reward: -131.42\n",
      "Iteration 586, Average Reward: -127.69\n",
      "Iteration 587, Average Reward: -113.18\n",
      "Iteration 588, Average Reward: -115.7\n",
      "Iteration 589, Average Reward: -127.19\n",
      "Iteration 590, Average Reward: -126.07\n",
      "Iteration 591, Average Reward: -108.0\n",
      "Iteration 592, Average Reward: -115.31\n",
      "Iteration 593, Average Reward: -122.23\n",
      "Iteration 594, Average Reward: -132.45\n",
      "Iteration 595, Average Reward: -135.17\n",
      "Iteration 596, Average Reward: -132.15\n",
      "Iteration 597, Average Reward: -120.44\n",
      "Iteration 598, Average Reward: -110.46\n",
      "Iteration 599, Average Reward: -105.68\n",
      "Iteration 600, Average Reward: -108.07\n",
      "Iteration 601, Average Reward: -111.34\n",
      "Iteration 602, Average Reward: -119.34\n",
      "Iteration 603, Average Reward: -109.38\n",
      "Iteration 604, Average Reward: -129.67\n",
      "Iteration 605, Average Reward: -127.29\n",
      "Iteration 606, Average Reward: -135.58\n",
      "Iteration 607, Average Reward: -126.55\n",
      "Iteration 608, Average Reward: -134.55\n",
      "Iteration 609, Average Reward: -127.2\n",
      "Iteration 610, Average Reward: -108.24\n",
      "Iteration 611, Average Reward: -103.54\n",
      "Iteration 612, Average Reward: -114.01\n",
      "Iteration 613, Average Reward: -111.96\n",
      "Iteration 614, Average Reward: -112.93\n",
      "Iteration 615, Average Reward: -120.15\n",
      "Iteration 616, Average Reward: -116.6\n",
      "Iteration 617, Average Reward: -115.5\n",
      "Iteration 618, Average Reward: -121.51\n",
      "Iteration 619, Average Reward: -122.49\n",
      "Iteration 620, Average Reward: -122.32\n",
      "Iteration 621, Average Reward: -134.52\n",
      "Iteration 622, Average Reward: -115.53\n",
      "Iteration 623, Average Reward: -115.97\n",
      "Iteration 624, Average Reward: -118.9\n",
      "Iteration 625, Average Reward: -122.02\n",
      "Iteration 626, Average Reward: -103.66\n",
      "Iteration 627, Average Reward: -91.77\n",
      "Iteration 628, Average Reward: -110.71\n",
      "Iteration 629, Average Reward: -127.82\n",
      "Iteration 630, Average Reward: -124.14\n",
      "Iteration 631, Average Reward: -116.57\n",
      "Iteration 632, Average Reward: -124.11\n",
      "Iteration 633, Average Reward: -122.82\n",
      "Iteration 634, Average Reward: -114.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 635, Average Reward: -117.43\n",
      "Iteration 636, Average Reward: -126.56\n",
      "Iteration 637, Average Reward: -120.25\n",
      "Iteration 638, Average Reward: -103.58\n",
      "Iteration 639, Average Reward: -113.25\n",
      "Iteration 640, Average Reward: -125.98\n",
      "Iteration 641, Average Reward: -128.42\n",
      "Iteration 642, Average Reward: -129.7\n",
      "Iteration 643, Average Reward: -144.54\n",
      "Iteration 644, Average Reward: -145.03\n",
      "Iteration 645, Average Reward: -114.76\n",
      "Iteration 646, Average Reward: -111.81\n",
      "Iteration 647, Average Reward: -127.49\n",
      "Iteration 648, Average Reward: -103.45\n",
      "Iteration 649, Average Reward: -104.47\n",
      "Iteration 650, Average Reward: -125.25\n",
      "Iteration 651, Average Reward: -127.43\n",
      "Iteration 652, Average Reward: -129.09\n",
      "Iteration 653, Average Reward: -135.11\n",
      "Iteration 654, Average Reward: -139.52\n",
      "Iteration 655, Average Reward: -125.65\n",
      "Iteration 656, Average Reward: -120.93\n",
      "Iteration 657, Average Reward: -114.14\n",
      "Iteration 658, Average Reward: -119.67\n",
      "Iteration 659, Average Reward: -115.54\n",
      "Iteration 660, Average Reward: -128.56\n",
      "Iteration 661, Average Reward: -129.49\n",
      "Iteration 662, Average Reward: -128.96\n",
      "Iteration 663, Average Reward: -133.46\n",
      "Iteration 664, Average Reward: -122.82\n",
      "Iteration 665, Average Reward: -120.89\n",
      "Iteration 666, Average Reward: -114.19\n",
      "Iteration 667, Average Reward: -116.26\n",
      "Iteration 668, Average Reward: -129.3\n",
      "Iteration 669, Average Reward: -119.73\n",
      "Iteration 670, Average Reward: -129.2\n",
      "Iteration 671, Average Reward: -123.35\n",
      "Iteration 672, Average Reward: -104.81\n",
      "Iteration 673, Average Reward: -120.76\n",
      "Iteration 674, Average Reward: -156.08\n",
      "Iteration 675, Average Reward: -160.57\n",
      "Iteration 676, Average Reward: -153.75\n",
      "Iteration 677, Average Reward: -123.12\n",
      "Iteration 678, Average Reward: -130.71\n",
      "Iteration 679, Average Reward: -120.76\n",
      "Iteration 680, Average Reward: -107.4\n",
      "Iteration 681, Average Reward: -102.05\n",
      "Iteration 682, Average Reward: -132.13\n",
      "Iteration 683, Average Reward: -125.57\n",
      "Iteration 684, Average Reward: -126.77\n",
      "Iteration 685, Average Reward: -132.5\n",
      "Iteration 686, Average Reward: -140.3\n",
      "Iteration 687, Average Reward: -130.48\n",
      "Iteration 688, Average Reward: -117.59\n",
      "Iteration 689, Average Reward: -134.37\n",
      "Iteration 690, Average Reward: -139.84\n",
      "Iteration 691, Average Reward: -132.56\n",
      "Iteration 692, Average Reward: -123.25\n",
      "Iteration 693, Average Reward: -108.67\n",
      "Iteration 694, Average Reward: -115.01\n",
      "Iteration 695, Average Reward: -108.62\n",
      "Iteration 696, Average Reward: -113.45\n",
      "Iteration 697, Average Reward: -116.51\n",
      "Iteration 698, Average Reward: -141.45\n",
      "Iteration 699, Average Reward: -155.05\n",
      "Iteration 700, Average Reward: -146.09\n",
      "Iteration 701, Average Reward: -137.75\n",
      "Iteration 702, Average Reward: -136.93\n",
      "Iteration 703, Average Reward: -140.57\n",
      "Iteration 704, Average Reward: -122.37\n",
      "Iteration 705, Average Reward: -111.77\n",
      "Iteration 706, Average Reward: -117.67\n",
      "Iteration 707, Average Reward: -122.03\n",
      "Iteration 708, Average Reward: -130.6\n",
      "Iteration 709, Average Reward: -124.3\n",
      "Iteration 710, Average Reward: -114.89\n",
      "Iteration 711, Average Reward: -113.97\n",
      "Iteration 712, Average Reward: -116.99\n",
      "Iteration 713, Average Reward: -123.09\n",
      "Iteration 714, Average Reward: -126.18\n",
      "Iteration 715, Average Reward: -130.15\n",
      "Iteration 716, Average Reward: -130.97\n",
      "Iteration 717, Average Reward: -131.07\n",
      "Iteration 718, Average Reward: -121.11\n",
      "Iteration 719, Average Reward: -116.32\n",
      "Iteration 720, Average Reward: -118.43\n",
      "Iteration 721, Average Reward: -119.39\n",
      "Iteration 722, Average Reward: -133.19\n",
      "Iteration 723, Average Reward: -146.52\n",
      "Iteration 724, Average Reward: -131.24\n",
      "Iteration 725, Average Reward: -124.43\n",
      "Iteration 726, Average Reward: -115.92\n",
      "Iteration 727, Average Reward: -111.21\n",
      "Iteration 728, Average Reward: -113.3\n",
      "Iteration 729, Average Reward: -124.04\n",
      "Iteration 730, Average Reward: -142.39\n",
      "Iteration 731, Average Reward: -150.27\n",
      "Iteration 732, Average Reward: -141.57\n",
      "Iteration 733, Average Reward: -125.61\n",
      "Iteration 734, Average Reward: -119.39\n",
      "Iteration 735, Average Reward: -106.24\n",
      "Iteration 736, Average Reward: -117.29\n",
      "Iteration 737, Average Reward: -128.26\n",
      "Iteration 738, Average Reward: -113.35\n",
      "Iteration 739, Average Reward: -125.29\n",
      "Iteration 740, Average Reward: -123.45\n",
      "Iteration 741, Average Reward: -127.86\n",
      "Iteration 742, Average Reward: -144.11\n",
      "Iteration 743, Average Reward: -141.41\n",
      "Iteration 744, Average Reward: -126.1\n",
      "Iteration 745, Average Reward: -125.9\n",
      "Iteration 746, Average Reward: -111.55\n",
      "Iteration 747, Average Reward: -119.99\n",
      "Iteration 748, Average Reward: -127.85\n",
      "Iteration 749, Average Reward: -152.88\n",
      "Iteration 750, Average Reward: -149.21\n",
      "Iteration 751, Average Reward: -138.08\n",
      "Iteration 752, Average Reward: -121.47\n",
      "Iteration 753, Average Reward: -114.42\n",
      "Iteration 754, Average Reward: -100.72\n",
      "Iteration 755, Average Reward: -104.81\n",
      "Iteration 756, Average Reward: -116.03\n",
      "Iteration 757, Average Reward: -114.73\n",
      "Iteration 758, Average Reward: -127.71\n",
      "Iteration 759, Average Reward: -128.69\n",
      "Iteration 760, Average Reward: -135.73\n",
      "Iteration 761, Average Reward: -146.01\n",
      "Iteration 762, Average Reward: -119.55\n",
      "Iteration 763, Average Reward: -110.89\n",
      "Iteration 764, Average Reward: -102.06\n",
      "Iteration 765, Average Reward: -110.91\n",
      "Iteration 766, Average Reward: -120.69\n",
      "Iteration 767, Average Reward: -103.85\n",
      "Iteration 768, Average Reward: -98.78\n",
      "Iteration 769, Average Reward: -109.55\n",
      "Iteration 770, Average Reward: -114.01\n",
      "Iteration 771, Average Reward: -128.08\n",
      "Iteration 772, Average Reward: -123.86\n",
      "Iteration 773, Average Reward: -125.63\n",
      "Iteration 774, Average Reward: -127.86\n",
      "Iteration 775, Average Reward: -116.8\n",
      "Iteration 776, Average Reward: -106.46\n",
      "Iteration 777, Average Reward: -106.95\n",
      "Iteration 778, Average Reward: -121.69\n",
      "Iteration 779, Average Reward: -108.2\n",
      "Iteration 780, Average Reward: -119.44\n",
      "Iteration 781, Average Reward: -117.56\n",
      "Iteration 782, Average Reward: -135.11\n",
      "Iteration 783, Average Reward: -132.18\n",
      "Iteration 784, Average Reward: -115.48\n",
      "Iteration 785, Average Reward: -107.68\n",
      "Iteration 786, Average Reward: -108.61\n",
      "Iteration 787, Average Reward: -114.45\n",
      "Iteration 788, Average Reward: -101.4\n",
      "Iteration 789, Average Reward: -103.03\n",
      "Iteration 790, Average Reward: -132.59\n",
      "Iteration 791, Average Reward: -145.07\n",
      "Iteration 792, Average Reward: -131.79\n",
      "Iteration 793, Average Reward: -120.95\n",
      "Iteration 794, Average Reward: -126.52\n",
      "Iteration 795, Average Reward: -126.17\n",
      "Iteration 796, Average Reward: -125.83\n",
      "Iteration 797, Average Reward: -117.55\n",
      "Iteration 798, Average Reward: -121.53\n",
      "Iteration 799, Average Reward: -119.95\n",
      "Iteration 800, Average Reward: -101.69\n",
      "Iteration 801, Average Reward: -112.63\n",
      "Iteration 802, Average Reward: -124.9\n",
      "Iteration 803, Average Reward: -115.93\n",
      "Iteration 804, Average Reward: -108.06\n",
      "Iteration 805, Average Reward: -109.38\n",
      "Iteration 806, Average Reward: -111.99\n",
      "Iteration 807, Average Reward: -116.94\n",
      "Iteration 808, Average Reward: -106.06\n",
      "Iteration 809, Average Reward: -110.79\n",
      "Iteration 810, Average Reward: -116.7\n",
      "Iteration 811, Average Reward: -110.28\n",
      "Iteration 812, Average Reward: -103.91\n",
      "Iteration 813, Average Reward: -140.17\n",
      "Iteration 814, Average Reward: -134.84\n",
      "Iteration 815, Average Reward: -109.69\n",
      "Iteration 816, Average Reward: -113.71\n",
      "Iteration 817, Average Reward: -118.86\n",
      "Iteration 818, Average Reward: -118.29\n",
      "Iteration 819, Average Reward: -125.59\n",
      "Iteration 820, Average Reward: -133.14\n",
      "Iteration 821, Average Reward: -123.74\n",
      "Iteration 822, Average Reward: -131.64\n",
      "Iteration 823, Average Reward: -121.86\n",
      "Iteration 824, Average Reward: -127.17\n",
      "Iteration 825, Average Reward: -114.69\n",
      "Iteration 826, Average Reward: -102.6\n",
      "Iteration 827, Average Reward: -105.77\n",
      "Iteration 828, Average Reward: -107.08\n",
      "Iteration 829, Average Reward: -116.12\n",
      "Iteration 830, Average Reward: -111.73\n",
      "Iteration 831, Average Reward: -112.52\n",
      "Iteration 832, Average Reward: -130.63\n",
      "Iteration 833, Average Reward: -122.53\n",
      "Iteration 834, Average Reward: -125.73\n",
      "Iteration 835, Average Reward: -144.25\n",
      "Iteration 836, Average Reward: -125.17\n",
      "Iteration 837, Average Reward: -126.5\n",
      "Iteration 838, Average Reward: -123.15\n",
      "Iteration 839, Average Reward: -121.95\n",
      "Iteration 840, Average Reward: -120.34\n",
      "Iteration 841, Average Reward: -126.85\n",
      "Iteration 842, Average Reward: -124.96\n",
      "Iteration 843, Average Reward: -133.41\n",
      "Iteration 844, Average Reward: -132.87\n",
      "Iteration 845, Average Reward: -133.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 846, Average Reward: -138.66\n",
      "Iteration 847, Average Reward: -124.77\n",
      "Iteration 848, Average Reward: -140.29\n",
      "Iteration 849, Average Reward: -128.95\n",
      "Iteration 850, Average Reward: -126.11\n",
      "Iteration 851, Average Reward: -116.17\n",
      "Iteration 852, Average Reward: -120.75\n",
      "Iteration 853, Average Reward: -118.0\n",
      "Iteration 854, Average Reward: -125.35\n",
      "Iteration 855, Average Reward: -140.06\n",
      "Iteration 856, Average Reward: -139.68\n",
      "Iteration 857, Average Reward: -118.28\n",
      "Iteration 858, Average Reward: -126.13\n",
      "Iteration 859, Average Reward: -130.97\n",
      "Iteration 860, Average Reward: -123.78\n",
      "Iteration 861, Average Reward: -117.55\n",
      "Iteration 862, Average Reward: -139.1\n",
      "Iteration 863, Average Reward: -129.04\n",
      "Iteration 864, Average Reward: -137.72\n",
      "Iteration 865, Average Reward: -130.9\n",
      "Iteration 866, Average Reward: -136.62\n",
      "Iteration 867, Average Reward: -115.42\n",
      "Iteration 868, Average Reward: -104.46\n",
      "Iteration 869, Average Reward: -119.81\n",
      "Iteration 870, Average Reward: -133.81\n",
      "Iteration 871, Average Reward: -126.87\n",
      "Iteration 872, Average Reward: -126.19\n",
      "Iteration 873, Average Reward: -133.98\n",
      "Iteration 874, Average Reward: -140.44\n",
      "Iteration 875, Average Reward: -146.57\n",
      "Iteration 876, Average Reward: -130.76\n",
      "Iteration 877, Average Reward: -131.82\n",
      "Iteration 878, Average Reward: -142.5\n",
      "Iteration 879, Average Reward: -135.0\n",
      "Iteration 880, Average Reward: -118.92\n",
      "Iteration 881, Average Reward: -112.11\n",
      "Iteration 882, Average Reward: -118.94\n",
      "Iteration 883, Average Reward: -110.35\n",
      "Iteration 884, Average Reward: -129.11\n",
      "Iteration 885, Average Reward: -137.43\n",
      "Iteration 886, Average Reward: -134.38\n",
      "Iteration 887, Average Reward: -135.33\n",
      "Iteration 888, Average Reward: -140.55\n",
      "Iteration 889, Average Reward: -144.42\n",
      "Iteration 890, Average Reward: -131.9\n",
      "Iteration 891, Average Reward: -126.29\n",
      "Iteration 892, Average Reward: -115.11\n",
      "Iteration 893, Average Reward: -112.91\n",
      "Iteration 894, Average Reward: -103.36\n",
      "Iteration 895, Average Reward: -124.41\n",
      "Iteration 896, Average Reward: -117.92\n",
      "Iteration 897, Average Reward: -105.93\n",
      "Iteration 898, Average Reward: -122.64\n",
      "Iteration 899, Average Reward: -124.37\n",
      "Iteration 900, Average Reward: -126.26\n",
      "Iteration 901, Average Reward: -126.55\n",
      "Iteration 902, Average Reward: -117.63\n",
      "Iteration 903, Average Reward: -119.46\n",
      "Iteration 904, Average Reward: -122.78\n",
      "Iteration 905, Average Reward: -117.73\n",
      "Iteration 906, Average Reward: -115.32\n",
      "Iteration 907, Average Reward: -122.2\n",
      "Iteration 908, Average Reward: -123.6\n",
      "Iteration 909, Average Reward: -112.98\n",
      "Iteration 910, Average Reward: -110.85\n",
      "Iteration 911, Average Reward: -108.03\n",
      "Iteration 912, Average Reward: -121.09\n",
      "Iteration 913, Average Reward: -124.05\n",
      "Iteration 914, Average Reward: -125.37\n",
      "Iteration 915, Average Reward: -130.13\n",
      "Iteration 916, Average Reward: -121.96\n",
      "Iteration 917, Average Reward: -118.15\n",
      "Iteration 918, Average Reward: -124.34\n",
      "Iteration 919, Average Reward: -127.54\n",
      "Iteration 920, Average Reward: -119.57\n",
      "Iteration 921, Average Reward: -127.28\n",
      "Iteration 922, Average Reward: -129.45\n",
      "Iteration 923, Average Reward: -139.36\n",
      "Iteration 924, Average Reward: -153.3\n",
      "Iteration 925, Average Reward: -140.04\n",
      "Iteration 926, Average Reward: -128.17\n",
      "Iteration 927, Average Reward: -119.58\n",
      "Iteration 928, Average Reward: -117.57\n",
      "Iteration 929, Average Reward: -117.44\n",
      "Iteration 930, Average Reward: -115.9\n",
      "Iteration 931, Average Reward: -120.51\n",
      "Iteration 932, Average Reward: -127.05\n",
      "Iteration 933, Average Reward: -113.15\n",
      "Iteration 934, Average Reward: -114.0\n",
      "Iteration 935, Average Reward: -129.51\n",
      "Iteration 936, Average Reward: -123.22\n",
      "Iteration 937, Average Reward: -117.9\n",
      "Iteration 938, Average Reward: -128.41\n",
      "Iteration 939, Average Reward: -122.48\n",
      "Iteration 940, Average Reward: -121.58\n",
      "Iteration 941, Average Reward: -137.46\n",
      "Iteration 942, Average Reward: -122.0\n",
      "Iteration 943, Average Reward: -114.81\n",
      "Iteration 944, Average Reward: -119.69\n",
      "Iteration 945, Average Reward: -131.91\n",
      "Iteration 946, Average Reward: -134.88\n",
      "Iteration 947, Average Reward: -128.53\n",
      "Iteration 948, Average Reward: -134.82\n",
      "Iteration 949, Average Reward: -145.51\n",
      "Iteration 950, Average Reward: -137.32\n",
      "Iteration 951, Average Reward: -123.42\n",
      "Iteration 952, Average Reward: -139.53\n",
      "Iteration 953, Average Reward: -132.04\n",
      "Iteration 954, Average Reward: -124.41\n",
      "Iteration 955, Average Reward: -128.76\n",
      "Iteration 956, Average Reward: -115.54\n",
      "Iteration 957, Average Reward: -121.27\n",
      "Iteration 958, Average Reward: -135.99\n",
      "Iteration 959, Average Reward: -145.74\n",
      "Iteration 960, Average Reward: -134.23\n",
      "Iteration 961, Average Reward: -131.04\n",
      "Iteration 962, Average Reward: -132.05\n",
      "Iteration 963, Average Reward: -129.23\n",
      "Iteration 964, Average Reward: -119.29\n",
      "Iteration 965, Average Reward: -124.47\n",
      "Iteration 966, Average Reward: -133.47\n",
      "Iteration 967, Average Reward: -125.36\n",
      "Iteration 968, Average Reward: -114.3\n",
      "Iteration 969, Average Reward: -131.0\n",
      "Iteration 970, Average Reward: -136.71\n",
      "Iteration 971, Average Reward: -132.21\n",
      "Iteration 972, Average Reward: -139.36\n",
      "Iteration 973, Average Reward: -121.51\n",
      "Iteration 974, Average Reward: -130.37\n",
      "Iteration 975, Average Reward: -133.68\n",
      "Iteration 976, Average Reward: -130.17\n",
      "Iteration 977, Average Reward: -131.03\n",
      "Iteration 978, Average Reward: -117.73\n",
      "Iteration 979, Average Reward: -103.38\n",
      "Iteration 980, Average Reward: -94.02\n",
      "Iteration 981, Average Reward: -98.25\n",
      "Iteration 982, Average Reward: -100.03\n",
      "Iteration 983, Average Reward: -104.17\n",
      "Iteration 984, Average Reward: -107.2\n",
      "Iteration 985, Average Reward: -125.23\n",
      "Iteration 986, Average Reward: -134.15\n",
      "Iteration 987, Average Reward: -134.62\n",
      "Iteration 988, Average Reward: -131.3\n",
      "Iteration 989, Average Reward: -122.69\n",
      "Iteration 990, Average Reward: -103.22\n",
      "Iteration 991, Average Reward: -107.0\n",
      "Iteration 992, Average Reward: -109.02\n",
      "Iteration 993, Average Reward: -114.61\n",
      "Iteration 994, Average Reward: -135.39\n",
      "Iteration 995, Average Reward: -133.33\n",
      "Iteration 996, Average Reward: -121.82\n",
      "Iteration 997, Average Reward: -124.5\n",
      "Iteration 998, Average Reward: -116.82\n",
      "Iteration 999, Average Reward: -126.14\n",
      "Iteration 1000, Average Reward: -125.46\n",
      "Iteration 1001, Average Reward: -121.01\n",
      "Iteration 1002, Average Reward: -110.39\n",
      "Iteration 1003, Average Reward: -120.65\n",
      "Iteration 1004, Average Reward: -119.79\n",
      "Iteration 1005, Average Reward: -136.46\n",
      "Iteration 1006, Average Reward: -129.15\n",
      "Iteration 1007, Average Reward: -126.89\n",
      "Iteration 1008, Average Reward: -123.84\n",
      "Iteration 1009, Average Reward: -123.55\n",
      "Iteration 1010, Average Reward: -119.41\n",
      "Iteration 1011, Average Reward: -121.65\n",
      "Iteration 1012, Average Reward: -112.32\n",
      "Iteration 1013, Average Reward: -113.49\n",
      "Iteration 1014, Average Reward: -104.8\n",
      "Iteration 1015, Average Reward: -120.34\n",
      "Iteration 1016, Average Reward: -129.29\n",
      "Iteration 1017, Average Reward: -131.5\n",
      "Iteration 1018, Average Reward: -120.75\n",
      "Iteration 1019, Average Reward: -125.84\n",
      "Iteration 1020, Average Reward: -128.38\n",
      "Iteration 1021, Average Reward: -114.27\n",
      "Iteration 1022, Average Reward: -111.14\n",
      "Iteration 1023, Average Reward: -111.0\n",
      "Iteration 1024, Average Reward: -95.99\n",
      "Iteration 1025, Average Reward: -107.5\n",
      "Iteration 1026, Average Reward: -133.11\n",
      "Iteration 1027, Average Reward: -115.23\n",
      "Iteration 1028, Average Reward: -111.55\n",
      "Iteration 1029, Average Reward: -120.46\n",
      "Iteration 1030, Average Reward: -130.09\n",
      "Iteration 1031, Average Reward: -133.57\n",
      "Iteration 1032, Average Reward: -123.76\n",
      "Iteration 1033, Average Reward: -139.69\n",
      "Iteration 1034, Average Reward: -136.18\n",
      "Iteration 1035, Average Reward: -121.32\n",
      "Iteration 1036, Average Reward: -119.1\n",
      "Iteration 1037, Average Reward: -131.2\n",
      "Iteration 1038, Average Reward: -128.05\n",
      "Iteration 1039, Average Reward: -113.34\n",
      "Iteration 1040, Average Reward: -120.24\n",
      "Iteration 1041, Average Reward: -128.83\n",
      "Iteration 1042, Average Reward: -130.86\n",
      "Iteration 1043, Average Reward: -124.4\n",
      "Iteration 1044, Average Reward: -136.82\n",
      "Iteration 1045, Average Reward: -131.28\n",
      "Iteration 1046, Average Reward: -105.6\n",
      "Iteration 1047, Average Reward: -111.52\n",
      "Iteration 1048, Average Reward: -121.35\n",
      "Iteration 1049, Average Reward: -115.81\n",
      "Iteration 1050, Average Reward: -120.78\n",
      "Iteration 1051, Average Reward: -141.33\n",
      "Iteration 1052, Average Reward: -131.76\n",
      "Iteration 1053, Average Reward: -144.92\n",
      "Iteration 1054, Average Reward: -117.77\n",
      "Iteration 1055, Average Reward: -113.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1056, Average Reward: -123.09\n",
      "Iteration 1057, Average Reward: -136.47\n",
      "Iteration 1058, Average Reward: -136.07\n",
      "Iteration 1059, Average Reward: -129.28\n",
      "Iteration 1060, Average Reward: -126.54\n",
      "Iteration 1061, Average Reward: -140.75\n",
      "Iteration 1062, Average Reward: -113.17\n",
      "Iteration 1063, Average Reward: -107.68\n",
      "Iteration 1064, Average Reward: -112.08\n",
      "Iteration 1065, Average Reward: -109.25\n",
      "Iteration 1066, Average Reward: -104.93\n",
      "Iteration 1067, Average Reward: -126.93\n",
      "Iteration 1068, Average Reward: -135.68\n",
      "Iteration 1069, Average Reward: -146.34\n",
      "Iteration 1070, Average Reward: -145.01\n",
      "Iteration 1071, Average Reward: -131.39\n",
      "Iteration 1072, Average Reward: -114.83\n",
      "Iteration 1073, Average Reward: -110.76\n",
      "Iteration 1074, Average Reward: -97.35\n",
      "Iteration 1075, Average Reward: -100.99\n",
      "Iteration 1076, Average Reward: -103.84\n",
      "Iteration 1077, Average Reward: -115.29\n",
      "Iteration 1078, Average Reward: -126.34\n",
      "Iteration 1079, Average Reward: -135.29\n",
      "Iteration 1080, Average Reward: -142.5\n",
      "Iteration 1081, Average Reward: -132.59\n",
      "Iteration 1082, Average Reward: -118.81\n",
      "Iteration 1083, Average Reward: -110.88\n",
      "Iteration 1084, Average Reward: -120.09\n",
      "Iteration 1085, Average Reward: -127.71\n",
      "Iteration 1086, Average Reward: -127.67\n",
      "Iteration 1087, Average Reward: -129.27\n",
      "Iteration 1088, Average Reward: -110.0\n",
      "Iteration 1089, Average Reward: -120.89\n",
      "Iteration 1090, Average Reward: -118.82\n",
      "Iteration 1091, Average Reward: -138.39\n",
      "Iteration 1092, Average Reward: -129.05\n",
      "Iteration 1093, Average Reward: -120.72\n",
      "Iteration 1094, Average Reward: -140.45\n",
      "Iteration 1095, Average Reward: -131.76\n",
      "Iteration 1096, Average Reward: -116.88\n",
      "Iteration 1097, Average Reward: -130.4\n",
      "Iteration 1098, Average Reward: -141.52\n",
      "Iteration 1099, Average Reward: -130.38\n",
      "Iteration 1100, Average Reward: -124.36\n",
      "Iteration 1101, Average Reward: -142.68\n",
      "Iteration 1102, Average Reward: -137.07\n",
      "Iteration 1103, Average Reward: -127.53\n",
      "Iteration 1104, Average Reward: -122.65\n",
      "Iteration 1105, Average Reward: -125.42\n",
      "Iteration 1106, Average Reward: -115.36\n",
      "Iteration 1107, Average Reward: -125.44\n",
      "Iteration 1108, Average Reward: -114.23\n",
      "Iteration 1109, Average Reward: -121.51\n",
      "Iteration 1110, Average Reward: -117.19\n",
      "Iteration 1111, Average Reward: -116.32\n",
      "Iteration 1112, Average Reward: -115.5\n",
      "Iteration 1113, Average Reward: -119.75\n",
      "Iteration 1114, Average Reward: -127.43\n",
      "Iteration 1115, Average Reward: -122.54\n",
      "Iteration 1116, Average Reward: -108.12\n",
      "Iteration 1117, Average Reward: -122.52\n",
      "Iteration 1118, Average Reward: -125.03\n",
      "Iteration 1119, Average Reward: -101.89\n",
      "Iteration 1120, Average Reward: -105.82\n",
      "Iteration 1121, Average Reward: -117.86\n",
      "Iteration 1122, Average Reward: -129.14\n",
      "Iteration 1123, Average Reward: -124.53\n",
      "Iteration 1124, Average Reward: -132.6\n",
      "Iteration 1125, Average Reward: -134.34\n",
      "Iteration 1126, Average Reward: -126.19\n",
      "Iteration 1127, Average Reward: -120.25\n",
      "Iteration 1128, Average Reward: -105.68\n",
      "Iteration 1129, Average Reward: -103.8\n",
      "Iteration 1130, Average Reward: -99.11\n",
      "Iteration 1131, Average Reward: -120.47\n",
      "Iteration 1132, Average Reward: -135.27\n",
      "Iteration 1133, Average Reward: -139.98\n",
      "Iteration 1134, Average Reward: -149.44\n",
      "Iteration 1135, Average Reward: -145.05\n",
      "Iteration 1136, Average Reward: -147.3\n",
      "Iteration 1137, Average Reward: -122.07\n",
      "Iteration 1138, Average Reward: -118.87\n",
      "Iteration 1139, Average Reward: -109.39\n",
      "Iteration 1140, Average Reward: -126.9\n",
      "Iteration 1141, Average Reward: -112.32\n",
      "Iteration 1142, Average Reward: -132.35\n",
      "Iteration 1143, Average Reward: -127.67\n",
      "Iteration 1144, Average Reward: -136.95\n",
      "Iteration 1145, Average Reward: -121.25\n",
      "Iteration 1146, Average Reward: -120.7\n",
      "Iteration 1147, Average Reward: -129.05\n",
      "Iteration 1148, Average Reward: -132.61\n",
      "Iteration 1149, Average Reward: -131.32\n",
      "Iteration 1150, Average Reward: -136.69\n",
      "Iteration 1151, Average Reward: -121.94\n",
      "Iteration 1152, Average Reward: -102.92\n",
      "Iteration 1153, Average Reward: -110.87\n",
      "Iteration 1154, Average Reward: -129.58\n",
      "Iteration 1155, Average Reward: -129.26\n",
      "Iteration 1156, Average Reward: -127.12\n",
      "Iteration 1157, Average Reward: -128.98\n",
      "Iteration 1158, Average Reward: -147.6\n",
      "Iteration 1159, Average Reward: -131.87\n",
      "Iteration 1160, Average Reward: -124.16\n",
      "Iteration 1161, Average Reward: -120.08\n",
      "Iteration 1162, Average Reward: -128.28\n",
      "Iteration 1163, Average Reward: -122.67\n",
      "Iteration 1164, Average Reward: -126.27\n",
      "Iteration 1165, Average Reward: -127.03\n",
      "Iteration 1166, Average Reward: -119.71\n",
      "Iteration 1167, Average Reward: -106.16\n",
      "Iteration 1168, Average Reward: -118.78\n",
      "Iteration 1169, Average Reward: -140.08\n",
      "Iteration 1170, Average Reward: -127.35\n",
      "Iteration 1171, Average Reward: -124.57\n",
      "Iteration 1172, Average Reward: -115.13\n",
      "Iteration 1173, Average Reward: -106.91\n",
      "Iteration 1174, Average Reward: -110.0\n",
      "Iteration 1175, Average Reward: -133.2\n",
      "Iteration 1176, Average Reward: -130.96\n",
      "Iteration 1177, Average Reward: -112.39\n",
      "Iteration 1178, Average Reward: -107.68\n",
      "Iteration 1179, Average Reward: -123.61\n",
      "Iteration 1180, Average Reward: -134.47\n",
      "Iteration 1181, Average Reward: -125.33\n",
      "Iteration 1182, Average Reward: -139.4\n",
      "Iteration 1183, Average Reward: -142.23\n",
      "Iteration 1184, Average Reward: -113.42\n",
      "Iteration 1185, Average Reward: -133.96\n",
      "Iteration 1186, Average Reward: -133.15\n",
      "Iteration 1187, Average Reward: -122.44\n",
      "Iteration 1188, Average Reward: -124.89\n",
      "Iteration 1189, Average Reward: -128.72\n",
      "Iteration 1190, Average Reward: -128.39\n",
      "Iteration 1191, Average Reward: -109.57\n",
      "Iteration 1192, Average Reward: -96.39\n",
      "Iteration 1193, Average Reward: -114.19\n",
      "Iteration 1194, Average Reward: -115.25\n",
      "Iteration 1195, Average Reward: -122.07\n",
      "Iteration 1196, Average Reward: -128.09\n",
      "Iteration 1197, Average Reward: -130.28\n",
      "Iteration 1198, Average Reward: -122.89\n",
      "Iteration 1199, Average Reward: -121.48\n",
      "Iteration 1200, Average Reward: -110.79\n",
      "Iteration 1201, Average Reward: -111.3\n",
      "Iteration 1202, Average Reward: -106.06\n",
      "Iteration 1203, Average Reward: -104.54\n",
      "Iteration 1204, Average Reward: -104.81\n",
      "Iteration 1205, Average Reward: -106.17\n",
      "Iteration 1206, Average Reward: -106.65\n",
      "Iteration 1207, Average Reward: -124.01\n",
      "Iteration 1208, Average Reward: -148.63\n",
      "Iteration 1209, Average Reward: -156.97\n",
      "Iteration 1210, Average Reward: -129.54\n",
      "Iteration 1211, Average Reward: -110.28\n",
      "Iteration 1212, Average Reward: -122.88\n",
      "Iteration 1213, Average Reward: -123.7\n",
      "Iteration 1214, Average Reward: -117.8\n",
      "Iteration 1215, Average Reward: -113.74\n",
      "Iteration 1216, Average Reward: -110.93\n",
      "Iteration 1217, Average Reward: -119.4\n",
      "Iteration 1218, Average Reward: -132.98\n",
      "Iteration 1219, Average Reward: -141.54\n",
      "Iteration 1220, Average Reward: -127.13\n",
      "Iteration 1221, Average Reward: -130.08\n",
      "Iteration 1222, Average Reward: -135.87\n",
      "Iteration 1223, Average Reward: -126.24\n",
      "Iteration 1224, Average Reward: -123.15\n",
      "Iteration 1225, Average Reward: -137.13\n",
      "Iteration 1226, Average Reward: -124.68\n",
      "Iteration 1227, Average Reward: -134.95\n",
      "Iteration 1228, Average Reward: -137.82\n",
      "Iteration 1229, Average Reward: -116.36\n",
      "Iteration 1230, Average Reward: -96.63\n",
      "Iteration 1231, Average Reward: -114.91\n",
      "Iteration 1232, Average Reward: -124.39\n",
      "Iteration 1233, Average Reward: -111.65\n",
      "Iteration 1234, Average Reward: -108.44\n",
      "Iteration 1235, Average Reward: -130.51\n",
      "Iteration 1236, Average Reward: -148.06\n",
      "Iteration 1237, Average Reward: -149.77\n",
      "Iteration 1238, Average Reward: -143.64\n",
      "Iteration 1239, Average Reward: -143.36\n",
      "Iteration 1240, Average Reward: -126.1\n",
      "Iteration 1241, Average Reward: -117.21\n",
      "Iteration 1242, Average Reward: -98.89\n",
      "Iteration 1243, Average Reward: -93.98\n",
      "Iteration 1244, Average Reward: -108.02\n",
      "Iteration 1245, Average Reward: -128.19\n",
      "Iteration 1246, Average Reward: -126.11\n",
      "Iteration 1247, Average Reward: -105.17\n",
      "Iteration 1248, Average Reward: -126.48\n",
      "Iteration 1249, Average Reward: -119.92\n",
      "Iteration 1250, Average Reward: -139.88\n",
      "Iteration 1251, Average Reward: -139.58\n",
      "Iteration 1252, Average Reward: -143.5\n",
      "Iteration 1253, Average Reward: -128.04\n",
      "Iteration 1254, Average Reward: -116.95\n",
      "Iteration 1255, Average Reward: -112.97\n",
      "Iteration 1256, Average Reward: -122.86\n",
      "Iteration 1257, Average Reward: -112.07\n",
      "Iteration 1258, Average Reward: -109.62\n",
      "Iteration 1259, Average Reward: -122.73\n",
      "Iteration 1260, Average Reward: -143.88\n",
      "Iteration 1261, Average Reward: -142.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1262, Average Reward: -142.42\n",
      "Iteration 1263, Average Reward: -146.49\n",
      "Iteration 1264, Average Reward: -139.48\n",
      "Iteration 1265, Average Reward: -127.33\n",
      "Iteration 1266, Average Reward: -125.45\n",
      "Iteration 1267, Average Reward: -137.91\n",
      "Iteration 1268, Average Reward: -132.31\n",
      "Iteration 1269, Average Reward: -125.0\n",
      "Iteration 1270, Average Reward: -120.05\n",
      "Iteration 1271, Average Reward: -114.1\n",
      "Iteration 1272, Average Reward: -116.61\n",
      "Iteration 1273, Average Reward: -146.82\n",
      "Iteration 1274, Average Reward: -142.15\n",
      "Iteration 1275, Average Reward: -142.18\n",
      "Iteration 1276, Average Reward: -135.19\n",
      "Iteration 1277, Average Reward: -128.56\n",
      "Iteration 1278, Average Reward: -125.51\n",
      "Iteration 1279, Average Reward: -97.43\n",
      "Iteration 1280, Average Reward: -115.16\n",
      "Iteration 1281, Average Reward: -155.85\n",
      "Iteration 1282, Average Reward: -149.41\n",
      "Iteration 1283, Average Reward: -138.84\n",
      "Iteration 1284, Average Reward: -128.0\n",
      "Iteration 1285, Average Reward: -126.25\n",
      "Iteration 1286, Average Reward: -123.12\n",
      "Iteration 1287, Average Reward: -121.67\n",
      "Iteration 1288, Average Reward: -119.57\n",
      "Iteration 1289, Average Reward: -132.12\n",
      "Iteration 1290, Average Reward: -123.5\n",
      "Iteration 1291, Average Reward: -131.15\n",
      "Iteration 1292, Average Reward: -119.9\n",
      "Iteration 1293, Average Reward: -121.99\n",
      "Iteration 1294, Average Reward: -112.33\n",
      "Iteration 1295, Average Reward: -124.81\n",
      "Iteration 1296, Average Reward: -123.35\n",
      "Iteration 1297, Average Reward: -127.83\n",
      "Iteration 1298, Average Reward: -132.21\n",
      "Iteration 1299, Average Reward: -137.26\n",
      "Iteration 1300, Average Reward: -130.65\n",
      "Iteration 1301, Average Reward: -116.41\n",
      "Iteration 1302, Average Reward: -115.71\n",
      "Iteration 1303, Average Reward: -129.74\n",
      "Iteration 1304, Average Reward: -138.89\n",
      "Iteration 1305, Average Reward: -130.66\n",
      "Iteration 1306, Average Reward: -127.31\n",
      "Iteration 1307, Average Reward: -133.97\n",
      "Iteration 1308, Average Reward: -128.51\n",
      "Iteration 1309, Average Reward: -123.91\n",
      "Iteration 1310, Average Reward: -127.78\n",
      "Iteration 1311, Average Reward: -137.46\n",
      "Iteration 1312, Average Reward: -132.5\n",
      "Iteration 1313, Average Reward: -120.9\n",
      "Iteration 1314, Average Reward: -111.78\n",
      "Iteration 1315, Average Reward: -110.53\n",
      "Iteration 1316, Average Reward: -110.3\n",
      "Iteration 1317, Average Reward: -114.31\n",
      "Iteration 1318, Average Reward: -117.33\n",
      "Iteration 1319, Average Reward: -128.47\n",
      "Iteration 1320, Average Reward: -159.92\n",
      "Iteration 1321, Average Reward: -136.59\n",
      "Iteration 1322, Average Reward: -115.37\n",
      "Iteration 1323, Average Reward: -102.74\n",
      "Iteration 1324, Average Reward: -116.07\n",
      "Iteration 1325, Average Reward: -112.63\n",
      "Iteration 1326, Average Reward: -112.55\n",
      "Iteration 1327, Average Reward: -110.89\n",
      "Iteration 1328, Average Reward: -124.32\n",
      "Iteration 1329, Average Reward: -130.87\n",
      "Iteration 1330, Average Reward: -147.8\n",
      "Iteration 1331, Average Reward: -137.97\n",
      "Iteration 1332, Average Reward: -122.82\n",
      "Iteration 1333, Average Reward: -107.53\n",
      "Iteration 1334, Average Reward: -104.7\n",
      "Iteration 1335, Average Reward: -103.56\n",
      "Iteration 1336, Average Reward: -114.63\n",
      "Iteration 1337, Average Reward: -114.24\n",
      "Iteration 1338, Average Reward: -126.07\n",
      "Iteration 1339, Average Reward: -132.82\n",
      "Iteration 1340, Average Reward: -126.63\n",
      "Iteration 1341, Average Reward: -118.29\n",
      "Iteration 1342, Average Reward: -125.48\n",
      "Iteration 1343, Average Reward: -117.63\n",
      "Iteration 1344, Average Reward: -133.96\n",
      "Iteration 1345, Average Reward: -148.38\n",
      "Iteration 1346, Average Reward: -126.92\n",
      "Iteration 1347, Average Reward: -106.04\n",
      "Iteration 1348, Average Reward: -110.03\n",
      "Iteration 1349, Average Reward: -135.7\n",
      "Iteration 1350, Average Reward: -144.98\n",
      "Iteration 1351, Average Reward: -133.69\n",
      "Iteration 1352, Average Reward: -132.14\n",
      "Iteration 1353, Average Reward: -129.8\n",
      "Iteration 1354, Average Reward: -125.61\n",
      "Iteration 1355, Average Reward: -127.5\n",
      "Iteration 1356, Average Reward: -133.84\n",
      "Iteration 1357, Average Reward: -120.22\n",
      "Iteration 1358, Average Reward: -114.51\n",
      "Iteration 1359, Average Reward: -125.81\n",
      "Iteration 1360, Average Reward: -109.16\n",
      "Iteration 1361, Average Reward: -115.49\n",
      "Iteration 1362, Average Reward: -107.81\n",
      "Iteration 1363, Average Reward: -111.39\n",
      "Iteration 1364, Average Reward: -130.29\n",
      "Iteration 1365, Average Reward: -136.34\n",
      "Iteration 1366, Average Reward: -129.5\n",
      "Iteration 1367, Average Reward: -144.51\n",
      "Iteration 1368, Average Reward: -149.58\n",
      "Iteration 1369, Average Reward: -138.71\n",
      "Iteration 1370, Average Reward: -130.52\n",
      "Iteration 1371, Average Reward: -124.89\n",
      "Iteration 1372, Average Reward: -116.92\n",
      "Iteration 1373, Average Reward: -97.12\n",
      "Iteration 1374, Average Reward: -110.95\n",
      "Iteration 1375, Average Reward: -122.14\n",
      "Iteration 1376, Average Reward: -123.49\n",
      "Iteration 1377, Average Reward: -115.28\n",
      "Iteration 1378, Average Reward: -123.58\n",
      "Iteration 1379, Average Reward: -136.62\n",
      "Iteration 1380, Average Reward: -120.49\n",
      "Iteration 1381, Average Reward: -104.75\n",
      "Iteration 1382, Average Reward: -115.27\n",
      "Iteration 1383, Average Reward: -119.61\n",
      "Iteration 1384, Average Reward: -121.51\n",
      "Iteration 1385, Average Reward: -119.47\n",
      "Iteration 1386, Average Reward: -116.94\n",
      "Iteration 1387, Average Reward: -117.86\n",
      "Iteration 1388, Average Reward: -125.77\n",
      "Iteration 1389, Average Reward: -130.47\n",
      "Iteration 1390, Average Reward: -114.74\n",
      "Iteration 1391, Average Reward: -117.35\n",
      "Iteration 1392, Average Reward: -122.63\n",
      "Iteration 1393, Average Reward: -125.93\n",
      "Iteration 1394, Average Reward: -126.84\n",
      "Iteration 1395, Average Reward: -133.48\n",
      "Iteration 1396, Average Reward: -122.74\n",
      "Iteration 1397, Average Reward: -134.75\n",
      "Iteration 1398, Average Reward: -132.51\n",
      "Iteration 1399, Average Reward: -133.7\n",
      "Iteration 1400, Average Reward: -148.31\n",
      "Iteration 1401, Average Reward: -133.99\n",
      "Iteration 1402, Average Reward: -117.02\n",
      "Iteration 1403, Average Reward: -116.63\n",
      "Iteration 1404, Average Reward: -132.18\n",
      "Iteration 1405, Average Reward: -135.36\n",
      "Iteration 1406, Average Reward: -117.72\n",
      "Iteration 1407, Average Reward: -115.23\n",
      "Iteration 1408, Average Reward: -131.47\n",
      "Iteration 1409, Average Reward: -121.65\n",
      "Iteration 1410, Average Reward: -104.88\n",
      "Iteration 1411, Average Reward: -96.04\n",
      "Iteration 1412, Average Reward: -116.88\n",
      "Iteration 1413, Average Reward: -121.95\n",
      "Iteration 1414, Average Reward: -126.13\n",
      "Iteration 1415, Average Reward: -123.28\n",
      "Iteration 1416, Average Reward: -128.64\n",
      "Iteration 1417, Average Reward: -130.67\n",
      "Iteration 1418, Average Reward: -112.28\n",
      "Iteration 1419, Average Reward: -124.42\n",
      "Iteration 1420, Average Reward: -125.69\n",
      "Iteration 1421, Average Reward: -121.66\n",
      "Iteration 1422, Average Reward: -131.43\n",
      "Iteration 1423, Average Reward: -121.2\n",
      "Iteration 1424, Average Reward: -113.53\n",
      "Iteration 1425, Average Reward: -109.96\n",
      "Iteration 1426, Average Reward: -100.23\n",
      "Iteration 1427, Average Reward: -107.52\n",
      "Iteration 1428, Average Reward: -108.01\n",
      "Iteration 1429, Average Reward: -119.91\n",
      "Iteration 1430, Average Reward: -150.34\n",
      "Iteration 1431, Average Reward: -147.54\n",
      "Iteration 1432, Average Reward: -137.39\n",
      "Iteration 1433, Average Reward: -113.42\n",
      "Iteration 1434, Average Reward: -111.79\n",
      "Iteration 1435, Average Reward: -89.99\n",
      "Iteration 1436, Average Reward: -110.35\n",
      "Iteration 1437, Average Reward: -124.81\n",
      "Iteration 1438, Average Reward: -127.84\n",
      "Iteration 1439, Average Reward: -133.93\n",
      "Iteration 1440, Average Reward: -125.9\n",
      "Iteration 1441, Average Reward: -121.61\n",
      "Iteration 1442, Average Reward: -133.7\n",
      "Iteration 1443, Average Reward: -129.36\n",
      "Iteration 1444, Average Reward: -121.77\n",
      "Iteration 1445, Average Reward: -128.38\n",
      "Iteration 1446, Average Reward: -127.16\n",
      "Iteration 1447, Average Reward: -134.8\n",
      "Iteration 1448, Average Reward: -132.71\n",
      "Iteration 1449, Average Reward: -126.37\n",
      "Iteration 1450, Average Reward: -128.65\n",
      "Iteration 1451, Average Reward: -116.76\n",
      "Iteration 1452, Average Reward: -103.55\n",
      "Iteration 1453, Average Reward: -109.58\n",
      "Iteration 1454, Average Reward: -108.24\n",
      "Iteration 1455, Average Reward: -123.15\n",
      "Iteration 1456, Average Reward: -136.86\n",
      "Iteration 1457, Average Reward: -141.43\n",
      "Iteration 1458, Average Reward: -136.28\n",
      "Iteration 1459, Average Reward: -128.85\n",
      "Iteration 1460, Average Reward: -128.99\n",
      "Iteration 1461, Average Reward: -131.7\n",
      "Iteration 1462, Average Reward: -113.36\n",
      "Iteration 1463, Average Reward: -109.79\n",
      "Iteration 1464, Average Reward: -110.69\n",
      "Iteration 1465, Average Reward: -117.76\n",
      "Iteration 1466, Average Reward: -124.07\n",
      "Iteration 1467, Average Reward: -116.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1468, Average Reward: -127.07\n",
      "Iteration 1469, Average Reward: -135.68\n",
      "Iteration 1470, Average Reward: -113.15\n",
      "Iteration 1471, Average Reward: -119.08\n",
      "Iteration 1472, Average Reward: -126.62\n",
      "Iteration 1473, Average Reward: -142.54\n",
      "Iteration 1474, Average Reward: -123.85\n",
      "Iteration 1475, Average Reward: -112.87\n",
      "Iteration 1476, Average Reward: -130.85\n",
      "Iteration 1477, Average Reward: -134.54\n",
      "Iteration 1478, Average Reward: -124.97\n",
      "Iteration 1479, Average Reward: -111.99\n",
      "Iteration 1480, Average Reward: -116.78\n",
      "Iteration 1481, Average Reward: -133.09\n",
      "Iteration 1482, Average Reward: -135.65\n",
      "Iteration 1483, Average Reward: -140.22\n",
      "Iteration 1484, Average Reward: -143.16\n",
      "Iteration 1485, Average Reward: -139.13\n",
      "Iteration 1486, Average Reward: -139.52\n",
      "Iteration 1487, Average Reward: -117.92\n",
      "Iteration 1488, Average Reward: -109.48\n",
      "Iteration 1489, Average Reward: -109.77\n",
      "Iteration 1490, Average Reward: -112.87\n",
      "Iteration 1491, Average Reward: -124.74\n",
      "Iteration 1492, Average Reward: -119.31\n",
      "Iteration 1493, Average Reward: -110.39\n",
      "Iteration 1494, Average Reward: -121.82\n",
      "Iteration 1495, Average Reward: -130.48\n",
      "Iteration 1496, Average Reward: -143.62\n",
      "Iteration 1497, Average Reward: -132.76\n",
      "Iteration 1498, Average Reward: -113.42\n",
      "Iteration 1499, Average Reward: -106.02\n",
      "Iteration 1500, Average Reward: -116.02\n",
      "Iteration 1501, Average Reward: -111.8\n",
      "Iteration 1502, Average Reward: -109.63\n",
      "Iteration 1503, Average Reward: -128.9\n",
      "Iteration 1504, Average Reward: -113.14\n",
      "Iteration 1505, Average Reward: -105.26\n",
      "Iteration 1506, Average Reward: -106.38\n",
      "Iteration 1507, Average Reward: -119.13\n",
      "Iteration 1508, Average Reward: -118.46\n",
      "Iteration 1509, Average Reward: -115.29\n",
      "Iteration 1510, Average Reward: -117.28\n",
      "Iteration 1511, Average Reward: -132.78\n",
      "Iteration 1512, Average Reward: -123.58\n",
      "Iteration 1513, Average Reward: -140.96\n",
      "Iteration 1514, Average Reward: -126.4\n",
      "Iteration 1515, Average Reward: -128.93\n",
      "Iteration 1516, Average Reward: -114.29\n",
      "Iteration 1517, Average Reward: -115.09\n",
      "Iteration 1518, Average Reward: -119.19\n",
      "Iteration 1519, Average Reward: -89.54\n",
      "Iteration 1520, Average Reward: -109.21\n",
      "Iteration 1521, Average Reward: -122.81\n",
      "Iteration 1522, Average Reward: -141.33\n",
      "Iteration 1523, Average Reward: -138.54\n",
      "Iteration 1524, Average Reward: -127.53\n",
      "Iteration 1525, Average Reward: -124.25\n",
      "Iteration 1526, Average Reward: -128.68\n",
      "Iteration 1527, Average Reward: -107.21\n",
      "Iteration 1528, Average Reward: -109.75\n",
      "Iteration 1529, Average Reward: -130.56\n",
      "Iteration 1530, Average Reward: -122.53\n",
      "Iteration 1531, Average Reward: -128.05\n",
      "Iteration 1532, Average Reward: -146.9\n",
      "Iteration 1533, Average Reward: -140.97\n",
      "Iteration 1534, Average Reward: -130.4\n",
      "Iteration 1535, Average Reward: -131.53\n",
      "Iteration 1536, Average Reward: -126.69\n",
      "Iteration 1537, Average Reward: -116.06\n",
      "Iteration 1538, Average Reward: -119.39\n",
      "Iteration 1539, Average Reward: -125.64\n",
      "Iteration 1540, Average Reward: -131.71\n",
      "Iteration 1541, Average Reward: -130.21\n",
      "Iteration 1542, Average Reward: -130.85\n",
      "Iteration 1543, Average Reward: -139.16\n",
      "Iteration 1544, Average Reward: -135.13\n",
      "Iteration 1545, Average Reward: -122.33\n",
      "Iteration 1546, Average Reward: -120.83\n",
      "Iteration 1547, Average Reward: -108.86\n",
      "Iteration 1548, Average Reward: -106.1\n",
      "Iteration 1549, Average Reward: -119.26\n",
      "Iteration 1550, Average Reward: -135.82\n",
      "Iteration 1551, Average Reward: -135.47\n",
      "Iteration 1552, Average Reward: -130.3\n",
      "Iteration 1553, Average Reward: -131.1\n",
      "Iteration 1554, Average Reward: -113.55\n",
      "Iteration 1555, Average Reward: -115.7\n",
      "Iteration 1556, Average Reward: -116.97\n",
      "Iteration 1557, Average Reward: -128.65\n",
      "Iteration 1558, Average Reward: -112.6\n",
      "Iteration 1559, Average Reward: -113.94\n",
      "Iteration 1560, Average Reward: -114.24\n",
      "Iteration 1561, Average Reward: -120.67\n",
      "Iteration 1562, Average Reward: -118.28\n",
      "Iteration 1563, Average Reward: -132.69\n",
      "Iteration 1564, Average Reward: -136.64\n",
      "Iteration 1565, Average Reward: -133.35\n",
      "Iteration 1566, Average Reward: -127.03\n",
      "Iteration 1567, Average Reward: -131.27\n",
      "Iteration 1568, Average Reward: -127.72\n",
      "Iteration 1569, Average Reward: -123.25\n",
      "Iteration 1570, Average Reward: -119.26\n",
      "Iteration 1571, Average Reward: -141.45\n",
      "Iteration 1572, Average Reward: -111.87\n",
      "Iteration 1573, Average Reward: -112.75\n",
      "Iteration 1574, Average Reward: -147.98\n",
      "Iteration 1575, Average Reward: -131.58\n",
      "Iteration 1576, Average Reward: -111.04\n",
      "Iteration 1577, Average Reward: -131.2\n",
      "Iteration 1578, Average Reward: -124.74\n",
      "Iteration 1579, Average Reward: -121.72\n",
      "Iteration 1580, Average Reward: -126.37\n",
      "Iteration 1581, Average Reward: -141.64\n",
      "Iteration 1582, Average Reward: -133.65\n",
      "Iteration 1583, Average Reward: -130.14\n",
      "Iteration 1584, Average Reward: -134.62\n",
      "Iteration 1585, Average Reward: -131.24\n",
      "Iteration 1586, Average Reward: -120.19\n",
      "Iteration 1587, Average Reward: -126.13\n",
      "Iteration 1588, Average Reward: -125.95\n",
      "Iteration 1589, Average Reward: -123.95\n",
      "Iteration 1590, Average Reward: -122.07\n",
      "Iteration 1591, Average Reward: -111.33\n",
      "Iteration 1592, Average Reward: -112.42\n",
      "Iteration 1593, Average Reward: -114.82\n",
      "Iteration 1594, Average Reward: -115.42\n",
      "Iteration 1595, Average Reward: -112.52\n",
      "Iteration 1596, Average Reward: -129.59\n",
      "Iteration 1597, Average Reward: -151.04\n",
      "Iteration 1598, Average Reward: -155.27\n",
      "Iteration 1599, Average Reward: -137.77\n",
      "Iteration 1600, Average Reward: -121.85\n",
      "Iteration 1601, Average Reward: -128.36\n",
      "Iteration 1602, Average Reward: -124.39\n",
      "Iteration 1603, Average Reward: -107.4\n",
      "Iteration 1604, Average Reward: -132.39\n",
      "Iteration 1605, Average Reward: -129.18\n",
      "Iteration 1606, Average Reward: -130.05\n",
      "Iteration 1607, Average Reward: -117.64\n",
      "Iteration 1608, Average Reward: -124.09\n",
      "Iteration 1609, Average Reward: -124.95\n",
      "Iteration 1610, Average Reward: -125.32\n",
      "Iteration 1611, Average Reward: -118.55\n",
      "Iteration 1612, Average Reward: -107.45\n",
      "Iteration 1613, Average Reward: -126.17\n",
      "Iteration 1614, Average Reward: -120.76\n",
      "Iteration 1615, Average Reward: -108.35\n",
      "Iteration 1616, Average Reward: -123.27\n",
      "Iteration 1617, Average Reward: -125.45\n",
      "Iteration 1618, Average Reward: -122.68\n",
      "Iteration 1619, Average Reward: -120.85\n",
      "Iteration 1620, Average Reward: -127.29\n",
      "Iteration 1621, Average Reward: -119.53\n",
      "Iteration 1622, Average Reward: -122.32\n",
      "Iteration 1623, Average Reward: -133.04\n",
      "Iteration 1624, Average Reward: -130.43\n",
      "Iteration 1625, Average Reward: -119.28\n",
      "Iteration 1626, Average Reward: -135.56\n",
      "Iteration 1627, Average Reward: -134.07\n",
      "Iteration 1628, Average Reward: -108.96\n",
      "Iteration 1629, Average Reward: -111.87\n",
      "Iteration 1630, Average Reward: -114.7\n",
      "Iteration 1631, Average Reward: -107.4\n",
      "Iteration 1632, Average Reward: -115.1\n",
      "Iteration 1633, Average Reward: -117.78\n",
      "Iteration 1634, Average Reward: -123.99\n",
      "Iteration 1635, Average Reward: -118.88\n",
      "Iteration 1636, Average Reward: -111.27\n",
      "Iteration 1637, Average Reward: -101.15\n",
      "Iteration 1638, Average Reward: -107.3\n",
      "Iteration 1639, Average Reward: -122.47\n",
      "Iteration 1640, Average Reward: -131.96\n",
      "Iteration 1641, Average Reward: -133.77\n",
      "Iteration 1642, Average Reward: -128.54\n",
      "Iteration 1643, Average Reward: -139.38\n",
      "Iteration 1644, Average Reward: -147.49\n",
      "Iteration 1645, Average Reward: -134.94\n",
      "Iteration 1646, Average Reward: -123.21\n",
      "Iteration 1647, Average Reward: -109.38\n",
      "Iteration 1648, Average Reward: -106.33\n",
      "Iteration 1649, Average Reward: -116.65\n",
      "Iteration 1650, Average Reward: -122.1\n",
      "Iteration 1651, Average Reward: -128.74\n",
      "Iteration 1652, Average Reward: -132.59\n",
      "Iteration 1653, Average Reward: -117.74\n",
      "Iteration 1654, Average Reward: -112.57\n",
      "Iteration 1655, Average Reward: -106.63\n",
      "Iteration 1656, Average Reward: -105.35\n",
      "Iteration 1657, Average Reward: -119.56\n",
      "Iteration 1658, Average Reward: -123.2\n",
      "Iteration 1659, Average Reward: -132.5\n",
      "Iteration 1660, Average Reward: -137.24\n",
      "Iteration 1661, Average Reward: -132.1\n",
      "Iteration 1662, Average Reward: -134.93\n",
      "Iteration 1663, Average Reward: -145.37\n",
      "Iteration 1664, Average Reward: -132.5\n",
      "Iteration 1665, Average Reward: -128.8\n",
      "Iteration 1666, Average Reward: -124.55\n",
      "Iteration 1667, Average Reward: -123.08\n",
      "Iteration 1668, Average Reward: -126.35\n",
      "Iteration 1669, Average Reward: -133.48\n",
      "Iteration 1670, Average Reward: -132.65\n",
      "Iteration 1671, Average Reward: -141.61\n",
      "Iteration 1672, Average Reward: -109.74\n",
      "Iteration 1673, Average Reward: -126.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1674, Average Reward: -113.23\n",
      "Iteration 1675, Average Reward: -112.59\n",
      "Iteration 1676, Average Reward: -122.96\n",
      "Iteration 1677, Average Reward: -138.56\n",
      "Iteration 1678, Average Reward: -142.48\n",
      "Iteration 1679, Average Reward: -136.24\n",
      "Iteration 1680, Average Reward: -116.14\n",
      "Iteration 1681, Average Reward: -110.51\n",
      "Iteration 1682, Average Reward: -116.91\n",
      "Iteration 1683, Average Reward: -120.1\n",
      "Iteration 1684, Average Reward: -114.47\n",
      "Iteration 1685, Average Reward: -121.42\n",
      "Iteration 1686, Average Reward: -98.51\n",
      "Iteration 1687, Average Reward: -107.4\n",
      "Iteration 1688, Average Reward: -112.91\n",
      "Iteration 1689, Average Reward: -115.89\n",
      "Iteration 1690, Average Reward: -114.01\n",
      "Iteration 1691, Average Reward: -108.93\n",
      "Iteration 1692, Average Reward: -110.08\n",
      "Iteration 1693, Average Reward: -121.63\n",
      "Iteration 1694, Average Reward: -131.6\n",
      "Iteration 1695, Average Reward: -126.89\n",
      "Iteration 1696, Average Reward: -138.33\n",
      "Iteration 1697, Average Reward: -138.92\n",
      "Iteration 1698, Average Reward: -121.21\n",
      "Iteration 1699, Average Reward: -114.93\n",
      "Iteration 1700, Average Reward: -116.51\n",
      "Iteration 1701, Average Reward: -140.91\n",
      "Iteration 1702, Average Reward: -138.79\n",
      "Iteration 1703, Average Reward: -131.02\n",
      "Iteration 1704, Average Reward: -144.72\n",
      "Iteration 1705, Average Reward: -132.6\n",
      "Iteration 1706, Average Reward: -120.83\n",
      "Iteration 1707, Average Reward: -114.49\n",
      "Iteration 1708, Average Reward: -115.37\n",
      "Iteration 1709, Average Reward: -131.82\n",
      "Iteration 1710, Average Reward: -129.46\n",
      "Iteration 1711, Average Reward: -123.51\n",
      "Iteration 1712, Average Reward: -132.04\n",
      "Iteration 1713, Average Reward: -144.72\n",
      "Iteration 1714, Average Reward: -128.34\n",
      "Iteration 1715, Average Reward: -115.85\n",
      "Iteration 1716, Average Reward: -110.32\n",
      "Iteration 1717, Average Reward: -112.72\n",
      "Iteration 1718, Average Reward: -116.05\n",
      "Iteration 1719, Average Reward: -122.3\n",
      "Iteration 1720, Average Reward: -133.74\n",
      "Iteration 1721, Average Reward: -133.9\n",
      "Iteration 1722, Average Reward: -123.57\n",
      "Iteration 1723, Average Reward: -128.14\n",
      "Iteration 1724, Average Reward: -120.44\n",
      "Iteration 1725, Average Reward: -108.26\n",
      "Iteration 1726, Average Reward: -110.83\n",
      "Iteration 1727, Average Reward: -117.23\n",
      "Iteration 1728, Average Reward: -119.8\n",
      "Iteration 1729, Average Reward: -127.73\n",
      "Iteration 1730, Average Reward: -120.17\n",
      "Iteration 1731, Average Reward: -114.79\n",
      "Iteration 1732, Average Reward: -129.93\n",
      "Iteration 1733, Average Reward: -140.96\n",
      "Iteration 1734, Average Reward: -153.05\n",
      "Iteration 1735, Average Reward: -136.86\n",
      "Iteration 1736, Average Reward: -97.19\n",
      "Iteration 1737, Average Reward: -116.08\n",
      "Iteration 1738, Average Reward: -111.89\n",
      "Iteration 1739, Average Reward: -93.23\n",
      "Iteration 1740, Average Reward: -95.38\n",
      "Iteration 1741, Average Reward: -112.11\n",
      "Iteration 1742, Average Reward: -109.31\n",
      "Iteration 1743, Average Reward: -132.55\n",
      "Iteration 1744, Average Reward: -143.22\n",
      "Iteration 1745, Average Reward: -144.44\n",
      "Iteration 1746, Average Reward: -144.21\n",
      "Iteration 1747, Average Reward: -125.73\n",
      "Iteration 1748, Average Reward: -120.22\n",
      "Iteration 1749, Average Reward: -127.35\n",
      "Iteration 1750, Average Reward: -124.61\n",
      "Iteration 1751, Average Reward: -126.36\n",
      "Iteration 1752, Average Reward: -111.07\n",
      "Iteration 1753, Average Reward: -132.2\n",
      "Iteration 1754, Average Reward: -124.88\n",
      "Iteration 1755, Average Reward: -112.88\n",
      "Iteration 1756, Average Reward: -116.78\n",
      "Iteration 1757, Average Reward: -109.7\n",
      "Iteration 1758, Average Reward: -112.68\n",
      "Iteration 1759, Average Reward: -127.17\n",
      "Iteration 1760, Average Reward: -153.11\n",
      "Iteration 1761, Average Reward: -145.45\n",
      "Iteration 1762, Average Reward: -122.09\n",
      "Iteration 1763, Average Reward: -130.01\n",
      "Iteration 1764, Average Reward: -114.13\n",
      "Iteration 1765, Average Reward: -105.22\n",
      "Iteration 1766, Average Reward: -112.53\n",
      "Iteration 1767, Average Reward: -123.86\n",
      "Iteration 1768, Average Reward: -113.54\n",
      "Iteration 1769, Average Reward: -114.26\n",
      "Iteration 1770, Average Reward: -125.93\n",
      "Iteration 1771, Average Reward: -111.22\n",
      "Iteration 1772, Average Reward: -112.35\n",
      "Iteration 1773, Average Reward: -130.72\n",
      "Iteration 1774, Average Reward: -131.48\n",
      "Iteration 1775, Average Reward: -115.45\n",
      "Iteration 1776, Average Reward: -109.16\n",
      "Iteration 1777, Average Reward: -114.61\n",
      "Iteration 1778, Average Reward: -111.32\n",
      "Iteration 1779, Average Reward: -108.77\n",
      "Iteration 1780, Average Reward: -114.22\n",
      "Iteration 1781, Average Reward: -130.91\n",
      "Iteration 1782, Average Reward: -125.87\n",
      "Iteration 1783, Average Reward: -114.28\n",
      "Iteration 1784, Average Reward: -136.26\n",
      "Iteration 1785, Average Reward: -145.54\n",
      "Iteration 1786, Average Reward: -140.83\n",
      "Iteration 1787, Average Reward: -122.45\n",
      "Iteration 1788, Average Reward: -131.7\n",
      "Iteration 1789, Average Reward: -107.26\n",
      "Iteration 1790, Average Reward: -103.93\n",
      "Iteration 1791, Average Reward: -114.68\n",
      "Iteration 1792, Average Reward: -123.63\n",
      "Iteration 1793, Average Reward: -116.72\n",
      "Iteration 1794, Average Reward: -136.84\n",
      "Iteration 1795, Average Reward: -142.02\n",
      "Iteration 1796, Average Reward: -118.22\n",
      "Iteration 1797, Average Reward: -119.18\n",
      "Iteration 1798, Average Reward: -125.41\n",
      "Iteration 1799, Average Reward: -129.35\n",
      "Iteration 1800, Average Reward: -111.65\n",
      "Iteration 1801, Average Reward: -115.08\n",
      "Iteration 1802, Average Reward: -115.87\n",
      "Iteration 1803, Average Reward: -104.14\n",
      "Iteration 1804, Average Reward: -110.47\n",
      "Iteration 1805, Average Reward: -131.79\n",
      "Iteration 1806, Average Reward: -140.13\n",
      "Iteration 1807, Average Reward: -140.38\n",
      "Iteration 1808, Average Reward: -136.51\n",
      "Iteration 1809, Average Reward: -122.05\n",
      "Iteration 1810, Average Reward: -121.6\n",
      "Iteration 1811, Average Reward: -112.51\n",
      "Iteration 1812, Average Reward: -95.66\n",
      "Iteration 1813, Average Reward: -117.34\n",
      "Iteration 1814, Average Reward: -123.76\n",
      "Iteration 1815, Average Reward: -116.59\n",
      "Iteration 1816, Average Reward: -121.71\n",
      "Iteration 1817, Average Reward: -114.81\n",
      "Iteration 1818, Average Reward: -111.23\n",
      "Iteration 1819, Average Reward: -124.69\n",
      "Iteration 1820, Average Reward: -114.57\n",
      "Iteration 1821, Average Reward: -122.57\n",
      "Iteration 1822, Average Reward: -138.39\n",
      "Iteration 1823, Average Reward: -120.8\n",
      "Iteration 1824, Average Reward: -123.09\n",
      "Iteration 1825, Average Reward: -125.02\n",
      "Iteration 1826, Average Reward: -119.83\n",
      "Iteration 1827, Average Reward: -122.53\n",
      "Iteration 1828, Average Reward: -119.38\n",
      "Iteration 1829, Average Reward: -128.55\n",
      "Iteration 1830, Average Reward: -115.43\n",
      "Iteration 1831, Average Reward: -117.18\n",
      "Iteration 1832, Average Reward: -126.54\n",
      "Iteration 1833, Average Reward: -127.79\n",
      "Iteration 1834, Average Reward: -117.69\n",
      "Iteration 1835, Average Reward: -121.55\n",
      "Iteration 1836, Average Reward: -124.26\n",
      "Iteration 1837, Average Reward: -121.38\n",
      "Iteration 1838, Average Reward: -115.28\n",
      "Iteration 1839, Average Reward: -124.62\n",
      "Iteration 1840, Average Reward: -139.16\n",
      "Iteration 1841, Average Reward: -145.49\n",
      "Iteration 1842, Average Reward: -129.35\n",
      "Iteration 1843, Average Reward: -126.83\n",
      "Iteration 1844, Average Reward: -129.45\n",
      "Iteration 1845, Average Reward: -123.75\n",
      "Iteration 1846, Average Reward: -117.45\n",
      "Iteration 1847, Average Reward: -110.75\n",
      "Iteration 1848, Average Reward: -116.3\n",
      "Iteration 1849, Average Reward: -119.4\n",
      "Iteration 1850, Average Reward: -124.67\n",
      "Iteration 1851, Average Reward: -130.47\n",
      "Iteration 1852, Average Reward: -116.69\n",
      "Iteration 1853, Average Reward: -115.51\n",
      "Iteration 1854, Average Reward: -132.52\n",
      "Iteration 1855, Average Reward: -134.67\n",
      "Iteration 1856, Average Reward: -126.48\n",
      "Iteration 1857, Average Reward: -115.38\n",
      "Iteration 1858, Average Reward: -120.81\n",
      "Iteration 1859, Average Reward: -127.15\n",
      "Iteration 1860, Average Reward: -119.72\n",
      "Iteration 1861, Average Reward: -107.05\n",
      "Iteration 1862, Average Reward: -94.44\n",
      "Iteration 1863, Average Reward: -106.77\n",
      "Iteration 1864, Average Reward: -127.83\n",
      "Iteration 1865, Average Reward: -136.84\n",
      "Iteration 1866, Average Reward: -131.35\n",
      "Iteration 1867, Average Reward: -123.96\n",
      "Iteration 1868, Average Reward: -117.4\n",
      "Iteration 1869, Average Reward: -102.48\n",
      "Iteration 1870, Average Reward: -96.29\n",
      "Iteration 1871, Average Reward: -105.8\n",
      "Iteration 1872, Average Reward: -109.83\n",
      "Iteration 1873, Average Reward: -110.44\n",
      "Iteration 1874, Average Reward: -104.12\n",
      "Iteration 1875, Average Reward: -99.53\n",
      "Iteration 1876, Average Reward: -105.86\n",
      "Iteration 1877, Average Reward: -147.79\n",
      "Iteration 1878, Average Reward: -140.96\n",
      "Iteration 1879, Average Reward: -125.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1880, Average Reward: -126.16\n",
      "Iteration 1881, Average Reward: -142.14\n",
      "Iteration 1882, Average Reward: -132.42\n",
      "Iteration 1883, Average Reward: -112.9\n",
      "Iteration 1884, Average Reward: -106.75\n",
      "Iteration 1885, Average Reward: -110.47\n",
      "Iteration 1886, Average Reward: -112.84\n",
      "Iteration 1887, Average Reward: -124.6\n",
      "Iteration 1888, Average Reward: -120.64\n",
      "Iteration 1889, Average Reward: -134.14\n",
      "Iteration 1890, Average Reward: -135.5\n",
      "Iteration 1891, Average Reward: -134.9\n",
      "Iteration 1892, Average Reward: -124.45\n",
      "Iteration 1893, Average Reward: -122.35\n",
      "Iteration 1894, Average Reward: -143.04\n",
      "Iteration 1895, Average Reward: -127.4\n",
      "Iteration 1896, Average Reward: -123.62\n",
      "Iteration 1897, Average Reward: -115.55\n",
      "Iteration 1898, Average Reward: -113.84\n",
      "Iteration 1899, Average Reward: -111.22\n",
      "Iteration 1900, Average Reward: -120.59\n",
      "Iteration 1901, Average Reward: -111.59\n",
      "Iteration 1902, Average Reward: -128.34\n",
      "Iteration 1903, Average Reward: -143.25\n",
      "Iteration 1904, Average Reward: -131.73\n",
      "Iteration 1905, Average Reward: -123.39\n",
      "Iteration 1906, Average Reward: -114.84\n",
      "Iteration 1907, Average Reward: -112.31\n",
      "Iteration 1908, Average Reward: -121.2\n",
      "Iteration 1909, Average Reward: -106.69\n",
      "Iteration 1910, Average Reward: -106.95\n",
      "Iteration 1911, Average Reward: -110.02\n",
      "Iteration 1912, Average Reward: -128.61\n",
      "Iteration 1913, Average Reward: -126.3\n",
      "Iteration 1914, Average Reward: -130.11\n",
      "Iteration 1915, Average Reward: -135.36\n",
      "Iteration 1916, Average Reward: -124.54\n",
      "Iteration 1917, Average Reward: -123.33\n",
      "Iteration 1918, Average Reward: -122.64\n",
      "Iteration 1919, Average Reward: -139.79\n",
      "Iteration 1920, Average Reward: -126.17\n",
      "Iteration 1921, Average Reward: -136.0\n",
      "Iteration 1922, Average Reward: -121.0\n",
      "Iteration 1923, Average Reward: -121.94\n",
      "Iteration 1924, Average Reward: -142.62\n",
      "Iteration 1925, Average Reward: -133.8\n",
      "Iteration 1926, Average Reward: -122.95\n",
      "Iteration 1927, Average Reward: -114.93\n",
      "Iteration 1928, Average Reward: -107.49\n",
      "Iteration 1929, Average Reward: -112.87\n",
      "Iteration 1930, Average Reward: -103.85\n",
      "Iteration 1931, Average Reward: -125.02\n",
      "Iteration 1932, Average Reward: -146.89\n",
      "Iteration 1933, Average Reward: -149.5\n",
      "Iteration 1934, Average Reward: -136.97\n",
      "Iteration 1935, Average Reward: -144.85\n",
      "Iteration 1936, Average Reward: -141.89\n",
      "Iteration 1937, Average Reward: -139.74\n",
      "Iteration 1938, Average Reward: -131.39\n",
      "Iteration 1939, Average Reward: -113.98\n",
      "Iteration 1940, Average Reward: -108.63\n",
      "Iteration 1941, Average Reward: -112.27\n",
      "Iteration 1942, Average Reward: -125.53\n",
      "Iteration 1943, Average Reward: -138.39\n",
      "Iteration 1944, Average Reward: -152.83\n",
      "Iteration 1945, Average Reward: -151.49\n",
      "Iteration 1946, Average Reward: -122.65\n",
      "Iteration 1947, Average Reward: -141.22\n",
      "Iteration 1948, Average Reward: -128.92\n",
      "Iteration 1949, Average Reward: -122.99\n",
      "Iteration 1950, Average Reward: -122.37\n",
      "Iteration 1951, Average Reward: -120.17\n",
      "Iteration 1952, Average Reward: -116.68\n",
      "Iteration 1953, Average Reward: -125.83\n",
      "Iteration 1954, Average Reward: -142.22\n",
      "Iteration 1955, Average Reward: -143.37\n",
      "Iteration 1956, Average Reward: -137.06\n",
      "Iteration 1957, Average Reward: -127.1\n",
      "Iteration 1958, Average Reward: -118.71\n",
      "Iteration 1959, Average Reward: -111.68\n",
      "Iteration 1960, Average Reward: -120.38\n",
      "Iteration 1961, Average Reward: -135.37\n",
      "Iteration 1962, Average Reward: -140.17\n",
      "Iteration 1963, Average Reward: -115.81\n",
      "Iteration 1964, Average Reward: -105.64\n",
      "Iteration 1965, Average Reward: -114.12\n",
      "Iteration 1966, Average Reward: -137.59\n",
      "Iteration 1967, Average Reward: -129.59\n",
      "Iteration 1968, Average Reward: -117.82\n",
      "Iteration 1969, Average Reward: -132.82\n",
      "Iteration 1970, Average Reward: -145.86\n",
      "Iteration 1971, Average Reward: -135.31\n",
      "Iteration 1972, Average Reward: -136.16\n",
      "Iteration 1973, Average Reward: -140.35\n",
      "Iteration 1974, Average Reward: -124.38\n",
      "Iteration 1975, Average Reward: -110.89\n",
      "Iteration 1976, Average Reward: -118.57\n",
      "Iteration 1977, Average Reward: -129.04\n",
      "Iteration 1978, Average Reward: -113.28\n",
      "Iteration 1979, Average Reward: -113.72\n",
      "Iteration 1980, Average Reward: -124.66\n",
      "Iteration 1981, Average Reward: -125.44\n",
      "Iteration 1982, Average Reward: -111.1\n",
      "Iteration 1983, Average Reward: -119.99\n",
      "Iteration 1984, Average Reward: -128.31\n",
      "Iteration 1985, Average Reward: -118.13\n",
      "Iteration 1986, Average Reward: -111.57\n",
      "Iteration 1987, Average Reward: -123.53\n",
      "Iteration 1988, Average Reward: -131.43\n",
      "Iteration 1989, Average Reward: -121.13\n",
      "Iteration 1990, Average Reward: -121.94\n",
      "Iteration 1991, Average Reward: -115.91\n",
      "Iteration 1992, Average Reward: -112.53\n",
      "Iteration 1993, Average Reward: -120.01\n",
      "Iteration 1994, Average Reward: -119.95\n",
      "Iteration 1995, Average Reward: -121.86\n",
      "Iteration 1996, Average Reward: -125.32\n",
      "Iteration 1997, Average Reward: -121.88\n",
      "Iteration 1998, Average Reward: -127.15\n",
      "Iteration 1999, Average Reward: -120.29\n",
      "Iteration 2000, Average Reward: -115.37\n",
      "Iteration 2001, Average Reward: -131.38\n",
      "Iteration 2002, Average Reward: -132.48\n",
      "Iteration 2003, Average Reward: -122.6\n",
      "Iteration 2004, Average Reward: -107.4\n",
      "Iteration 2005, Average Reward: -101.67\n",
      "Iteration 2006, Average Reward: -121.76\n",
      "Iteration 2007, Average Reward: -129.77\n",
      "Iteration 2008, Average Reward: -136.09\n",
      "Iteration 2009, Average Reward: -146.03\n",
      "Iteration 2010, Average Reward: -137.75\n",
      "Iteration 2011, Average Reward: -139.13\n",
      "Iteration 2012, Average Reward: -132.28\n",
      "Iteration 2013, Average Reward: -111.24\n",
      "Iteration 2014, Average Reward: -124.58\n",
      "Iteration 2015, Average Reward: -110.29\n",
      "Iteration 2016, Average Reward: -120.61\n",
      "Iteration 2017, Average Reward: -116.26\n",
      "Iteration 2018, Average Reward: -126.86\n",
      "Iteration 2019, Average Reward: -147.16\n",
      "Iteration 2020, Average Reward: -144.03\n",
      "Iteration 2021, Average Reward: -141.72\n",
      "Iteration 2022, Average Reward: -120.5\n",
      "Iteration 2023, Average Reward: -127.37\n",
      "Iteration 2024, Average Reward: -139.56\n",
      "Iteration 2025, Average Reward: -135.61\n",
      "Iteration 2026, Average Reward: -109.37\n",
      "Iteration 2027, Average Reward: -105.52\n",
      "Iteration 2028, Average Reward: -117.75\n",
      "Iteration 2029, Average Reward: -123.31\n",
      "Iteration 2030, Average Reward: -121.97\n",
      "Iteration 2031, Average Reward: -118.29\n",
      "Iteration 2032, Average Reward: -124.51\n",
      "Iteration 2033, Average Reward: -132.99\n",
      "Iteration 2034, Average Reward: -130.45\n",
      "Iteration 2035, Average Reward: -136.63\n",
      "Iteration 2036, Average Reward: -133.12\n",
      "Iteration 2037, Average Reward: -131.32\n",
      "Iteration 2038, Average Reward: -119.4\n",
      "Iteration 2039, Average Reward: -105.66\n",
      "Iteration 2040, Average Reward: -130.72\n",
      "Iteration 2041, Average Reward: -117.91\n",
      "Iteration 2042, Average Reward: -132.7\n",
      "Iteration 2043, Average Reward: -135.68\n",
      "Iteration 2044, Average Reward: -134.88\n",
      "Iteration 2045, Average Reward: -118.51\n",
      "Iteration 2046, Average Reward: -125.36\n",
      "Iteration 2047, Average Reward: -132.45\n",
      "Iteration 2048, Average Reward: -116.26\n",
      "Iteration 2049, Average Reward: -112.26\n",
      "Iteration 2050, Average Reward: -110.39\n",
      "Iteration 2051, Average Reward: -113.95\n",
      "Iteration 2052, Average Reward: -121.36\n",
      "Iteration 2053, Average Reward: -121.86\n",
      "Iteration 2054, Average Reward: -133.86\n",
      "Iteration 2055, Average Reward: -136.43\n",
      "Iteration 2056, Average Reward: -127.07\n",
      "Iteration 2057, Average Reward: -121.66\n",
      "Iteration 2058, Average Reward: -126.25\n",
      "Iteration 2059, Average Reward: -100.98\n",
      "Iteration 2060, Average Reward: -109.28\n",
      "Iteration 2061, Average Reward: -116.3\n",
      "Iteration 2062, Average Reward: -124.87\n",
      "Iteration 2063, Average Reward: -125.67\n",
      "Iteration 2064, Average Reward: -127.04\n",
      "Iteration 2065, Average Reward: -128.75\n",
      "Iteration 2066, Average Reward: -118.81\n",
      "Iteration 2067, Average Reward: -134.92\n",
      "Iteration 2068, Average Reward: -136.73\n",
      "Iteration 2069, Average Reward: -123.22\n",
      "Iteration 2070, Average Reward: -145.11\n",
      "Iteration 2071, Average Reward: -143.73\n",
      "Iteration 2072, Average Reward: -119.57\n",
      "Iteration 2073, Average Reward: -115.33\n",
      "Iteration 2074, Average Reward: -129.72\n",
      "Iteration 2075, Average Reward: -121.17\n",
      "Iteration 2076, Average Reward: -114.56\n",
      "Iteration 2077, Average Reward: -109.87\n",
      "Iteration 2078, Average Reward: -107.86\n",
      "Iteration 2079, Average Reward: -120.96\n",
      "Iteration 2080, Average Reward: -126.01\n",
      "Iteration 2081, Average Reward: -129.7\n",
      "Iteration 2082, Average Reward: -116.46\n",
      "Iteration 2083, Average Reward: -123.22\n",
      "Iteration 2084, Average Reward: -136.7\n",
      "Iteration 2085, Average Reward: -130.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2086, Average Reward: -115.01\n",
      "Iteration 2087, Average Reward: -115.14\n",
      "Iteration 2088, Average Reward: -114.69\n",
      "Iteration 2089, Average Reward: -119.66\n",
      "Iteration 2090, Average Reward: -134.8\n",
      "Iteration 2091, Average Reward: -133.51\n",
      "Iteration 2092, Average Reward: -132.09\n",
      "Iteration 2093, Average Reward: -127.3\n",
      "Iteration 2094, Average Reward: -126.28\n",
      "Iteration 2095, Average Reward: -113.74\n",
      "Iteration 2096, Average Reward: -119.8\n",
      "Iteration 2097, Average Reward: -125.75\n",
      "Iteration 2098, Average Reward: -126.18\n",
      "Iteration 2099, Average Reward: -121.93\n",
      "Iteration 2100, Average Reward: -131.83\n",
      "Iteration 2101, Average Reward: -120.83\n",
      "Iteration 2102, Average Reward: -119.37\n",
      "Iteration 2103, Average Reward: -125.45\n",
      "Iteration 2104, Average Reward: -126.2\n",
      "Iteration 2105, Average Reward: -126.8\n",
      "Iteration 2106, Average Reward: -130.96\n",
      "Iteration 2107, Average Reward: -109.31\n",
      "Iteration 2108, Average Reward: -117.74\n",
      "Iteration 2109, Average Reward: -128.0\n",
      "Iteration 2110, Average Reward: -116.47\n",
      "Iteration 2111, Average Reward: -107.62\n",
      "Iteration 2112, Average Reward: -117.44\n",
      "Iteration 2113, Average Reward: -115.19\n",
      "Iteration 2114, Average Reward: -129.7\n",
      "Iteration 2115, Average Reward: -134.23\n",
      "Iteration 2116, Average Reward: -122.89\n",
      "Iteration 2117, Average Reward: -111.32\n",
      "Iteration 2118, Average Reward: -114.42\n",
      "Iteration 2119, Average Reward: -111.45\n",
      "Iteration 2120, Average Reward: -130.83\n",
      "Iteration 2121, Average Reward: -125.17\n",
      "Iteration 2122, Average Reward: -135.77\n",
      "Iteration 2123, Average Reward: -128.24\n",
      "Iteration 2124, Average Reward: -135.91\n",
      "Iteration 2125, Average Reward: -113.68\n",
      "Iteration 2126, Average Reward: -130.36\n",
      "Iteration 2127, Average Reward: -124.78\n",
      "Iteration 2128, Average Reward: -124.08\n",
      "Iteration 2129, Average Reward: -112.83\n",
      "Iteration 2130, Average Reward: -124.25\n",
      "Iteration 2131, Average Reward: -135.29\n",
      "Iteration 2132, Average Reward: -127.11\n",
      "Iteration 2133, Average Reward: -131.39\n",
      "Iteration 2134, Average Reward: -125.79\n",
      "Iteration 2135, Average Reward: -124.97\n",
      "Iteration 2136, Average Reward: -130.27\n",
      "Iteration 2137, Average Reward: -126.4\n",
      "Iteration 2138, Average Reward: -117.95\n",
      "Iteration 2139, Average Reward: -124.19\n",
      "Iteration 2140, Average Reward: -124.79\n",
      "Iteration 2141, Average Reward: -117.0\n",
      "Iteration 2142, Average Reward: -113.0\n",
      "Iteration 2143, Average Reward: -113.12\n",
      "Iteration 2144, Average Reward: -121.28\n",
      "Iteration 2145, Average Reward: -124.26\n",
      "Iteration 2146, Average Reward: -126.76\n",
      "Iteration 2147, Average Reward: -134.93\n",
      "Iteration 2148, Average Reward: -138.75\n",
      "Iteration 2149, Average Reward: -138.71\n",
      "Iteration 2150, Average Reward: -122.11\n",
      "Iteration 2151, Average Reward: -117.2\n",
      "Iteration 2152, Average Reward: -113.46\n",
      "Iteration 2153, Average Reward: -124.45\n",
      "Iteration 2154, Average Reward: -131.85\n",
      "Iteration 2155, Average Reward: -131.43\n",
      "Iteration 2156, Average Reward: -142.56\n",
      "Iteration 2157, Average Reward: -127.47\n",
      "Iteration 2158, Average Reward: -120.43\n",
      "Iteration 2159, Average Reward: -126.03\n",
      "Iteration 2160, Average Reward: -110.88\n",
      "Iteration 2161, Average Reward: -100.43\n",
      "Iteration 2162, Average Reward: -111.99\n",
      "Iteration 2163, Average Reward: -121.55\n",
      "Iteration 2164, Average Reward: -113.33\n",
      "Iteration 2165, Average Reward: -110.32\n",
      "Iteration 2166, Average Reward: -120.84\n",
      "Iteration 2167, Average Reward: -123.98\n",
      "Iteration 2168, Average Reward: -123.79\n",
      "Iteration 2169, Average Reward: -130.4\n",
      "Iteration 2170, Average Reward: -129.15\n",
      "Iteration 2171, Average Reward: -128.03\n",
      "Iteration 2172, Average Reward: -114.06\n",
      "Iteration 2173, Average Reward: -126.18\n",
      "Iteration 2174, Average Reward: -119.22\n",
      "Iteration 2175, Average Reward: -125.23\n",
      "Iteration 2176, Average Reward: -126.79\n",
      "Iteration 2177, Average Reward: -118.21\n",
      "Iteration 2178, Average Reward: -109.14\n",
      "Iteration 2179, Average Reward: -114.88\n",
      "Iteration 2180, Average Reward: -123.32\n",
      "Iteration 2181, Average Reward: -122.35\n",
      "Iteration 2182, Average Reward: -146.93\n",
      "Iteration 2183, Average Reward: -142.71\n",
      "Iteration 2184, Average Reward: -133.68\n",
      "Iteration 2185, Average Reward: -139.36\n",
      "Iteration 2186, Average Reward: -125.86\n",
      "Iteration 2187, Average Reward: -118.95\n",
      "Iteration 2188, Average Reward: -119.27\n",
      "Iteration 2189, Average Reward: -116.88\n",
      "Iteration 2190, Average Reward: -106.55\n",
      "Iteration 2191, Average Reward: -119.81\n",
      "Iteration 2192, Average Reward: -122.43\n",
      "Iteration 2193, Average Reward: -115.6\n",
      "Iteration 2194, Average Reward: -120.22\n",
      "Iteration 2195, Average Reward: -126.26\n",
      "Iteration 2196, Average Reward: -127.91\n",
      "Iteration 2197, Average Reward: -125.11\n",
      "Iteration 2198, Average Reward: -142.15\n",
      "Iteration 2199, Average Reward: -143.15\n",
      "Iteration 2200, Average Reward: -124.08\n",
      "Iteration 2201, Average Reward: -117.65\n",
      "Iteration 2202, Average Reward: -102.74\n",
      "Iteration 2203, Average Reward: -117.87\n",
      "Iteration 2204, Average Reward: -113.77\n",
      "Iteration 2205, Average Reward: -100.76\n",
      "Iteration 2206, Average Reward: -117.17\n",
      "Iteration 2207, Average Reward: -119.52\n",
      "Iteration 2208, Average Reward: -121.92\n",
      "Iteration 2209, Average Reward: -131.05\n",
      "Iteration 2210, Average Reward: -147.3\n",
      "Iteration 2211, Average Reward: -139.69\n",
      "Iteration 2212, Average Reward: -128.8\n",
      "Iteration 2213, Average Reward: -119.14\n",
      "Iteration 2214, Average Reward: -113.98\n",
      "Iteration 2215, Average Reward: -108.31\n",
      "Iteration 2216, Average Reward: -115.31\n",
      "Iteration 2217, Average Reward: -107.89\n",
      "Iteration 2218, Average Reward: -109.76\n",
      "Iteration 2219, Average Reward: -114.34\n",
      "Iteration 2220, Average Reward: -123.59\n",
      "Iteration 2221, Average Reward: -129.79\n",
      "Iteration 2222, Average Reward: -132.01\n",
      "Iteration 2223, Average Reward: -116.03\n",
      "Iteration 2224, Average Reward: -126.76\n",
      "Iteration 2225, Average Reward: -126.8\n",
      "Iteration 2226, Average Reward: -121.57\n",
      "Iteration 2227, Average Reward: -126.83\n",
      "Iteration 2228, Average Reward: -138.48\n",
      "Iteration 2229, Average Reward: -128.39\n",
      "Iteration 2230, Average Reward: -106.66\n",
      "Iteration 2231, Average Reward: -109.45\n",
      "Iteration 2232, Average Reward: -124.76\n",
      "Iteration 2233, Average Reward: -126.39\n",
      "Iteration 2234, Average Reward: -130.49\n",
      "Iteration 2235, Average Reward: -137.75\n",
      "Iteration 2236, Average Reward: -122.67\n",
      "Iteration 2237, Average Reward: -120.18\n",
      "Iteration 2238, Average Reward: -126.37\n",
      "Iteration 2239, Average Reward: -121.0\n",
      "Iteration 2240, Average Reward: -119.31\n",
      "Iteration 2241, Average Reward: -110.5\n",
      "Iteration 2242, Average Reward: -124.36\n",
      "Iteration 2243, Average Reward: -123.49\n",
      "Iteration 2244, Average Reward: -113.36\n",
      "Iteration 2245, Average Reward: -112.77\n",
      "Iteration 2246, Average Reward: -128.77\n",
      "Iteration 2247, Average Reward: -130.02\n",
      "Iteration 2248, Average Reward: -123.16\n",
      "Iteration 2249, Average Reward: -133.93\n",
      "Iteration 2250, Average Reward: -128.49\n",
      "Iteration 2251, Average Reward: -118.79\n",
      "Iteration 2252, Average Reward: -112.57\n",
      "Iteration 2253, Average Reward: -116.86\n",
      "Iteration 2254, Average Reward: -124.74\n",
      "Iteration 2255, Average Reward: -140.59\n",
      "Iteration 2256, Average Reward: -125.56\n",
      "Iteration 2257, Average Reward: -106.22\n",
      "Iteration 2258, Average Reward: -96.65\n",
      "Iteration 2259, Average Reward: -104.15\n",
      "Iteration 2260, Average Reward: -122.76\n",
      "Iteration 2261, Average Reward: -142.33\n",
      "Iteration 2262, Average Reward: -151.12\n",
      "Iteration 2263, Average Reward: -136.13\n",
      "Iteration 2264, Average Reward: -126.05\n",
      "Iteration 2265, Average Reward: -119.46\n",
      "Iteration 2266, Average Reward: -139.56\n",
      "Iteration 2267, Average Reward: -134.77\n",
      "Iteration 2268, Average Reward: -137.92\n",
      "Iteration 2269, Average Reward: -143.11\n",
      "Iteration 2270, Average Reward: -132.83\n",
      "Iteration 2271, Average Reward: -113.53\n",
      "Iteration 2272, Average Reward: -148.69\n",
      "Iteration 2273, Average Reward: -149.95\n",
      "Iteration 2274, Average Reward: -135.6\n",
      "Iteration 2275, Average Reward: -131.96\n",
      "Iteration 2276, Average Reward: -117.61\n",
      "Iteration 2277, Average Reward: -111.08\n",
      "Iteration 2278, Average Reward: -116.66\n",
      "Iteration 2279, Average Reward: -129.75\n",
      "Iteration 2280, Average Reward: -133.01\n",
      "Iteration 2281, Average Reward: -115.37\n",
      "Iteration 2282, Average Reward: -123.7\n",
      "Iteration 2283, Average Reward: -126.85\n",
      "Iteration 2284, Average Reward: -121.2\n",
      "Iteration 2285, Average Reward: -134.3\n",
      "Iteration 2286, Average Reward: -130.75\n",
      "Iteration 2287, Average Reward: -108.21\n",
      "Iteration 2288, Average Reward: -108.21\n",
      "Iteration 2289, Average Reward: -131.18\n",
      "Iteration 2290, Average Reward: -123.63\n",
      "Iteration 2291, Average Reward: -150.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2292, Average Reward: -137.35\n",
      "Iteration 2293, Average Reward: -113.97\n",
      "Iteration 2294, Average Reward: -109.62\n",
      "Iteration 2295, Average Reward: -109.16\n",
      "Iteration 2296, Average Reward: -99.32\n",
      "Iteration 2297, Average Reward: -103.71\n",
      "Iteration 2298, Average Reward: -112.42\n",
      "Iteration 2299, Average Reward: -112.3\n",
      "Iteration 2300, Average Reward: -118.43\n",
      "Iteration 2301, Average Reward: -129.83\n",
      "Iteration 2302, Average Reward: -117.05\n",
      "Iteration 2303, Average Reward: -118.23\n",
      "Iteration 2304, Average Reward: -115.66\n",
      "Iteration 2305, Average Reward: -113.71\n",
      "Iteration 2306, Average Reward: -123.51\n",
      "Iteration 2307, Average Reward: -134.68\n",
      "Iteration 2308, Average Reward: -115.3\n",
      "Iteration 2309, Average Reward: -112.42\n",
      "Iteration 2310, Average Reward: -120.49\n",
      "Iteration 2311, Average Reward: -120.94\n",
      "Iteration 2312, Average Reward: -134.06\n",
      "Iteration 2313, Average Reward: -137.26\n",
      "Iteration 2314, Average Reward: -137.52\n",
      "Iteration 2315, Average Reward: -135.11\n",
      "Iteration 2316, Average Reward: -120.33\n",
      "Iteration 2317, Average Reward: -108.9\n",
      "Iteration 2318, Average Reward: -107.77\n",
      "Iteration 2319, Average Reward: -105.3\n",
      "Iteration 2320, Average Reward: -114.35\n",
      "Iteration 2321, Average Reward: -102.8\n",
      "Iteration 2322, Average Reward: -102.15\n",
      "Iteration 2323, Average Reward: -130.11\n",
      "Iteration 2324, Average Reward: -132.52\n",
      "Iteration 2325, Average Reward: -139.85\n",
      "Iteration 2326, Average Reward: -138.22\n",
      "Iteration 2327, Average Reward: -133.43\n",
      "Iteration 2328, Average Reward: -115.59\n",
      "Iteration 2329, Average Reward: -106.5\n",
      "Iteration 2330, Average Reward: -106.67\n",
      "Iteration 2331, Average Reward: -98.58\n",
      "Iteration 2332, Average Reward: -112.69\n",
      "Iteration 2333, Average Reward: -123.65\n",
      "Iteration 2334, Average Reward: -122.03\n",
      "Iteration 2335, Average Reward: -127.1\n",
      "Iteration 2336, Average Reward: -130.67\n",
      "Iteration 2337, Average Reward: -123.49\n",
      "Iteration 2338, Average Reward: -116.32\n",
      "Iteration 2339, Average Reward: -115.43\n",
      "Iteration 2340, Average Reward: -108.82\n",
      "Iteration 2341, Average Reward: -115.41\n",
      "Iteration 2342, Average Reward: -126.19\n",
      "Iteration 2343, Average Reward: -128.12\n",
      "Iteration 2344, Average Reward: -126.35\n",
      "Iteration 2345, Average Reward: -121.34\n",
      "Iteration 2346, Average Reward: -129.77\n",
      "Iteration 2347, Average Reward: -136.77\n",
      "Iteration 2348, Average Reward: -110.48\n",
      "Iteration 2349, Average Reward: -101.78\n",
      "Iteration 2350, Average Reward: -131.81\n",
      "Iteration 2351, Average Reward: -139.61\n",
      "Iteration 2352, Average Reward: -131.86\n",
      "Iteration 2353, Average Reward: -126.21\n",
      "Iteration 2354, Average Reward: -122.54\n",
      "Iteration 2355, Average Reward: -122.32\n",
      "Iteration 2356, Average Reward: -132.51\n",
      "Iteration 2357, Average Reward: -112.89\n",
      "Iteration 2358, Average Reward: -104.17\n",
      "Iteration 2359, Average Reward: -125.92\n",
      "Iteration 2360, Average Reward: -147.85\n",
      "Iteration 2361, Average Reward: -150.68\n",
      "Iteration 2362, Average Reward: -136.34\n",
      "Iteration 2363, Average Reward: -139.88\n",
      "Iteration 2364, Average Reward: -131.54\n",
      "Iteration 2365, Average Reward: -107.96\n",
      "Iteration 2366, Average Reward: -103.51\n",
      "Iteration 2367, Average Reward: -119.03\n",
      "Iteration 2368, Average Reward: -128.61\n",
      "Iteration 2369, Average Reward: -114.8\n",
      "Iteration 2370, Average Reward: -118.08\n",
      "Iteration 2371, Average Reward: -135.74\n",
      "Iteration 2372, Average Reward: -138.84\n",
      "Iteration 2373, Average Reward: -127.72\n",
      "Iteration 2374, Average Reward: -135.65\n",
      "Iteration 2375, Average Reward: -112.47\n",
      "Iteration 2376, Average Reward: -94.22\n",
      "Iteration 2377, Average Reward: -106.61\n",
      "Iteration 2378, Average Reward: -119.69\n",
      "Iteration 2379, Average Reward: -128.1\n",
      "Iteration 2380, Average Reward: -142.74\n",
      "Iteration 2381, Average Reward: -133.68\n",
      "Iteration 2382, Average Reward: -117.2\n",
      "Iteration 2383, Average Reward: -114.33\n",
      "Iteration 2384, Average Reward: -134.89\n",
      "Iteration 2385, Average Reward: -136.47\n",
      "Iteration 2386, Average Reward: -127.09\n",
      "Iteration 2387, Average Reward: -140.78\n",
      "Iteration 2388, Average Reward: -129.97\n",
      "Iteration 2389, Average Reward: -120.93\n",
      "Iteration 2390, Average Reward: -120.95\n",
      "Iteration 2391, Average Reward: -113.19\n",
      "Iteration 2392, Average Reward: -101.15\n",
      "Iteration 2393, Average Reward: -114.94\n",
      "Iteration 2394, Average Reward: -129.84\n",
      "Iteration 2395, Average Reward: -146.12\n",
      "Iteration 2396, Average Reward: -142.91\n",
      "Iteration 2397, Average Reward: -138.26\n",
      "Iteration 2398, Average Reward: -134.83\n",
      "Iteration 2399, Average Reward: -129.41\n",
      "Iteration 2400, Average Reward: -115.33\n",
      "Iteration 2401, Average Reward: -112.13\n",
      "Iteration 2402, Average Reward: -127.94\n",
      "Iteration 2403, Average Reward: -126.47\n",
      "Iteration 2404, Average Reward: -128.48\n",
      "Iteration 2405, Average Reward: -126.67\n",
      "Iteration 2406, Average Reward: -128.7\n",
      "Iteration 2407, Average Reward: -129.86\n",
      "Iteration 2408, Average Reward: -135.53\n",
      "Iteration 2409, Average Reward: -139.21\n",
      "Iteration 2410, Average Reward: -137.5\n",
      "Iteration 2411, Average Reward: -136.12\n",
      "Iteration 2412, Average Reward: -134.9\n",
      "Iteration 2413, Average Reward: -136.93\n",
      "Iteration 2414, Average Reward: -121.63\n",
      "Iteration 2415, Average Reward: -114.17\n",
      "Iteration 2416, Average Reward: -132.48\n",
      "Iteration 2417, Average Reward: -133.52\n",
      "Iteration 2418, Average Reward: -129.75\n",
      "Iteration 2419, Average Reward: -138.76\n",
      "Iteration 2420, Average Reward: -133.62\n",
      "Iteration 2421, Average Reward: -116.18\n",
      "Iteration 2422, Average Reward: -107.07\n",
      "Iteration 2423, Average Reward: -126.8\n",
      "Iteration 2424, Average Reward: -123.27\n",
      "Iteration 2425, Average Reward: -116.98\n",
      "Iteration 2426, Average Reward: -116.37\n",
      "Iteration 2427, Average Reward: -132.51\n",
      "Iteration 2428, Average Reward: -138.21\n",
      "Iteration 2429, Average Reward: -119.75\n",
      "Iteration 2430, Average Reward: -130.91\n",
      "Iteration 2431, Average Reward: -131.16\n",
      "Iteration 2432, Average Reward: -131.38\n",
      "Iteration 2433, Average Reward: -117.63\n",
      "Iteration 2434, Average Reward: -106.25\n",
      "Iteration 2435, Average Reward: -117.02\n",
      "Iteration 2436, Average Reward: -121.28\n",
      "Iteration 2437, Average Reward: -129.24\n",
      "Iteration 2438, Average Reward: -140.57\n",
      "Iteration 2439, Average Reward: -141.37\n",
      "Iteration 2440, Average Reward: -134.02\n",
      "Iteration 2441, Average Reward: -142.32\n",
      "Iteration 2442, Average Reward: -141.35\n",
      "Iteration 2443, Average Reward: -114.54\n",
      "Iteration 2444, Average Reward: -101.74\n",
      "Iteration 2445, Average Reward: -99.75\n",
      "Iteration 2446, Average Reward: -114.15\n",
      "Iteration 2447, Average Reward: -116.05\n",
      "Iteration 2448, Average Reward: -112.2\n",
      "Iteration 2449, Average Reward: -113.35\n",
      "Iteration 2450, Average Reward: -118.37\n",
      "Iteration 2451, Average Reward: -134.0\n",
      "Iteration 2452, Average Reward: -129.55\n",
      "Iteration 2453, Average Reward: -112.19\n",
      "Iteration 2454, Average Reward: -126.13\n",
      "Iteration 2455, Average Reward: -119.95\n",
      "Iteration 2456, Average Reward: -124.2\n",
      "Iteration 2457, Average Reward: -130.35\n",
      "Iteration 2458, Average Reward: -122.89\n",
      "Iteration 2459, Average Reward: -127.88\n",
      "Iteration 2460, Average Reward: -138.72\n",
      "Iteration 2461, Average Reward: -132.53\n",
      "Iteration 2462, Average Reward: -124.53\n",
      "Iteration 2463, Average Reward: -127.45\n",
      "Iteration 2464, Average Reward: -124.17\n",
      "Iteration 2465, Average Reward: -127.02\n",
      "Iteration 2466, Average Reward: -124.6\n",
      "Iteration 2467, Average Reward: -110.67\n",
      "Iteration 2468, Average Reward: -102.23\n",
      "Iteration 2469, Average Reward: -125.25\n",
      "Iteration 2470, Average Reward: -132.53\n",
      "Iteration 2471, Average Reward: -119.87\n",
      "Iteration 2472, Average Reward: -116.62\n",
      "Iteration 2473, Average Reward: -122.77\n",
      "Iteration 2474, Average Reward: -116.82\n",
      "Iteration 2475, Average Reward: -117.92\n",
      "Iteration 2476, Average Reward: -143.52\n",
      "Iteration 2477, Average Reward: -147.3\n",
      "Iteration 2478, Average Reward: -123.8\n",
      "Iteration 2479, Average Reward: -129.12\n",
      "Iteration 2480, Average Reward: -118.54\n",
      "Iteration 2481, Average Reward: -112.25\n",
      "Iteration 2482, Average Reward: -122.3\n",
      "Iteration 2483, Average Reward: -118.24\n",
      "Iteration 2484, Average Reward: -122.55\n",
      "Iteration 2485, Average Reward: -120.92\n",
      "Iteration 2486, Average Reward: -110.42\n",
      "Iteration 2487, Average Reward: -107.83\n",
      "Iteration 2488, Average Reward: -95.97\n",
      "Iteration 2489, Average Reward: -121.06\n",
      "Iteration 2490, Average Reward: -127.97\n",
      "Iteration 2491, Average Reward: -141.81\n",
      "Iteration 2492, Average Reward: -132.98\n",
      "Iteration 2493, Average Reward: -120.4\n",
      "Iteration 2494, Average Reward: -126.99\n",
      "Iteration 2495, Average Reward: -122.08\n",
      "Iteration 2496, Average Reward: -112.91\n",
      "Iteration 2497, Average Reward: -123.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2498, Average Reward: -141.84\n",
      "Iteration 2499, Average Reward: -147.65\n",
      "Iteration 2500, Average Reward: -128.63\n",
      "Iteration 2501, Average Reward: -129.31\n",
      "Iteration 2502, Average Reward: -129.51\n",
      "Iteration 2503, Average Reward: -124.47\n",
      "Iteration 2504, Average Reward: -122.7\n",
      "Iteration 2505, Average Reward: -124.66\n",
      "Iteration 2506, Average Reward: -125.24\n",
      "Iteration 2507, Average Reward: -130.1\n",
      "Iteration 2508, Average Reward: -122.92\n",
      "Iteration 2509, Average Reward: -129.77\n",
      "Iteration 2510, Average Reward: -132.81\n",
      "Iteration 2511, Average Reward: -134.5\n",
      "Iteration 2512, Average Reward: -129.74\n",
      "Iteration 2513, Average Reward: -114.65\n",
      "Iteration 2514, Average Reward: -112.17\n",
      "Iteration 2515, Average Reward: -127.74\n",
      "Iteration 2516, Average Reward: -128.38\n",
      "Iteration 2517, Average Reward: -119.1\n",
      "Iteration 2518, Average Reward: -124.85\n",
      "Iteration 2519, Average Reward: -124.16\n",
      "Iteration 2520, Average Reward: -130.83\n",
      "Iteration 2521, Average Reward: -130.53\n",
      "Iteration 2522, Average Reward: -127.78\n",
      "Iteration 2523, Average Reward: -124.27\n",
      "Iteration 2524, Average Reward: -124.24\n",
      "Iteration 2525, Average Reward: -113.6\n",
      "Iteration 2526, Average Reward: -114.75\n",
      "Iteration 2527, Average Reward: -128.7\n",
      "Iteration 2528, Average Reward: -107.95\n",
      "Iteration 2529, Average Reward: -88.19\n",
      "Iteration 2530, Average Reward: -119.06\n",
      "Iteration 2531, Average Reward: -141.12\n",
      "Iteration 2532, Average Reward: -134.34\n",
      "Iteration 2533, Average Reward: -138.78\n",
      "Iteration 2534, Average Reward: -138.03\n",
      "Iteration 2535, Average Reward: -132.23\n",
      "Iteration 2536, Average Reward: -110.61\n",
      "Iteration 2537, Average Reward: -107.32\n",
      "Iteration 2538, Average Reward: -118.28\n",
      "Iteration 2539, Average Reward: -123.54\n",
      "Iteration 2540, Average Reward: -137.08\n",
      "Iteration 2541, Average Reward: -117.95\n",
      "Iteration 2542, Average Reward: -131.03\n",
      "Iteration 2543, Average Reward: -134.71\n",
      "Iteration 2544, Average Reward: -140.77\n",
      "Iteration 2545, Average Reward: -142.03\n",
      "Iteration 2546, Average Reward: -130.3\n",
      "Iteration 2547, Average Reward: -141.26\n",
      "Iteration 2548, Average Reward: -127.81\n",
      "Iteration 2549, Average Reward: -135.47\n",
      "Iteration 2550, Average Reward: -120.69\n",
      "Iteration 2551, Average Reward: -116.25\n",
      "Iteration 2552, Average Reward: -112.8\n",
      "Iteration 2553, Average Reward: -128.19\n",
      "Iteration 2554, Average Reward: -123.32\n",
      "Iteration 2555, Average Reward: -111.42\n",
      "Iteration 2556, Average Reward: -125.36\n",
      "Iteration 2557, Average Reward: -125.81\n",
      "Iteration 2558, Average Reward: -134.02\n",
      "Iteration 2559, Average Reward: -139.28\n",
      "Iteration 2560, Average Reward: -128.56\n",
      "Iteration 2561, Average Reward: -128.71\n",
      "Iteration 2562, Average Reward: -115.99\n",
      "Iteration 2563, Average Reward: -116.63\n",
      "Iteration 2564, Average Reward: -98.12\n",
      "Iteration 2565, Average Reward: -120.18\n",
      "Iteration 2566, Average Reward: -126.14\n",
      "Iteration 2567, Average Reward: -138.13\n",
      "Iteration 2568, Average Reward: -135.73\n",
      "Iteration 2569, Average Reward: -116.98\n",
      "Iteration 2570, Average Reward: -140.29\n",
      "Iteration 2571, Average Reward: -129.25\n",
      "Iteration 2572, Average Reward: -104.52\n",
      "Iteration 2573, Average Reward: -99.87\n",
      "Iteration 2574, Average Reward: -108.57\n",
      "Iteration 2575, Average Reward: -137.74\n",
      "Iteration 2576, Average Reward: -140.44\n",
      "Iteration 2577, Average Reward: -132.7\n",
      "Iteration 2578, Average Reward: -129.22\n",
      "Iteration 2579, Average Reward: -135.95\n",
      "Iteration 2580, Average Reward: -128.61\n",
      "Iteration 2581, Average Reward: -120.8\n",
      "Iteration 2582, Average Reward: -127.02\n",
      "Iteration 2583, Average Reward: -121.56\n",
      "Iteration 2584, Average Reward: -129.7\n",
      "Iteration 2585, Average Reward: -131.78\n",
      "Iteration 2586, Average Reward: -136.43\n",
      "Iteration 2587, Average Reward: -138.8\n",
      "Iteration 2588, Average Reward: -122.88\n",
      "Iteration 2589, Average Reward: -127.33\n",
      "Iteration 2590, Average Reward: -135.19\n",
      "Iteration 2591, Average Reward: -122.6\n",
      "Iteration 2592, Average Reward: -119.8\n",
      "Iteration 2593, Average Reward: -129.44\n",
      "Iteration 2594, Average Reward: -130.39\n",
      "Iteration 2595, Average Reward: -121.45\n",
      "Iteration 2596, Average Reward: -112.77\n",
      "Iteration 2597, Average Reward: -134.3\n",
      "Iteration 2598, Average Reward: -116.38\n",
      "Iteration 2599, Average Reward: -94.43\n",
      "Iteration 2600, Average Reward: -115.14\n",
      "Iteration 2601, Average Reward: -126.49\n",
      "Iteration 2602, Average Reward: -117.96\n",
      "Iteration 2603, Average Reward: -119.28\n",
      "Iteration 2604, Average Reward: -124.73\n",
      "Iteration 2605, Average Reward: -130.35\n",
      "Iteration 2606, Average Reward: -114.41\n",
      "Iteration 2607, Average Reward: -124.61\n",
      "Iteration 2608, Average Reward: -130.54\n",
      "Iteration 2609, Average Reward: -124.99\n",
      "Iteration 2610, Average Reward: -120.55\n",
      "Iteration 2611, Average Reward: -117.28\n",
      "Iteration 2612, Average Reward: -125.56\n",
      "Iteration 2613, Average Reward: -126.25\n",
      "Iteration 2614, Average Reward: -110.21\n",
      "Iteration 2615, Average Reward: -115.64\n",
      "Iteration 2616, Average Reward: -112.88\n",
      "Iteration 2617, Average Reward: -107.86\n",
      "Iteration 2618, Average Reward: -120.93\n",
      "Iteration 2619, Average Reward: -127.63\n",
      "Iteration 2620, Average Reward: -119.39\n",
      "Iteration 2621, Average Reward: -106.11\n",
      "Iteration 2622, Average Reward: -113.15\n",
      "Iteration 2623, Average Reward: -121.67\n",
      "Iteration 2624, Average Reward: -128.95\n",
      "Iteration 2625, Average Reward: -134.2\n",
      "Iteration 2626, Average Reward: -125.94\n",
      "Iteration 2627, Average Reward: -119.44\n",
      "Iteration 2628, Average Reward: -122.81\n",
      "Iteration 2629, Average Reward: -114.55\n",
      "Iteration 2630, Average Reward: -136.78\n",
      "Iteration 2631, Average Reward: -143.95\n",
      "Iteration 2632, Average Reward: -130.49\n",
      "Iteration 2633, Average Reward: -125.57\n",
      "Iteration 2634, Average Reward: -129.65\n",
      "Iteration 2635, Average Reward: -122.12\n",
      "Iteration 2636, Average Reward: -123.99\n",
      "Iteration 2637, Average Reward: -126.49\n",
      "Iteration 2638, Average Reward: -134.42\n",
      "Iteration 2639, Average Reward: -130.72\n",
      "Iteration 2640, Average Reward: -126.31\n",
      "Iteration 2641, Average Reward: -120.8\n",
      "Iteration 2642, Average Reward: -123.64\n",
      "Iteration 2643, Average Reward: -114.2\n",
      "Iteration 2644, Average Reward: -115.66\n",
      "Iteration 2645, Average Reward: -120.59\n",
      "Iteration 2646, Average Reward: -133.2\n",
      "Iteration 2647, Average Reward: -144.1\n",
      "Iteration 2648, Average Reward: -129.49\n",
      "Iteration 2649, Average Reward: -116.52\n",
      "Iteration 2650, Average Reward: -119.09\n",
      "Iteration 2651, Average Reward: -127.22\n",
      "Iteration 2652, Average Reward: -116.99\n",
      "Iteration 2653, Average Reward: -109.07\n",
      "Iteration 2654, Average Reward: -115.63\n",
      "Iteration 2655, Average Reward: -139.88\n",
      "Iteration 2656, Average Reward: -140.95\n",
      "Iteration 2657, Average Reward: -127.77\n",
      "Iteration 2658, Average Reward: -125.81\n",
      "Iteration 2659, Average Reward: -129.65\n",
      "Iteration 2660, Average Reward: -127.23\n",
      "Iteration 2661, Average Reward: -116.62\n",
      "Iteration 2662, Average Reward: -142.47\n",
      "Iteration 2663, Average Reward: -129.07\n",
      "Iteration 2664, Average Reward: -129.33\n",
      "Iteration 2665, Average Reward: -141.9\n",
      "Iteration 2666, Average Reward: -131.86\n",
      "Iteration 2667, Average Reward: -125.71\n",
      "Iteration 2668, Average Reward: -121.12\n",
      "Iteration 2669, Average Reward: -121.04\n",
      "Iteration 2670, Average Reward: -127.88\n",
      "Iteration 2671, Average Reward: -126.4\n",
      "Iteration 2672, Average Reward: -116.58\n",
      "Iteration 2673, Average Reward: -136.61\n",
      "Iteration 2674, Average Reward: -140.2\n",
      "Iteration 2675, Average Reward: -128.07\n",
      "Iteration 2676, Average Reward: -118.83\n",
      "Iteration 2677, Average Reward: -139.06\n",
      "Iteration 2678, Average Reward: -128.91\n",
      "Iteration 2679, Average Reward: -110.49\n",
      "Iteration 2680, Average Reward: -116.56\n",
      "Iteration 2681, Average Reward: -109.35\n",
      "Iteration 2682, Average Reward: -121.15\n",
      "Iteration 2683, Average Reward: -134.62\n",
      "Iteration 2684, Average Reward: -140.71\n",
      "Iteration 2685, Average Reward: -124.82\n",
      "Iteration 2686, Average Reward: -131.29\n",
      "Iteration 2687, Average Reward: -150.24\n",
      "Iteration 2688, Average Reward: -120.38\n",
      "Iteration 2689, Average Reward: -105.46\n",
      "Iteration 2690, Average Reward: -102.88\n",
      "Iteration 2691, Average Reward: -104.73\n",
      "Iteration 2692, Average Reward: -106.56\n",
      "Iteration 2693, Average Reward: -107.04\n",
      "Iteration 2694, Average Reward: -109.77\n",
      "Iteration 2695, Average Reward: -114.64\n",
      "Iteration 2696, Average Reward: -100.27\n",
      "Iteration 2697, Average Reward: -119.21\n",
      "Iteration 2698, Average Reward: -123.12\n",
      "Iteration 2699, Average Reward: -133.8\n",
      "Iteration 2700, Average Reward: -132.59\n",
      "Iteration 2701, Average Reward: -129.92\n",
      "Iteration 2702, Average Reward: -122.28\n",
      "Iteration 2703, Average Reward: -116.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2704, Average Reward: -114.55\n",
      "Iteration 2705, Average Reward: -105.61\n",
      "Iteration 2706, Average Reward: -106.13\n",
      "Iteration 2707, Average Reward: -109.17\n",
      "Iteration 2708, Average Reward: -114.94\n",
      "Iteration 2709, Average Reward: -125.75\n",
      "Iteration 2710, Average Reward: -127.99\n",
      "Iteration 2711, Average Reward: -147.31\n",
      "Iteration 2712, Average Reward: -148.35\n",
      "Iteration 2713, Average Reward: -130.83\n",
      "Iteration 2714, Average Reward: -128.8\n",
      "Iteration 2715, Average Reward: -128.17\n",
      "Iteration 2716, Average Reward: -121.85\n",
      "Iteration 2717, Average Reward: -108.36\n",
      "Iteration 2718, Average Reward: -109.18\n",
      "Iteration 2719, Average Reward: -123.89\n",
      "Iteration 2720, Average Reward: -144.98\n",
      "Iteration 2721, Average Reward: -137.87\n",
      "Iteration 2722, Average Reward: -134.07\n",
      "Iteration 2723, Average Reward: -130.4\n",
      "Iteration 2724, Average Reward: -136.42\n",
      "Iteration 2725, Average Reward: -114.7\n",
      "Iteration 2726, Average Reward: -107.6\n",
      "Iteration 2727, Average Reward: -127.3\n",
      "Iteration 2728, Average Reward: -123.43\n",
      "Iteration 2729, Average Reward: -125.67\n",
      "Iteration 2730, Average Reward: -130.21\n",
      "Iteration 2731, Average Reward: -144.44\n",
      "Iteration 2732, Average Reward: -118.71\n",
      "Iteration 2733, Average Reward: -101.75\n",
      "Iteration 2734, Average Reward: -101.4\n",
      "Iteration 2735, Average Reward: -115.75\n",
      "Iteration 2736, Average Reward: -102.5\n",
      "Iteration 2737, Average Reward: -106.58\n",
      "Iteration 2738, Average Reward: -117.26\n",
      "Iteration 2739, Average Reward: -110.55\n",
      "Iteration 2740, Average Reward: -112.97\n",
      "Iteration 2741, Average Reward: -121.79\n",
      "Iteration 2742, Average Reward: -125.47\n",
      "Iteration 2743, Average Reward: -129.87\n",
      "Iteration 2744, Average Reward: -118.15\n",
      "Iteration 2745, Average Reward: -103.09\n",
      "Iteration 2746, Average Reward: -120.1\n",
      "Iteration 2747, Average Reward: -114.81\n",
      "Iteration 2748, Average Reward: -130.15\n",
      "Iteration 2749, Average Reward: -143.68\n",
      "Iteration 2750, Average Reward: -145.59\n",
      "Iteration 2751, Average Reward: -137.97\n",
      "Iteration 2752, Average Reward: -135.82\n",
      "Iteration 2753, Average Reward: -130.05\n",
      "Iteration 2754, Average Reward: -107.37\n",
      "Iteration 2755, Average Reward: -113.15\n",
      "Iteration 2756, Average Reward: -125.23\n",
      "Iteration 2757, Average Reward: -116.77\n",
      "Iteration 2758, Average Reward: -127.16\n",
      "Iteration 2759, Average Reward: -134.75\n",
      "Iteration 2760, Average Reward: -135.86\n",
      "Iteration 2761, Average Reward: -129.03\n",
      "Iteration 2762, Average Reward: -148.39\n",
      "Iteration 2763, Average Reward: -138.4\n",
      "Iteration 2764, Average Reward: -133.58\n",
      "Iteration 2765, Average Reward: -124.78\n",
      "Iteration 2766, Average Reward: -120.71\n",
      "Iteration 2767, Average Reward: -130.71\n",
      "Iteration 2768, Average Reward: -131.96\n",
      "Iteration 2769, Average Reward: -119.67\n",
      "Iteration 2770, Average Reward: -95.21\n",
      "Iteration 2771, Average Reward: -89.78\n",
      "Iteration 2772, Average Reward: -110.8\n",
      "Iteration 2773, Average Reward: -129.33\n",
      "Iteration 2774, Average Reward: -126.92\n",
      "Iteration 2775, Average Reward: -123.56\n",
      "Iteration 2776, Average Reward: -112.42\n",
      "Iteration 2777, Average Reward: -115.06\n",
      "Iteration 2778, Average Reward: -124.11\n",
      "Iteration 2779, Average Reward: -118.2\n",
      "Iteration 2780, Average Reward: -123.57\n",
      "Iteration 2781, Average Reward: -101.47\n",
      "Iteration 2782, Average Reward: -123.0\n",
      "Iteration 2783, Average Reward: -121.89\n",
      "Iteration 2784, Average Reward: -130.55\n",
      "Iteration 2785, Average Reward: -129.54\n",
      "Iteration 2786, Average Reward: -132.64\n",
      "Iteration 2787, Average Reward: -107.1\n",
      "Iteration 2788, Average Reward: -111.37\n",
      "Iteration 2789, Average Reward: -137.01\n",
      "Iteration 2790, Average Reward: -140.76\n",
      "Iteration 2791, Average Reward: -109.85\n",
      "Iteration 2792, Average Reward: -120.94\n",
      "Iteration 2793, Average Reward: -121.35\n",
      "Iteration 2794, Average Reward: -115.25\n",
      "Iteration 2795, Average Reward: -125.87\n",
      "Iteration 2796, Average Reward: -131.43\n",
      "Iteration 2797, Average Reward: -147.73\n",
      "Iteration 2798, Average Reward: -144.17\n",
      "Iteration 2799, Average Reward: -126.38\n",
      "Iteration 2800, Average Reward: -116.14\n",
      "Iteration 2801, Average Reward: -126.25\n",
      "Iteration 2802, Average Reward: -125.55\n",
      "Iteration 2803, Average Reward: -123.32\n",
      "Iteration 2804, Average Reward: -121.17\n",
      "Iteration 2805, Average Reward: -109.9\n",
      "Iteration 2806, Average Reward: -126.34\n",
      "Iteration 2807, Average Reward: -116.01\n",
      "Iteration 2808, Average Reward: -122.3\n",
      "Iteration 2809, Average Reward: -116.65\n",
      "Iteration 2810, Average Reward: -107.57\n",
      "Iteration 2811, Average Reward: -106.77\n",
      "Iteration 2812, Average Reward: -111.67\n",
      "Iteration 2813, Average Reward: -123.61\n",
      "Iteration 2814, Average Reward: -127.64\n",
      "Iteration 2815, Average Reward: -130.61\n",
      "Iteration 2816, Average Reward: -135.67\n",
      "Iteration 2817, Average Reward: -127.28\n",
      "Iteration 2818, Average Reward: -109.46\n",
      "Iteration 2819, Average Reward: -112.02\n",
      "Iteration 2820, Average Reward: -110.62\n",
      "Iteration 2821, Average Reward: -108.82\n",
      "Iteration 2822, Average Reward: -114.64\n",
      "Iteration 2823, Average Reward: -114.76\n",
      "Iteration 2824, Average Reward: -132.78\n",
      "Iteration 2825, Average Reward: -135.91\n",
      "Iteration 2826, Average Reward: -133.85\n",
      "Iteration 2827, Average Reward: -134.27\n",
      "Iteration 2828, Average Reward: -116.17\n",
      "Iteration 2829, Average Reward: -102.61\n",
      "Iteration 2830, Average Reward: -95.77\n",
      "Iteration 2831, Average Reward: -98.22\n",
      "Iteration 2832, Average Reward: -110.3\n",
      "Iteration 2833, Average Reward: -143.84\n",
      "Iteration 2834, Average Reward: -132.13\n",
      "Iteration 2835, Average Reward: -127.25\n",
      "Iteration 2836, Average Reward: -120.81\n",
      "Iteration 2837, Average Reward: -128.14\n",
      "Iteration 2838, Average Reward: -115.08\n",
      "Iteration 2839, Average Reward: -117.8\n",
      "Iteration 2840, Average Reward: -112.57\n",
      "Iteration 2841, Average Reward: -116.91\n",
      "Iteration 2842, Average Reward: -112.87\n",
      "Iteration 2843, Average Reward: -111.25\n",
      "Iteration 2844, Average Reward: -112.9\n",
      "Iteration 2845, Average Reward: -120.21\n",
      "Iteration 2846, Average Reward: -136.89\n",
      "Iteration 2847, Average Reward: -126.38\n",
      "Iteration 2848, Average Reward: -118.22\n",
      "Iteration 2849, Average Reward: -105.95\n",
      "Iteration 2850, Average Reward: -120.16\n",
      "Iteration 2851, Average Reward: -119.88\n",
      "Iteration 2852, Average Reward: -102.57\n",
      "Iteration 2853, Average Reward: -115.98\n",
      "Iteration 2854, Average Reward: -131.45\n",
      "Iteration 2855, Average Reward: -142.22\n",
      "Iteration 2856, Average Reward: -125.81\n",
      "Iteration 2857, Average Reward: -130.68\n",
      "Iteration 2858, Average Reward: -137.5\n",
      "Iteration 2859, Average Reward: -133.85\n",
      "Iteration 2860, Average Reward: -113.24\n",
      "Iteration 2861, Average Reward: -115.13\n",
      "Iteration 2862, Average Reward: -114.68\n",
      "Iteration 2863, Average Reward: -107.77\n",
      "Iteration 2864, Average Reward: -92.8\n",
      "Iteration 2865, Average Reward: -113.33\n",
      "Iteration 2866, Average Reward: -134.54\n",
      "Iteration 2867, Average Reward: -130.43\n",
      "Iteration 2868, Average Reward: -128.79\n",
      "Iteration 2869, Average Reward: -129.84\n",
      "Iteration 2870, Average Reward: -130.36\n",
      "Iteration 2871, Average Reward: -121.75\n",
      "Iteration 2872, Average Reward: -135.56\n",
      "Iteration 2873, Average Reward: -130.76\n",
      "Iteration 2874, Average Reward: -122.51\n",
      "Iteration 2875, Average Reward: -118.31\n",
      "Iteration 2876, Average Reward: -126.57\n",
      "Iteration 2877, Average Reward: -136.53\n",
      "Iteration 2878, Average Reward: -129.36\n",
      "Iteration 2879, Average Reward: -124.86\n",
      "Iteration 2880, Average Reward: -112.87\n",
      "Iteration 2881, Average Reward: -128.43\n",
      "Iteration 2882, Average Reward: -134.26\n",
      "Iteration 2883, Average Reward: -139.05\n",
      "Iteration 2884, Average Reward: -130.24\n",
      "Iteration 2885, Average Reward: -138.48\n",
      "Iteration 2886, Average Reward: -127.08\n",
      "Iteration 2887, Average Reward: -117.43\n",
      "Iteration 2888, Average Reward: -121.45\n",
      "Iteration 2889, Average Reward: -133.8\n",
      "Iteration 2890, Average Reward: -128.53\n",
      "Iteration 2891, Average Reward: -109.64\n",
      "Iteration 2892, Average Reward: -112.77\n",
      "Iteration 2893, Average Reward: -121.11\n",
      "Iteration 2894, Average Reward: -113.6\n",
      "Iteration 2895, Average Reward: -133.46\n",
      "Iteration 2896, Average Reward: -134.89\n",
      "Iteration 2897, Average Reward: -142.94\n",
      "Iteration 2898, Average Reward: -139.18\n",
      "Iteration 2899, Average Reward: -124.86\n",
      "Iteration 2900, Average Reward: -131.55\n",
      "Iteration 2901, Average Reward: -134.76\n",
      "Iteration 2902, Average Reward: -125.03\n",
      "Iteration 2903, Average Reward: -113.01\n",
      "Iteration 2904, Average Reward: -107.48\n",
      "Iteration 2905, Average Reward: -129.06\n",
      "Iteration 2906, Average Reward: -130.62\n",
      "Iteration 2907, Average Reward: -130.75\n",
      "Iteration 2908, Average Reward: -127.39\n",
      "Iteration 2909, Average Reward: -130.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2910, Average Reward: -122.74\n",
      "Iteration 2911, Average Reward: -115.75\n",
      "Iteration 2912, Average Reward: -126.07\n",
      "Iteration 2913, Average Reward: -123.67\n",
      "Iteration 2914, Average Reward: -123.38\n",
      "Iteration 2915, Average Reward: -119.97\n",
      "Iteration 2916, Average Reward: -128.68\n",
      "Iteration 2917, Average Reward: -137.61\n",
      "Iteration 2918, Average Reward: -131.98\n",
      "Iteration 2919, Average Reward: -139.44\n",
      "Iteration 2920, Average Reward: -143.83\n",
      "Iteration 2921, Average Reward: -130.71\n",
      "Iteration 2922, Average Reward: -116.6\n",
      "Iteration 2923, Average Reward: -107.87\n",
      "Iteration 2924, Average Reward: -109.64\n",
      "Iteration 2925, Average Reward: -113.57\n",
      "Iteration 2926, Average Reward: -134.54\n",
      "Iteration 2927, Average Reward: -138.1\n",
      "Iteration 2928, Average Reward: -131.18\n",
      "Iteration 2929, Average Reward: -122.68\n",
      "Iteration 2930, Average Reward: -128.63\n",
      "Iteration 2931, Average Reward: -134.91\n",
      "Iteration 2932, Average Reward: -140.61\n",
      "Iteration 2933, Average Reward: -129.13\n",
      "Iteration 2934, Average Reward: -130.9\n",
      "Iteration 2935, Average Reward: -140.9\n",
      "Iteration 2936, Average Reward: -132.38\n",
      "Iteration 2937, Average Reward: -147.01\n",
      "Iteration 2938, Average Reward: -130.0\n",
      "Iteration 2939, Average Reward: -128.97\n",
      "Iteration 2940, Average Reward: -130.69\n",
      "Iteration 2941, Average Reward: -123.99\n",
      "Iteration 2942, Average Reward: -125.82\n",
      "Iteration 2943, Average Reward: -134.63\n",
      "Iteration 2944, Average Reward: -136.66\n",
      "Iteration 2945, Average Reward: -157.19\n",
      "Iteration 2946, Average Reward: -137.18\n",
      "Iteration 2947, Average Reward: -118.07\n",
      "Iteration 2948, Average Reward: -120.6\n",
      "Iteration 2949, Average Reward: -117.38\n",
      "Iteration 2950, Average Reward: -127.64\n",
      "Iteration 2951, Average Reward: -120.03\n",
      "Iteration 2952, Average Reward: -110.43\n",
      "Iteration 2953, Average Reward: -119.58\n",
      "Iteration 2954, Average Reward: -118.6\n",
      "Iteration 2955, Average Reward: -114.98\n",
      "Iteration 2956, Average Reward: -120.57\n",
      "Iteration 2957, Average Reward: -119.55\n",
      "Iteration 2958, Average Reward: -127.06\n",
      "Iteration 2959, Average Reward: -127.94\n",
      "Iteration 2960, Average Reward: -121.13\n",
      "Iteration 2961, Average Reward: -130.66\n",
      "Iteration 2962, Average Reward: -132.63\n",
      "Iteration 2963, Average Reward: -136.27\n",
      "Iteration 2964, Average Reward: -138.72\n",
      "Iteration 2965, Average Reward: -134.7\n",
      "Iteration 2966, Average Reward: -128.28\n",
      "Iteration 2967, Average Reward: -116.42\n",
      "Iteration 2968, Average Reward: -116.77\n",
      "Iteration 2969, Average Reward: -107.12\n",
      "Iteration 2970, Average Reward: -112.28\n",
      "Iteration 2971, Average Reward: -120.23\n",
      "Iteration 2972, Average Reward: -142.71\n",
      "Iteration 2973, Average Reward: -151.64\n",
      "Iteration 2974, Average Reward: -147.44\n",
      "Iteration 2975, Average Reward: -129.99\n",
      "Iteration 2976, Average Reward: -117.95\n",
      "Iteration 2977, Average Reward: -108.87\n",
      "Iteration 2978, Average Reward: -108.88\n",
      "Iteration 2979, Average Reward: -122.29\n",
      "Iteration 2980, Average Reward: -126.44\n",
      "Iteration 2981, Average Reward: -128.85\n",
      "Iteration 2982, Average Reward: -122.15\n",
      "Iteration 2983, Average Reward: -114.27\n",
      "Iteration 2984, Average Reward: -121.39\n",
      "Iteration 2985, Average Reward: -103.43\n",
      "Iteration 2986, Average Reward: -89.57\n",
      "Iteration 2987, Average Reward: -118.73\n",
      "Iteration 2988, Average Reward: -124.93\n",
      "Iteration 2989, Average Reward: -124.69\n",
      "Iteration 2990, Average Reward: -125.05\n",
      "Iteration 2991, Average Reward: -127.05\n",
      "Iteration 2992, Average Reward: -121.75\n",
      "Iteration 2993, Average Reward: -113.52\n",
      "Iteration 2994, Average Reward: -126.7\n",
      "Iteration 2995, Average Reward: -125.26\n",
      "Iteration 2996, Average Reward: -116.63\n",
      "Iteration 2997, Average Reward: -126.47\n",
      "Iteration 2998, Average Reward: -133.8\n",
      "Iteration 2999, Average Reward: -136.42\n",
      "Iteration 3000, Average Reward: -114.83\n",
      "Iteration 3001, Average Reward: -114.6\n",
      "Iteration 3002, Average Reward: -114.0\n",
      "Iteration 3003, Average Reward: -116.36\n",
      "Iteration 3004, Average Reward: -116.54\n",
      "Iteration 3005, Average Reward: -129.68\n",
      "Iteration 3006, Average Reward: -128.38\n",
      "Iteration 3007, Average Reward: -116.32\n",
      "Iteration 3008, Average Reward: -122.26\n",
      "Iteration 3009, Average Reward: -109.96\n",
      "Iteration 3010, Average Reward: -113.73\n",
      "Iteration 3011, Average Reward: -127.35\n",
      "Iteration 3012, Average Reward: -133.67\n",
      "Iteration 3013, Average Reward: -125.17\n",
      "Iteration 3014, Average Reward: -118.56\n",
      "Iteration 3015, Average Reward: -139.31\n",
      "Iteration 3016, Average Reward: -144.73\n",
      "Iteration 3017, Average Reward: -135.92\n",
      "Iteration 3018, Average Reward: -137.92\n",
      "Iteration 3019, Average Reward: -126.75\n",
      "Iteration 3020, Average Reward: -113.35\n",
      "Iteration 3021, Average Reward: -122.24\n",
      "Iteration 3022, Average Reward: -114.42\n",
      "Iteration 3023, Average Reward: -114.57\n",
      "Iteration 3024, Average Reward: -119.74\n",
      "Iteration 3025, Average Reward: -130.93\n",
      "Iteration 3026, Average Reward: -122.88\n",
      "Iteration 3027, Average Reward: -133.54\n",
      "Iteration 3028, Average Reward: -143.34\n",
      "Iteration 3029, Average Reward: -150.01\n",
      "Iteration 3030, Average Reward: -114.95\n",
      "Iteration 3031, Average Reward: -118.52\n",
      "Iteration 3032, Average Reward: -103.03\n",
      "Iteration 3033, Average Reward: -107.33\n",
      "Iteration 3034, Average Reward: -107.95\n",
      "Iteration 3035, Average Reward: -128.91\n",
      "Iteration 3036, Average Reward: -140.87\n",
      "Iteration 3037, Average Reward: -146.22\n",
      "Iteration 3038, Average Reward: -139.61\n",
      "Iteration 3039, Average Reward: -117.31\n",
      "Iteration 3040, Average Reward: -116.35\n",
      "Iteration 3041, Average Reward: -121.69\n",
      "Iteration 3042, Average Reward: -119.9\n",
      "Iteration 3043, Average Reward: -101.88\n",
      "Iteration 3044, Average Reward: -92.49\n",
      "Iteration 3045, Average Reward: -117.17\n",
      "Iteration 3046, Average Reward: -121.14\n",
      "Iteration 3047, Average Reward: -119.24\n",
      "Iteration 3048, Average Reward: -109.89\n",
      "Iteration 3049, Average Reward: -101.72\n",
      "Iteration 3050, Average Reward: -103.86\n",
      "Iteration 3051, Average Reward: -114.43\n",
      "Iteration 3052, Average Reward: -117.37\n",
      "Iteration 3053, Average Reward: -140.78\n",
      "Iteration 3054, Average Reward: -125.41\n",
      "Iteration 3055, Average Reward: -129.54\n",
      "Iteration 3056, Average Reward: -131.89\n",
      "Iteration 3057, Average Reward: -127.56\n",
      "Iteration 3058, Average Reward: -112.15\n",
      "Iteration 3059, Average Reward: -121.85\n",
      "Iteration 3060, Average Reward: -128.89\n",
      "Iteration 3061, Average Reward: -125.75\n",
      "Iteration 3062, Average Reward: -105.97\n",
      "Iteration 3063, Average Reward: -111.9\n",
      "Iteration 3064, Average Reward: -105.79\n",
      "Iteration 3065, Average Reward: -126.08\n",
      "Iteration 3066, Average Reward: -131.99\n",
      "Iteration 3067, Average Reward: -135.86\n",
      "Iteration 3068, Average Reward: -150.51\n",
      "Iteration 3069, Average Reward: -148.12\n",
      "Iteration 3070, Average Reward: -121.78\n",
      "Iteration 3071, Average Reward: -114.66\n",
      "Iteration 3072, Average Reward: -114.14\n",
      "Iteration 3073, Average Reward: -116.09\n",
      "Iteration 3074, Average Reward: -121.49\n",
      "Iteration 3075, Average Reward: -114.37\n",
      "Iteration 3076, Average Reward: -119.19\n",
      "Iteration 3077, Average Reward: -112.54\n",
      "Iteration 3078, Average Reward: -110.41\n",
      "Iteration 3079, Average Reward: -105.13\n",
      "Iteration 3080, Average Reward: -103.02\n",
      "Iteration 3081, Average Reward: -105.81\n",
      "Iteration 3082, Average Reward: -109.69\n",
      "Iteration 3083, Average Reward: -104.26\n",
      "Iteration 3084, Average Reward: -98.35\n",
      "Iteration 3085, Average Reward: -112.58\n",
      "Iteration 3086, Average Reward: -112.86\n",
      "Iteration 3087, Average Reward: -109.63\n",
      "Iteration 3088, Average Reward: -120.64\n",
      "Iteration 3089, Average Reward: -122.35\n",
      "Iteration 3090, Average Reward: -118.56\n",
      "Iteration 3091, Average Reward: -115.37\n",
      "Iteration 3092, Average Reward: -111.36\n",
      "Iteration 3093, Average Reward: -130.71\n",
      "Iteration 3094, Average Reward: -131.14\n",
      "Iteration 3095, Average Reward: -121.47\n",
      "Iteration 3096, Average Reward: -111.86\n",
      "Iteration 3097, Average Reward: -122.44\n",
      "Iteration 3098, Average Reward: -121.86\n",
      "Iteration 3099, Average Reward: -121.28\n",
      "Iteration 3100, Average Reward: -112.81\n",
      "Iteration 3101, Average Reward: -108.83\n",
      "Iteration 3102, Average Reward: -114.19\n",
      "Iteration 3103, Average Reward: -122.13\n",
      "Iteration 3104, Average Reward: -118.11\n",
      "Iteration 3105, Average Reward: -125.92\n",
      "Iteration 3106, Average Reward: -120.84\n",
      "Iteration 3107, Average Reward: -132.96\n",
      "Iteration 3108, Average Reward: -124.13\n",
      "Iteration 3109, Average Reward: -119.51\n",
      "Iteration 3110, Average Reward: -111.41\n",
      "Iteration 3111, Average Reward: -119.28\n",
      "Iteration 3112, Average Reward: -129.0\n",
      "Iteration 3113, Average Reward: -124.7\n",
      "Iteration 3114, Average Reward: -118.94\n",
      "Iteration 3115, Average Reward: -125.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3116, Average Reward: -141.04\n",
      "Iteration 3117, Average Reward: -141.18\n",
      "Iteration 3118, Average Reward: -135.36\n",
      "Iteration 3119, Average Reward: -122.79\n",
      "Iteration 3120, Average Reward: -110.27\n",
      "Iteration 3121, Average Reward: -113.79\n",
      "Iteration 3122, Average Reward: -100.02\n",
      "Iteration 3123, Average Reward: -124.75\n",
      "Iteration 3124, Average Reward: -132.92\n",
      "Iteration 3125, Average Reward: -139.73\n",
      "Iteration 3126, Average Reward: -150.45\n",
      "Iteration 3127, Average Reward: -140.42\n",
      "Iteration 3128, Average Reward: -128.66\n",
      "Iteration 3129, Average Reward: -104.33\n",
      "Iteration 3130, Average Reward: -95.72\n",
      "Iteration 3131, Average Reward: -110.05\n",
      "Iteration 3132, Average Reward: -109.44\n",
      "Iteration 3133, Average Reward: -101.33\n",
      "Iteration 3134, Average Reward: -127.92\n",
      "Iteration 3135, Average Reward: -120.76\n",
      "Iteration 3136, Average Reward: -118.51\n",
      "Iteration 3137, Average Reward: -137.26\n",
      "Iteration 3138, Average Reward: -139.94\n",
      "Iteration 3139, Average Reward: -119.68\n",
      "Iteration 3140, Average Reward: -114.18\n",
      "Iteration 3141, Average Reward: -122.44\n",
      "Iteration 3142, Average Reward: -130.76\n",
      "Iteration 3143, Average Reward: -138.98\n",
      "Iteration 3144, Average Reward: -126.25\n",
      "Iteration 3145, Average Reward: -125.22\n",
      "Iteration 3146, Average Reward: -116.24\n",
      "Iteration 3147, Average Reward: -116.38\n",
      "Iteration 3148, Average Reward: -121.61\n",
      "Iteration 3149, Average Reward: -103.58\n",
      "Iteration 3150, Average Reward: -108.49\n",
      "Iteration 3151, Average Reward: -109.14\n",
      "Iteration 3152, Average Reward: -115.52\n",
      "Iteration 3153, Average Reward: -124.55\n",
      "Iteration 3154, Average Reward: -139.5\n",
      "Iteration 3155, Average Reward: -154.09\n",
      "Iteration 3156, Average Reward: -146.2\n",
      "Iteration 3157, Average Reward: -138.14\n",
      "Iteration 3158, Average Reward: -135.68\n",
      "Iteration 3159, Average Reward: -121.68\n",
      "Iteration 3160, Average Reward: -115.67\n",
      "Iteration 3161, Average Reward: -106.42\n",
      "Iteration 3162, Average Reward: -93.94\n",
      "Iteration 3163, Average Reward: -110.06\n",
      "Iteration 3164, Average Reward: -115.0\n",
      "Iteration 3165, Average Reward: -104.65\n",
      "Iteration 3166, Average Reward: -115.37\n",
      "Iteration 3167, Average Reward: -128.73\n",
      "Iteration 3168, Average Reward: -123.05\n",
      "Iteration 3169, Average Reward: -115.59\n",
      "Iteration 3170, Average Reward: -124.37\n",
      "Iteration 3171, Average Reward: -109.65\n",
      "Iteration 3172, Average Reward: -97.25\n",
      "Iteration 3173, Average Reward: -113.74\n",
      "Iteration 3174, Average Reward: -129.1\n",
      "Iteration 3175, Average Reward: -134.05\n",
      "Iteration 3176, Average Reward: -136.03\n",
      "Iteration 3177, Average Reward: -129.32\n",
      "Iteration 3178, Average Reward: -123.24\n",
      "Iteration 3179, Average Reward: -98.96\n",
      "Iteration 3180, Average Reward: -104.62\n",
      "Iteration 3181, Average Reward: -91.89\n",
      "Iteration 3182, Average Reward: -101.18\n",
      "Iteration 3183, Average Reward: -107.79\n",
      "Iteration 3184, Average Reward: -112.96\n",
      "Iteration 3185, Average Reward: -113.53\n",
      "Iteration 3186, Average Reward: -130.26\n",
      "Iteration 3187, Average Reward: -133.8\n",
      "Iteration 3188, Average Reward: -136.87\n",
      "Iteration 3189, Average Reward: -127.65\n",
      "Iteration 3190, Average Reward: -121.01\n",
      "Iteration 3191, Average Reward: -116.36\n",
      "Iteration 3192, Average Reward: -115.56\n",
      "Iteration 3193, Average Reward: -120.7\n",
      "Iteration 3194, Average Reward: -132.71\n",
      "Iteration 3195, Average Reward: -132.15\n",
      "Iteration 3196, Average Reward: -127.15\n",
      "Iteration 3197, Average Reward: -128.08\n",
      "Iteration 3198, Average Reward: -125.76\n",
      "Iteration 3199, Average Reward: -117.73\n",
      "Iteration 3200, Average Reward: -132.23\n",
      "Iteration 3201, Average Reward: -133.55\n",
      "Iteration 3202, Average Reward: -121.62\n",
      "Iteration 3203, Average Reward: -135.31\n",
      "Iteration 3204, Average Reward: -141.6\n",
      "Iteration 3205, Average Reward: -129.08\n",
      "Iteration 3206, Average Reward: -136.63\n",
      "Iteration 3207, Average Reward: -129.56\n",
      "Iteration 3208, Average Reward: -128.53\n",
      "Iteration 3209, Average Reward: -115.82\n",
      "Iteration 3210, Average Reward: -129.63\n",
      "Iteration 3211, Average Reward: -134.29\n",
      "Iteration 3212, Average Reward: -131.59\n",
      "Iteration 3213, Average Reward: -115.77\n",
      "Iteration 3214, Average Reward: -117.64\n",
      "Iteration 3215, Average Reward: -128.07\n",
      "Iteration 3216, Average Reward: -121.53\n",
      "Iteration 3217, Average Reward: -120.52\n",
      "Iteration 3218, Average Reward: -123.69\n",
      "Iteration 3219, Average Reward: -131.68\n",
      "Iteration 3220, Average Reward: -117.86\n",
      "Iteration 3221, Average Reward: -108.82\n",
      "Iteration 3222, Average Reward: -102.47\n",
      "Iteration 3223, Average Reward: -109.58\n",
      "Iteration 3224, Average Reward: -97.27\n",
      "Iteration 3225, Average Reward: -116.01\n",
      "Iteration 3226, Average Reward: -124.28\n",
      "Iteration 3227, Average Reward: -132.89\n",
      "Iteration 3228, Average Reward: -140.6\n",
      "Iteration 3229, Average Reward: -143.12\n",
      "Iteration 3230, Average Reward: -122.3\n",
      "Iteration 3231, Average Reward: -107.15\n",
      "Iteration 3232, Average Reward: -111.21\n",
      "Iteration 3233, Average Reward: -110.87\n",
      "Iteration 3234, Average Reward: -106.98\n",
      "Iteration 3235, Average Reward: -118.84\n",
      "Iteration 3236, Average Reward: -121.58\n",
      "Iteration 3237, Average Reward: -127.02\n",
      "Iteration 3238, Average Reward: -128.92\n",
      "Iteration 3239, Average Reward: -132.5\n",
      "Iteration 3240, Average Reward: -137.08\n",
      "Iteration 3241, Average Reward: -122.29\n",
      "Iteration 3242, Average Reward: -104.97\n",
      "Iteration 3243, Average Reward: -119.75\n",
      "Iteration 3244, Average Reward: -112.21\n",
      "Iteration 3245, Average Reward: -117.29\n",
      "Iteration 3246, Average Reward: -146.16\n",
      "Iteration 3247, Average Reward: -115.35\n",
      "Iteration 3248, Average Reward: -111.39\n",
      "Iteration 3249, Average Reward: -116.04\n",
      "Iteration 3250, Average Reward: -122.28\n",
      "Iteration 3251, Average Reward: -128.23\n",
      "Iteration 3252, Average Reward: -140.68\n",
      "Iteration 3253, Average Reward: -148.66\n",
      "Iteration 3254, Average Reward: -139.49\n",
      "Iteration 3255, Average Reward: -116.34\n",
      "Iteration 3256, Average Reward: -94.61\n",
      "Iteration 3257, Average Reward: -86.61\n",
      "Iteration 3258, Average Reward: -82.31\n",
      "Iteration 3259, Average Reward: -106.55\n",
      "Iteration 3260, Average Reward: -119.18\n",
      "Iteration 3261, Average Reward: -119.45\n",
      "Iteration 3262, Average Reward: -130.04\n",
      "Iteration 3263, Average Reward: -144.49\n",
      "Iteration 3264, Average Reward: -141.83\n",
      "Iteration 3265, Average Reward: -126.13\n",
      "Iteration 3266, Average Reward: -118.73\n",
      "Iteration 3267, Average Reward: -102.96\n",
      "Iteration 3268, Average Reward: -100.37\n",
      "Iteration 3269, Average Reward: -114.82\n",
      "Iteration 3270, Average Reward: -117.1\n",
      "Iteration 3271, Average Reward: -116.13\n",
      "Iteration 3272, Average Reward: -119.26\n",
      "Iteration 3273, Average Reward: -136.2\n",
      "Iteration 3274, Average Reward: -145.26\n",
      "Iteration 3275, Average Reward: -140.48\n",
      "Iteration 3276, Average Reward: -149.59\n",
      "Iteration 3277, Average Reward: -128.43\n",
      "Iteration 3278, Average Reward: -99.41\n",
      "Iteration 3279, Average Reward: -104.17\n",
      "Iteration 3280, Average Reward: -92.86\n",
      "Iteration 3281, Average Reward: -98.39\n",
      "Iteration 3282, Average Reward: -128.02\n",
      "Iteration 3283, Average Reward: -144.23\n",
      "Iteration 3284, Average Reward: -152.67\n",
      "Iteration 3285, Average Reward: -143.11\n",
      "Iteration 3286, Average Reward: -132.38\n",
      "Iteration 3287, Average Reward: -121.07\n",
      "Iteration 3288, Average Reward: -118.61\n",
      "Iteration 3289, Average Reward: -117.91\n",
      "Iteration 3290, Average Reward: -127.69\n",
      "Iteration 3291, Average Reward: -134.93\n",
      "Iteration 3292, Average Reward: -133.77\n",
      "Iteration 3293, Average Reward: -130.72\n",
      "Iteration 3294, Average Reward: -134.42\n",
      "Iteration 3295, Average Reward: -133.55\n",
      "Iteration 3296, Average Reward: -119.31\n",
      "Iteration 3297, Average Reward: -131.41\n",
      "Iteration 3298, Average Reward: -121.95\n",
      "Iteration 3299, Average Reward: -119.91\n",
      "Iteration 3300, Average Reward: -124.31\n",
      "Iteration 3301, Average Reward: -136.67\n",
      "Iteration 3302, Average Reward: -126.77\n",
      "Iteration 3303, Average Reward: -137.28\n",
      "Iteration 3304, Average Reward: -129.95\n",
      "Iteration 3305, Average Reward: -112.49\n",
      "Iteration 3306, Average Reward: -106.49\n",
      "Iteration 3307, Average Reward: -108.12\n",
      "Iteration 3308, Average Reward: -114.68\n",
      "Iteration 3309, Average Reward: -126.51\n",
      "Iteration 3310, Average Reward: -123.95\n",
      "Iteration 3311, Average Reward: -132.57\n",
      "Iteration 3312, Average Reward: -125.03\n",
      "Iteration 3313, Average Reward: -121.37\n",
      "Iteration 3314, Average Reward: -113.23\n",
      "Iteration 3315, Average Reward: -106.28\n",
      "Iteration 3316, Average Reward: -110.97\n",
      "Iteration 3317, Average Reward: -120.14\n",
      "Iteration 3318, Average Reward: -114.73\n",
      "Iteration 3319, Average Reward: -115.59\n",
      "Iteration 3320, Average Reward: -111.07\n",
      "Iteration 3321, Average Reward: -114.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3322, Average Reward: -128.63\n",
      "Iteration 3323, Average Reward: -132.44\n",
      "Iteration 3324, Average Reward: -130.31\n",
      "Iteration 3325, Average Reward: -134.04\n",
      "Iteration 3326, Average Reward: -134.67\n",
      "Iteration 3327, Average Reward: -128.96\n",
      "Iteration 3328, Average Reward: -112.14\n",
      "Iteration 3329, Average Reward: -100.44\n",
      "Iteration 3330, Average Reward: -97.52\n",
      "Iteration 3331, Average Reward: -100.47\n",
      "Iteration 3332, Average Reward: -117.43\n",
      "Iteration 3333, Average Reward: -126.76\n",
      "Iteration 3334, Average Reward: -127.6\n",
      "Iteration 3335, Average Reward: -135.57\n",
      "Iteration 3336, Average Reward: -138.07\n",
      "Iteration 3337, Average Reward: -130.64\n",
      "Iteration 3338, Average Reward: -111.51\n",
      "Iteration 3339, Average Reward: -112.6\n",
      "Iteration 3340, Average Reward: -118.24\n",
      "Iteration 3341, Average Reward: -109.31\n",
      "Iteration 3342, Average Reward: -104.3\n",
      "Iteration 3343, Average Reward: -119.22\n",
      "Iteration 3344, Average Reward: -132.2\n",
      "Iteration 3345, Average Reward: -133.18\n",
      "Iteration 3346, Average Reward: -136.02\n",
      "Iteration 3347, Average Reward: -117.21\n",
      "Iteration 3348, Average Reward: -119.67\n",
      "Iteration 3349, Average Reward: -125.72\n",
      "Iteration 3350, Average Reward: -124.97\n",
      "Iteration 3351, Average Reward: -106.39\n",
      "Iteration 3352, Average Reward: -108.48\n",
      "Iteration 3353, Average Reward: -121.78\n",
      "Iteration 3354, Average Reward: -136.66\n",
      "Iteration 3355, Average Reward: -130.2\n",
      "Iteration 3356, Average Reward: -119.0\n",
      "Iteration 3357, Average Reward: -115.05\n",
      "Iteration 3358, Average Reward: -109.78\n",
      "Iteration 3359, Average Reward: -116.0\n",
      "Iteration 3360, Average Reward: -129.27\n",
      "Iteration 3361, Average Reward: -117.71\n",
      "Iteration 3362, Average Reward: -119.49\n",
      "Iteration 3363, Average Reward: -119.94\n",
      "Iteration 3364, Average Reward: -126.33\n",
      "Iteration 3365, Average Reward: -124.8\n",
      "Iteration 3366, Average Reward: -113.52\n",
      "Iteration 3367, Average Reward: -121.01\n",
      "Iteration 3368, Average Reward: -125.67\n",
      "Iteration 3369, Average Reward: -125.41\n",
      "Iteration 3370, Average Reward: -115.25\n",
      "Iteration 3371, Average Reward: -103.13\n",
      "Iteration 3372, Average Reward: -128.88\n",
      "Iteration 3373, Average Reward: -130.87\n",
      "Iteration 3374, Average Reward: -125.52\n",
      "Iteration 3375, Average Reward: -135.92\n",
      "Iteration 3376, Average Reward: -132.58\n",
      "Iteration 3377, Average Reward: -132.33\n",
      "Iteration 3378, Average Reward: -127.26\n",
      "Iteration 3379, Average Reward: -135.5\n",
      "Iteration 3380, Average Reward: -126.85\n",
      "Iteration 3381, Average Reward: -123.69\n",
      "Iteration 3382, Average Reward: -142.98\n",
      "Iteration 3383, Average Reward: -128.62\n",
      "Iteration 3384, Average Reward: -122.67\n",
      "Iteration 3385, Average Reward: -122.95\n",
      "Iteration 3386, Average Reward: -128.99\n",
      "Iteration 3387, Average Reward: -132.27\n",
      "Iteration 3388, Average Reward: -139.23\n",
      "Iteration 3389, Average Reward: -134.94\n",
      "Iteration 3390, Average Reward: -121.86\n",
      "Iteration 3391, Average Reward: -119.77\n",
      "Iteration 3392, Average Reward: -102.95\n",
      "Iteration 3393, Average Reward: -115.2\n",
      "Iteration 3394, Average Reward: -109.54\n",
      "Iteration 3395, Average Reward: -121.63\n",
      "Iteration 3396, Average Reward: -126.75\n",
      "Iteration 3397, Average Reward: -128.66\n",
      "Iteration 3398, Average Reward: -116.7\n",
      "Iteration 3399, Average Reward: -116.98\n",
      "Iteration 3400, Average Reward: -116.97\n",
      "Iteration 3401, Average Reward: -112.53\n",
      "Iteration 3402, Average Reward: -114.5\n",
      "Iteration 3403, Average Reward: -116.79\n",
      "Iteration 3404, Average Reward: -135.48\n",
      "Iteration 3405, Average Reward: -144.74\n",
      "Iteration 3406, Average Reward: -141.27\n",
      "Iteration 3407, Average Reward: -130.08\n",
      "Iteration 3408, Average Reward: -120.91\n",
      "Iteration 3409, Average Reward: -114.79\n",
      "Iteration 3410, Average Reward: -110.32\n",
      "Iteration 3411, Average Reward: -106.65\n",
      "Iteration 3412, Average Reward: -117.7\n",
      "Iteration 3413, Average Reward: -134.36\n",
      "Iteration 3414, Average Reward: -117.79\n",
      "Iteration 3415, Average Reward: -105.86\n",
      "Iteration 3416, Average Reward: -105.18\n",
      "Iteration 3417, Average Reward: -119.15\n",
      "Iteration 3418, Average Reward: -129.32\n",
      "Iteration 3419, Average Reward: -146.35\n",
      "Iteration 3420, Average Reward: -139.68\n",
      "Iteration 3421, Average Reward: -120.03\n",
      "Iteration 3422, Average Reward: -125.16\n",
      "Iteration 3423, Average Reward: -140.36\n",
      "Iteration 3424, Average Reward: -122.56\n",
      "Iteration 3425, Average Reward: -120.68\n",
      "Iteration 3426, Average Reward: -119.43\n",
      "Iteration 3427, Average Reward: -123.64\n",
      "Iteration 3428, Average Reward: -108.97\n",
      "Iteration 3429, Average Reward: -120.34\n",
      "Iteration 3430, Average Reward: -110.71\n",
      "Iteration 3431, Average Reward: -107.82\n",
      "Iteration 3432, Average Reward: -118.11\n",
      "Iteration 3433, Average Reward: -127.96\n",
      "Iteration 3434, Average Reward: -127.59\n",
      "Iteration 3435, Average Reward: -137.18\n",
      "Iteration 3436, Average Reward: -117.17\n",
      "Iteration 3437, Average Reward: -105.25\n",
      "Iteration 3438, Average Reward: -111.3\n",
      "Iteration 3439, Average Reward: -111.33\n",
      "Iteration 3440, Average Reward: -132.92\n",
      "Iteration 3441, Average Reward: -123.42\n",
      "Iteration 3442, Average Reward: -107.12\n",
      "Iteration 3443, Average Reward: -124.77\n",
      "Iteration 3444, Average Reward: -118.55\n",
      "Iteration 3445, Average Reward: -109.91\n",
      "Iteration 3446, Average Reward: -118.97\n",
      "Iteration 3447, Average Reward: -129.53\n",
      "Iteration 3448, Average Reward: -110.46\n",
      "Iteration 3449, Average Reward: -102.88\n",
      "Iteration 3450, Average Reward: -114.17\n",
      "Iteration 3451, Average Reward: -117.76\n",
      "Iteration 3452, Average Reward: -106.88\n",
      "Iteration 3453, Average Reward: -123.11\n",
      "Iteration 3454, Average Reward: -138.53\n",
      "Iteration 3455, Average Reward: -129.61\n",
      "Iteration 3456, Average Reward: -123.54\n",
      "Iteration 3457, Average Reward: -133.49\n",
      "Iteration 3458, Average Reward: -126.79\n",
      "Iteration 3459, Average Reward: -130.2\n",
      "Iteration 3460, Average Reward: -129.66\n",
      "Iteration 3461, Average Reward: -123.53\n",
      "Iteration 3462, Average Reward: -133.62\n",
      "Iteration 3463, Average Reward: -124.12\n",
      "Iteration 3464, Average Reward: -109.13\n",
      "Iteration 3465, Average Reward: -112.32\n",
      "Iteration 3466, Average Reward: -136.03\n",
      "Iteration 3467, Average Reward: -135.27\n",
      "Iteration 3468, Average Reward: -134.42\n",
      "Iteration 3469, Average Reward: -127.37\n",
      "Iteration 3470, Average Reward: -135.97\n",
      "Iteration 3471, Average Reward: -135.66\n",
      "Iteration 3472, Average Reward: -131.39\n",
      "Iteration 3473, Average Reward: -119.9\n",
      "Iteration 3474, Average Reward: -129.58\n",
      "Iteration 3475, Average Reward: -130.33\n",
      "Iteration 3476, Average Reward: -113.4\n",
      "Iteration 3477, Average Reward: -119.87\n",
      "Iteration 3478, Average Reward: -126.98\n",
      "Iteration 3479, Average Reward: -130.06\n",
      "Iteration 3480, Average Reward: -116.24\n",
      "Iteration 3481, Average Reward: -115.77\n",
      "Iteration 3482, Average Reward: -117.7\n",
      "Iteration 3483, Average Reward: -130.55\n",
      "Iteration 3484, Average Reward: -136.17\n",
      "Iteration 3485, Average Reward: -117.47\n",
      "Iteration 3486, Average Reward: -108.58\n",
      "Iteration 3487, Average Reward: -109.99\n",
      "Iteration 3488, Average Reward: -102.27\n",
      "Iteration 3489, Average Reward: -94.33\n",
      "Iteration 3490, Average Reward: -108.53\n",
      "Iteration 3491, Average Reward: -142.12\n",
      "Iteration 3492, Average Reward: -142.14\n",
      "Iteration 3493, Average Reward: -131.28\n",
      "Iteration 3494, Average Reward: -126.34\n",
      "Iteration 3495, Average Reward: -126.45\n",
      "Iteration 3496, Average Reward: -119.26\n",
      "Iteration 3497, Average Reward: -111.86\n",
      "Iteration 3498, Average Reward: -120.84\n",
      "Iteration 3499, Average Reward: -129.53\n",
      "Iteration 3500, Average Reward: -119.39\n",
      "Iteration 3501, Average Reward: -123.56\n",
      "Iteration 3502, Average Reward: -132.48\n",
      "Iteration 3503, Average Reward: -130.1\n",
      "Iteration 3504, Average Reward: -133.33\n",
      "Iteration 3505, Average Reward: -140.52\n",
      "Iteration 3506, Average Reward: -121.14\n",
      "Iteration 3507, Average Reward: -136.34\n",
      "Iteration 3508, Average Reward: -117.95\n",
      "Iteration 3509, Average Reward: -119.65\n",
      "Iteration 3510, Average Reward: -132.33\n",
      "Iteration 3511, Average Reward: -139.45\n",
      "Iteration 3512, Average Reward: -133.46\n",
      "Iteration 3513, Average Reward: -135.54\n",
      "Iteration 3514, Average Reward: -132.96\n",
      "Iteration 3515, Average Reward: -128.21\n",
      "Iteration 3516, Average Reward: -123.66\n",
      "Iteration 3517, Average Reward: -112.57\n",
      "Iteration 3518, Average Reward: -121.28\n",
      "Iteration 3519, Average Reward: -122.09\n",
      "Iteration 3520, Average Reward: -121.58\n",
      "Iteration 3521, Average Reward: -123.37\n",
      "Iteration 3522, Average Reward: -115.36\n",
      "Iteration 3523, Average Reward: -116.16\n",
      "Iteration 3524, Average Reward: -123.6\n",
      "Iteration 3525, Average Reward: -127.27\n",
      "Iteration 3526, Average Reward: -126.32\n",
      "Iteration 3527, Average Reward: -126.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3528, Average Reward: -126.57\n",
      "Iteration 3529, Average Reward: -113.13\n",
      "Iteration 3530, Average Reward: -110.58\n",
      "Iteration 3531, Average Reward: -125.7\n",
      "Iteration 3532, Average Reward: -132.0\n",
      "Iteration 3533, Average Reward: -135.22\n",
      "Iteration 3534, Average Reward: -118.28\n",
      "Iteration 3535, Average Reward: -112.84\n",
      "Iteration 3536, Average Reward: -123.68\n",
      "Iteration 3537, Average Reward: -126.04\n",
      "Iteration 3538, Average Reward: -117.72\n",
      "Iteration 3539, Average Reward: -139.16\n",
      "Iteration 3540, Average Reward: -131.81\n",
      "Iteration 3541, Average Reward: -127.86\n",
      "Iteration 3542, Average Reward: -114.94\n",
      "Iteration 3543, Average Reward: -105.48\n",
      "Iteration 3544, Average Reward: -113.58\n",
      "Iteration 3545, Average Reward: -119.78\n",
      "Iteration 3546, Average Reward: -121.54\n",
      "Iteration 3547, Average Reward: -132.26\n",
      "Iteration 3548, Average Reward: -144.94\n",
      "Iteration 3549, Average Reward: -143.03\n",
      "Iteration 3550, Average Reward: -119.63\n",
      "Iteration 3551, Average Reward: -120.77\n",
      "Iteration 3552, Average Reward: -128.1\n",
      "Iteration 3553, Average Reward: -108.24\n",
      "Iteration 3554, Average Reward: -104.82\n",
      "Iteration 3555, Average Reward: -116.07\n",
      "Iteration 3556, Average Reward: -141.47\n",
      "Iteration 3557, Average Reward: -147.75\n",
      "Iteration 3558, Average Reward: -148.63\n",
      "Iteration 3559, Average Reward: -142.03\n",
      "Iteration 3560, Average Reward: -126.77\n",
      "Iteration 3561, Average Reward: -103.49\n",
      "Iteration 3562, Average Reward: -104.6\n",
      "Iteration 3563, Average Reward: -110.31\n",
      "Iteration 3564, Average Reward: -111.12\n",
      "Iteration 3565, Average Reward: -109.46\n",
      "Iteration 3566, Average Reward: -116.62\n",
      "Iteration 3567, Average Reward: -128.36\n",
      "Iteration 3568, Average Reward: -130.73\n",
      "Iteration 3569, Average Reward: -119.93\n",
      "Iteration 3570, Average Reward: -110.83\n",
      "Iteration 3571, Average Reward: -109.82\n",
      "Iteration 3572, Average Reward: -98.7\n",
      "Iteration 3573, Average Reward: -102.16\n",
      "Iteration 3574, Average Reward: -114.14\n",
      "Iteration 3575, Average Reward: -124.64\n",
      "Iteration 3576, Average Reward: -122.28\n",
      "Iteration 3577, Average Reward: -126.78\n",
      "Iteration 3578, Average Reward: -138.51\n",
      "Iteration 3579, Average Reward: -141.42\n",
      "Iteration 3580, Average Reward: -134.82\n",
      "Iteration 3581, Average Reward: -138.99\n",
      "Iteration 3582, Average Reward: -114.49\n",
      "Iteration 3583, Average Reward: -122.62\n",
      "Iteration 3584, Average Reward: -127.04\n",
      "Iteration 3585, Average Reward: -109.41\n",
      "Iteration 3586, Average Reward: -113.52\n",
      "Iteration 3587, Average Reward: -126.72\n",
      "Iteration 3588, Average Reward: -132.2\n",
      "Iteration 3589, Average Reward: -141.73\n",
      "Iteration 3590, Average Reward: -123.33\n",
      "Iteration 3591, Average Reward: -124.86\n",
      "Iteration 3592, Average Reward: -120.19\n",
      "Iteration 3593, Average Reward: -132.83\n",
      "Iteration 3594, Average Reward: -114.51\n",
      "Iteration 3595, Average Reward: -129.71\n",
      "Iteration 3596, Average Reward: -150.09\n",
      "Iteration 3597, Average Reward: -156.9\n",
      "Iteration 3598, Average Reward: -143.32\n",
      "Iteration 3599, Average Reward: -126.98\n",
      "Iteration 3600, Average Reward: -118.54\n",
      "Iteration 3601, Average Reward: -114.95\n",
      "Iteration 3602, Average Reward: -103.21\n",
      "Iteration 3603, Average Reward: -116.66\n",
      "Iteration 3604, Average Reward: -113.44\n",
      "Iteration 3605, Average Reward: -128.78\n",
      "Iteration 3606, Average Reward: -129.85\n",
      "Iteration 3607, Average Reward: -130.82\n",
      "Iteration 3608, Average Reward: -138.02\n",
      "Iteration 3609, Average Reward: -137.22\n",
      "Iteration 3610, Average Reward: -142.71\n",
      "Iteration 3611, Average Reward: -131.07\n",
      "Iteration 3612, Average Reward: -107.65\n",
      "Iteration 3613, Average Reward: -110.48\n",
      "Iteration 3614, Average Reward: -110.55\n",
      "Iteration 3615, Average Reward: -104.42\n",
      "Iteration 3616, Average Reward: -96.85\n",
      "Iteration 3617, Average Reward: -94.32\n",
      "Iteration 3618, Average Reward: -115.97\n",
      "Iteration 3619, Average Reward: -119.89\n",
      "Iteration 3620, Average Reward: -143.32\n",
      "Iteration 3621, Average Reward: -139.22\n",
      "Iteration 3622, Average Reward: -132.29\n",
      "Iteration 3623, Average Reward: -111.01\n",
      "Iteration 3624, Average Reward: -100.42\n",
      "Iteration 3625, Average Reward: -108.39\n",
      "Iteration 3626, Average Reward: -109.25\n",
      "Iteration 3627, Average Reward: -126.28\n",
      "Iteration 3628, Average Reward: -132.47\n",
      "Iteration 3629, Average Reward: -123.59\n",
      "Iteration 3630, Average Reward: -125.36\n",
      "Iteration 3631, Average Reward: -117.49\n",
      "Iteration 3632, Average Reward: -120.62\n",
      "Iteration 3633, Average Reward: -117.15\n",
      "Iteration 3634, Average Reward: -134.34\n",
      "Iteration 3635, Average Reward: -126.93\n",
      "Iteration 3636, Average Reward: -120.1\n",
      "Iteration 3637, Average Reward: -119.19\n",
      "Iteration 3638, Average Reward: -122.55\n",
      "Iteration 3639, Average Reward: -121.01\n",
      "Iteration 3640, Average Reward: -119.89\n",
      "Iteration 3641, Average Reward: -126.3\n",
      "Iteration 3642, Average Reward: -121.68\n",
      "Iteration 3643, Average Reward: -114.26\n",
      "Iteration 3644, Average Reward: -119.34\n",
      "Iteration 3645, Average Reward: -129.38\n",
      "Iteration 3646, Average Reward: -128.25\n",
      "Iteration 3647, Average Reward: -115.11\n",
      "Iteration 3648, Average Reward: -117.9\n",
      "Iteration 3649, Average Reward: -130.54\n",
      "Iteration 3650, Average Reward: -140.64\n",
      "Iteration 3651, Average Reward: -128.49\n",
      "Iteration 3652, Average Reward: -114.05\n",
      "Iteration 3653, Average Reward: -107.51\n",
      "Iteration 3654, Average Reward: -109.2\n",
      "Iteration 3655, Average Reward: -117.82\n",
      "Iteration 3656, Average Reward: -97.32\n",
      "Iteration 3657, Average Reward: -113.54\n",
      "Iteration 3658, Average Reward: -134.39\n",
      "Iteration 3659, Average Reward: -137.14\n",
      "Iteration 3660, Average Reward: -148.68\n",
      "Iteration 3661, Average Reward: -150.91\n",
      "Iteration 3662, Average Reward: -140.79\n",
      "Iteration 3663, Average Reward: -134.4\n",
      "Iteration 3664, Average Reward: -133.39\n",
      "Iteration 3665, Average Reward: -111.11\n",
      "Iteration 3666, Average Reward: -123.7\n",
      "Iteration 3667, Average Reward: -105.78\n",
      "Iteration 3668, Average Reward: -108.69\n",
      "Iteration 3669, Average Reward: -114.52\n",
      "Iteration 3670, Average Reward: -120.96\n",
      "Iteration 3671, Average Reward: -130.03\n",
      "Iteration 3672, Average Reward: -127.96\n",
      "Iteration 3673, Average Reward: -126.28\n",
      "Iteration 3674, Average Reward: -124.94\n",
      "Iteration 3675, Average Reward: -136.6\n",
      "Iteration 3676, Average Reward: -133.04\n",
      "Iteration 3677, Average Reward: -133.73\n",
      "Iteration 3678, Average Reward: -123.09\n",
      "Iteration 3679, Average Reward: -117.18\n",
      "Iteration 3680, Average Reward: -122.8\n",
      "Iteration 3681, Average Reward: -121.6\n",
      "Iteration 3682, Average Reward: -135.82\n",
      "Iteration 3683, Average Reward: -144.99\n",
      "Iteration 3684, Average Reward: -132.25\n",
      "Iteration 3685, Average Reward: -116.99\n",
      "Iteration 3686, Average Reward: -115.48\n",
      "Iteration 3687, Average Reward: -117.76\n",
      "Iteration 3688, Average Reward: -109.94\n",
      "Iteration 3689, Average Reward: -114.09\n",
      "Iteration 3690, Average Reward: -120.65\n",
      "Iteration 3691, Average Reward: -110.09\n",
      "Iteration 3692, Average Reward: -119.46\n",
      "Iteration 3693, Average Reward: -129.59\n",
      "Iteration 3694, Average Reward: -129.02\n",
      "Iteration 3695, Average Reward: -129.98\n",
      "Iteration 3696, Average Reward: -124.18\n",
      "Iteration 3697, Average Reward: -124.83\n",
      "Iteration 3698, Average Reward: -135.44\n",
      "Iteration 3699, Average Reward: -121.05\n",
      "Iteration 3700, Average Reward: -119.19\n",
      "Iteration 3701, Average Reward: -111.98\n",
      "Iteration 3702, Average Reward: -107.91\n",
      "Iteration 3703, Average Reward: -117.36\n",
      "Iteration 3704, Average Reward: -130.9\n",
      "Iteration 3705, Average Reward: -126.44\n",
      "Iteration 3706, Average Reward: -121.19\n",
      "Iteration 3707, Average Reward: -133.03\n",
      "Iteration 3708, Average Reward: -124.88\n",
      "Iteration 3709, Average Reward: -122.42\n",
      "Iteration 3710, Average Reward: -111.35\n",
      "Iteration 3711, Average Reward: -116.64\n",
      "Iteration 3712, Average Reward: -118.14\n",
      "Iteration 3713, Average Reward: -127.1\n",
      "Iteration 3714, Average Reward: -119.05\n",
      "Iteration 3715, Average Reward: -125.96\n",
      "Iteration 3716, Average Reward: -125.18\n",
      "Iteration 3717, Average Reward: -122.11\n",
      "Iteration 3718, Average Reward: -118.27\n",
      "Iteration 3719, Average Reward: -132.78\n",
      "Iteration 3720, Average Reward: -127.87\n",
      "Iteration 3721, Average Reward: -129.86\n",
      "Iteration 3722, Average Reward: -130.72\n",
      "Iteration 3723, Average Reward: -132.83\n",
      "Iteration 3724, Average Reward: -118.73\n",
      "Iteration 3725, Average Reward: -110.84\n",
      "Iteration 3726, Average Reward: -126.02\n",
      "Iteration 3727, Average Reward: -124.22\n",
      "Iteration 3728, Average Reward: -139.91\n",
      "Iteration 3729, Average Reward: -146.26\n",
      "Iteration 3730, Average Reward: -141.03\n",
      "Iteration 3731, Average Reward: -129.29\n",
      "Iteration 3732, Average Reward: -141.57\n",
      "Iteration 3733, Average Reward: -126.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3734, Average Reward: -113.38\n",
      "Iteration 3735, Average Reward: -114.34\n",
      "Iteration 3736, Average Reward: -103.53\n",
      "Iteration 3737, Average Reward: -102.42\n",
      "Iteration 3738, Average Reward: -135.29\n",
      "Iteration 3739, Average Reward: -138.86\n",
      "Iteration 3740, Average Reward: -142.84\n",
      "Iteration 3741, Average Reward: -144.43\n",
      "Iteration 3742, Average Reward: -150.16\n",
      "Iteration 3743, Average Reward: -131.44\n",
      "Iteration 3744, Average Reward: -111.04\n",
      "Iteration 3745, Average Reward: -104.21\n",
      "Iteration 3746, Average Reward: -112.07\n",
      "Iteration 3747, Average Reward: -114.6\n",
      "Iteration 3748, Average Reward: -120.65\n",
      "Iteration 3749, Average Reward: -137.55\n",
      "Iteration 3750, Average Reward: -138.97\n",
      "Iteration 3751, Average Reward: -133.05\n",
      "Iteration 3752, Average Reward: -140.34\n",
      "Iteration 3753, Average Reward: -136.58\n",
      "Iteration 3754, Average Reward: -126.04\n",
      "Iteration 3755, Average Reward: -124.11\n",
      "Iteration 3756, Average Reward: -112.93\n",
      "Iteration 3757, Average Reward: -102.44\n",
      "Iteration 3758, Average Reward: -108.64\n",
      "Iteration 3759, Average Reward: -111.21\n",
      "Iteration 3760, Average Reward: -130.73\n",
      "Iteration 3761, Average Reward: -149.19\n",
      "Iteration 3762, Average Reward: -129.66\n",
      "Iteration 3763, Average Reward: -110.88\n",
      "Iteration 3764, Average Reward: -98.95\n",
      "Iteration 3765, Average Reward: -107.57\n",
      "Iteration 3766, Average Reward: -128.14\n",
      "Iteration 3767, Average Reward: -126.89\n",
      "Iteration 3768, Average Reward: -139.29\n",
      "Iteration 3769, Average Reward: -137.92\n",
      "Iteration 3770, Average Reward: -121.12\n",
      "Iteration 3771, Average Reward: -126.95\n",
      "Iteration 3772, Average Reward: -125.81\n",
      "Iteration 3773, Average Reward: -126.5\n",
      "Iteration 3774, Average Reward: -110.82\n",
      "Iteration 3775, Average Reward: -110.68\n",
      "Iteration 3776, Average Reward: -116.17\n",
      "Iteration 3777, Average Reward: -128.29\n",
      "Iteration 3778, Average Reward: -135.14\n",
      "Iteration 3779, Average Reward: -120.41\n",
      "Iteration 3780, Average Reward: -120.62\n",
      "Iteration 3781, Average Reward: -97.46\n",
      "Iteration 3782, Average Reward: -99.33\n",
      "Iteration 3783, Average Reward: -110.83\n",
      "Iteration 3784, Average Reward: -105.75\n",
      "Iteration 3785, Average Reward: -110.12\n",
      "Iteration 3786, Average Reward: -116.04\n",
      "Iteration 3787, Average Reward: -130.68\n",
      "Iteration 3788, Average Reward: -119.61\n",
      "Iteration 3789, Average Reward: -115.81\n",
      "Iteration 3790, Average Reward: -117.19\n",
      "Iteration 3791, Average Reward: -132.64\n",
      "Iteration 3792, Average Reward: -120.82\n",
      "Iteration 3793, Average Reward: -125.87\n",
      "Iteration 3794, Average Reward: -112.97\n",
      "Iteration 3795, Average Reward: -100.38\n",
      "Iteration 3796, Average Reward: -116.22\n",
      "Iteration 3797, Average Reward: -114.74\n",
      "Iteration 3798, Average Reward: -126.26\n",
      "Iteration 3799, Average Reward: -133.43\n",
      "Iteration 3800, Average Reward: -130.72\n",
      "Iteration 3801, Average Reward: -129.48\n",
      "Iteration 3802, Average Reward: -125.73\n",
      "Iteration 3803, Average Reward: -124.74\n",
      "Iteration 3804, Average Reward: -134.64\n",
      "Iteration 3805, Average Reward: -123.57\n",
      "Iteration 3806, Average Reward: -113.66\n",
      "Iteration 3807, Average Reward: -108.63\n",
      "Iteration 3808, Average Reward: -125.02\n",
      "Iteration 3809, Average Reward: -126.37\n",
      "Iteration 3810, Average Reward: -118.12\n",
      "Iteration 3811, Average Reward: -126.27\n",
      "Iteration 3812, Average Reward: -137.85\n",
      "Iteration 3813, Average Reward: -144.67\n",
      "Iteration 3814, Average Reward: -141.9\n",
      "Iteration 3815, Average Reward: -131.74\n",
      "Iteration 3816, Average Reward: -128.58\n",
      "Iteration 3817, Average Reward: -109.17\n",
      "Iteration 3818, Average Reward: -105.21\n",
      "Iteration 3819, Average Reward: -103.69\n",
      "Iteration 3820, Average Reward: -93.46\n",
      "Iteration 3821, Average Reward: -104.34\n",
      "Iteration 3822, Average Reward: -123.95\n",
      "Iteration 3823, Average Reward: -129.45\n",
      "Iteration 3824, Average Reward: -131.22\n",
      "Iteration 3825, Average Reward: -145.85\n",
      "Iteration 3826, Average Reward: -134.98\n",
      "Iteration 3827, Average Reward: -103.96\n",
      "Iteration 3828, Average Reward: -115.69\n",
      "Iteration 3829, Average Reward: -119.06\n",
      "Iteration 3830, Average Reward: -125.22\n",
      "Iteration 3831, Average Reward: -125.8\n",
      "Iteration 3832, Average Reward: -121.47\n",
      "Iteration 3833, Average Reward: -108.49\n",
      "Iteration 3834, Average Reward: -117.79\n",
      "Iteration 3835, Average Reward: -133.23\n",
      "Iteration 3836, Average Reward: -125.84\n",
      "Iteration 3837, Average Reward: -123.26\n",
      "Iteration 3838, Average Reward: -123.75\n",
      "Iteration 3839, Average Reward: -133.45\n",
      "Iteration 3840, Average Reward: -121.91\n",
      "Iteration 3841, Average Reward: -139.78\n",
      "Iteration 3842, Average Reward: -132.97\n",
      "Iteration 3843, Average Reward: -123.61\n",
      "Iteration 3844, Average Reward: -112.32\n",
      "Iteration 3845, Average Reward: -106.86\n",
      "Iteration 3846, Average Reward: -119.37\n",
      "Iteration 3847, Average Reward: -132.96\n",
      "Iteration 3848, Average Reward: -120.96\n",
      "Iteration 3849, Average Reward: -120.04\n",
      "Iteration 3850, Average Reward: -131.3\n",
      "Iteration 3851, Average Reward: -124.48\n",
      "Iteration 3852, Average Reward: -101.21\n",
      "Iteration 3853, Average Reward: -113.35\n",
      "Iteration 3854, Average Reward: -137.52\n",
      "Iteration 3855, Average Reward: -106.62\n",
      "Iteration 3856, Average Reward: -97.53\n",
      "Iteration 3857, Average Reward: -105.77\n",
      "Iteration 3858, Average Reward: -121.69\n",
      "Iteration 3859, Average Reward: -120.85\n",
      "Iteration 3860, Average Reward: -140.48\n",
      "Iteration 3861, Average Reward: -138.09\n",
      "Iteration 3862, Average Reward: -121.62\n",
      "Iteration 3863, Average Reward: -113.05\n",
      "Iteration 3864, Average Reward: -110.27\n",
      "Iteration 3865, Average Reward: -107.75\n",
      "Iteration 3866, Average Reward: -114.92\n",
      "Iteration 3867, Average Reward: -107.83\n",
      "Iteration 3868, Average Reward: -122.66\n",
      "Iteration 3869, Average Reward: -130.13\n",
      "Iteration 3870, Average Reward: -128.96\n",
      "Iteration 3871, Average Reward: -132.62\n",
      "Iteration 3872, Average Reward: -112.35\n",
      "Iteration 3873, Average Reward: -116.96\n",
      "Iteration 3874, Average Reward: -121.96\n",
      "Iteration 3875, Average Reward: -131.72\n",
      "Iteration 3876, Average Reward: -114.22\n",
      "Iteration 3877, Average Reward: -114.25\n",
      "Iteration 3878, Average Reward: -113.8\n",
      "Iteration 3879, Average Reward: -119.65\n",
      "Iteration 3880, Average Reward: -151.96\n",
      "Iteration 3881, Average Reward: -154.46\n",
      "Iteration 3882, Average Reward: -130.31\n",
      "Iteration 3883, Average Reward: -116.62\n",
      "Iteration 3884, Average Reward: -120.06\n",
      "Iteration 3885, Average Reward: -100.86\n",
      "Iteration 3886, Average Reward: -101.91\n",
      "Iteration 3887, Average Reward: -119.39\n",
      "Iteration 3888, Average Reward: -129.59\n",
      "Iteration 3889, Average Reward: -118.9\n",
      "Iteration 3890, Average Reward: -119.9\n",
      "Iteration 3891, Average Reward: -131.89\n",
      "Iteration 3892, Average Reward: -138.67\n",
      "Iteration 3893, Average Reward: -138.37\n",
      "Iteration 3894, Average Reward: -126.07\n",
      "Iteration 3895, Average Reward: -130.22\n",
      "Iteration 3896, Average Reward: -136.37\n",
      "Iteration 3897, Average Reward: -126.83\n",
      "Iteration 3898, Average Reward: -108.23\n",
      "Iteration 3899, Average Reward: -119.8\n",
      "Iteration 3900, Average Reward: -122.91\n",
      "Iteration 3901, Average Reward: -117.34\n",
      "Iteration 3902, Average Reward: -103.84\n",
      "Iteration 3903, Average Reward: -123.7\n",
      "Iteration 3904, Average Reward: -137.15\n",
      "Iteration 3905, Average Reward: -115.56\n",
      "Iteration 3906, Average Reward: -122.43\n",
      "Iteration 3907, Average Reward: -137.06\n",
      "Iteration 3908, Average Reward: -134.71\n",
      "Iteration 3909, Average Reward: -121.75\n",
      "Iteration 3910, Average Reward: -126.09\n",
      "Iteration 3911, Average Reward: -131.96\n",
      "Iteration 3912, Average Reward: -114.19\n",
      "Iteration 3913, Average Reward: -103.56\n",
      "Iteration 3914, Average Reward: -106.34\n",
      "Iteration 3915, Average Reward: -109.21\n",
      "Iteration 3916, Average Reward: -114.68\n",
      "Iteration 3917, Average Reward: -114.36\n",
      "Iteration 3918, Average Reward: -115.86\n",
      "Iteration 3919, Average Reward: -132.57\n",
      "Iteration 3920, Average Reward: -128.17\n",
      "Iteration 3921, Average Reward: -131.46\n",
      "Iteration 3922, Average Reward: -133.31\n",
      "Iteration 3923, Average Reward: -114.59\n",
      "Iteration 3924, Average Reward: -128.01\n",
      "Iteration 3925, Average Reward: -118.83\n",
      "Iteration 3926, Average Reward: -126.27\n",
      "Iteration 3927, Average Reward: -128.69\n",
      "Iteration 3928, Average Reward: -127.1\n",
      "Iteration 3929, Average Reward: -115.7\n",
      "Iteration 3930, Average Reward: -105.22\n",
      "Iteration 3931, Average Reward: -106.9\n",
      "Iteration 3932, Average Reward: -115.03\n",
      "Iteration 3933, Average Reward: -128.09\n",
      "Iteration 3934, Average Reward: -153.33\n",
      "Iteration 3935, Average Reward: -139.3\n",
      "Iteration 3936, Average Reward: -131.01\n",
      "Iteration 3937, Average Reward: -132.24\n",
      "Iteration 3938, Average Reward: -122.06\n",
      "Iteration 3939, Average Reward: -115.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3940, Average Reward: -118.81\n",
      "Iteration 3941, Average Reward: -117.76\n",
      "Iteration 3942, Average Reward: -124.62\n",
      "Iteration 3943, Average Reward: -121.92\n",
      "Iteration 3944, Average Reward: -122.92\n",
      "Iteration 3945, Average Reward: -131.44\n",
      "Iteration 3946, Average Reward: -122.47\n",
      "Iteration 3947, Average Reward: -117.63\n",
      "Iteration 3948, Average Reward: -123.11\n",
      "Iteration 3949, Average Reward: -125.75\n",
      "Iteration 3950, Average Reward: -127.54\n",
      "Iteration 3951, Average Reward: -119.76\n",
      "Iteration 3952, Average Reward: -122.23\n",
      "Iteration 3953, Average Reward: -111.54\n",
      "Iteration 3954, Average Reward: -121.92\n",
      "Iteration 3955, Average Reward: -136.1\n",
      "Iteration 3956, Average Reward: -132.84\n",
      "Iteration 3957, Average Reward: -129.26\n",
      "Iteration 3958, Average Reward: -124.93\n",
      "Iteration 3959, Average Reward: -134.43\n",
      "Iteration 3960, Average Reward: -122.35\n",
      "Iteration 3961, Average Reward: -113.62\n",
      "Iteration 3962, Average Reward: -106.9\n",
      "Iteration 3963, Average Reward: -105.83\n",
      "Iteration 3964, Average Reward: -109.75\n",
      "Iteration 3965, Average Reward: -124.85\n",
      "Iteration 3966, Average Reward: -124.13\n",
      "Iteration 3967, Average Reward: -123.72\n",
      "Iteration 3968, Average Reward: -131.29\n",
      "Iteration 3969, Average Reward: -140.11\n",
      "Iteration 3970, Average Reward: -121.09\n",
      "Iteration 3971, Average Reward: -130.13\n",
      "Iteration 3972, Average Reward: -125.33\n",
      "Iteration 3973, Average Reward: -131.75\n",
      "Iteration 3974, Average Reward: -129.16\n",
      "Iteration 3975, Average Reward: -123.85\n",
      "Iteration 3976, Average Reward: -118.04\n",
      "Iteration 3977, Average Reward: -106.32\n",
      "Iteration 3978, Average Reward: -109.93\n",
      "Iteration 3979, Average Reward: -117.27\n",
      "Iteration 3980, Average Reward: -128.58\n",
      "Iteration 3981, Average Reward: -128.29\n",
      "Iteration 3982, Average Reward: -123.5\n",
      "Iteration 3983, Average Reward: -136.64\n",
      "Iteration 3984, Average Reward: -123.42\n",
      "Iteration 3985, Average Reward: -122.02\n",
      "Iteration 3986, Average Reward: -111.24\n",
      "Iteration 3987, Average Reward: -121.66\n",
      "Iteration 3988, Average Reward: -122.28\n",
      "Iteration 3989, Average Reward: -131.27\n",
      "Iteration 3990, Average Reward: -124.71\n",
      "Iteration 3991, Average Reward: -125.84\n",
      "Iteration 3992, Average Reward: -126.6\n",
      "Iteration 3993, Average Reward: -131.85\n",
      "Iteration 3994, Average Reward: -113.94\n",
      "Iteration 3995, Average Reward: -107.9\n",
      "Iteration 3996, Average Reward: -103.5\n",
      "Iteration 3997, Average Reward: -106.29\n",
      "Iteration 3998, Average Reward: -118.89\n",
      "Iteration 3999, Average Reward: -134.01\n",
      "Iteration 4000, Average Reward: -120.88\n",
      "Iteration 4001, Average Reward: -120.64\n",
      "Iteration 4002, Average Reward: -116.85\n",
      "Iteration 4003, Average Reward: -106.15\n",
      "Iteration 4004, Average Reward: -101.13\n",
      "Iteration 4005, Average Reward: -101.14\n",
      "Iteration 4006, Average Reward: -120.15\n",
      "Iteration 4007, Average Reward: -117.68\n",
      "Iteration 4008, Average Reward: -125.5\n",
      "Iteration 4009, Average Reward: -119.39\n",
      "Iteration 4010, Average Reward: -122.73\n",
      "Iteration 4011, Average Reward: -139.3\n",
      "Iteration 4012, Average Reward: -138.35\n",
      "Iteration 4013, Average Reward: -112.22\n",
      "Iteration 4014, Average Reward: -121.42\n",
      "Iteration 4015, Average Reward: -129.45\n",
      "Iteration 4016, Average Reward: -133.37\n",
      "Iteration 4017, Average Reward: -132.09\n",
      "Iteration 4018, Average Reward: -129.32\n",
      "Iteration 4019, Average Reward: -131.75\n",
      "Iteration 4020, Average Reward: -112.98\n",
      "Iteration 4021, Average Reward: -109.33\n",
      "Iteration 4022, Average Reward: -132.1\n",
      "Iteration 4023, Average Reward: -107.84\n",
      "Iteration 4024, Average Reward: -112.82\n",
      "Iteration 4025, Average Reward: -116.18\n",
      "Iteration 4026, Average Reward: -92.64\n",
      "Iteration 4027, Average Reward: -116.11\n",
      "Iteration 4028, Average Reward: -135.83\n",
      "Iteration 4029, Average Reward: -110.33\n",
      "Iteration 4030, Average Reward: -120.72\n",
      "Iteration 4031, Average Reward: -143.39\n",
      "Iteration 4032, Average Reward: -150.01\n",
      "Iteration 4033, Average Reward: -141.33\n",
      "Iteration 4034, Average Reward: -144.65\n",
      "Iteration 4035, Average Reward: -144.95\n",
      "Iteration 4036, Average Reward: -140.27\n",
      "Iteration 4037, Average Reward: -131.72\n",
      "Iteration 4038, Average Reward: -134.78\n",
      "Iteration 4039, Average Reward: -124.47\n",
      "Iteration 4040, Average Reward: -127.38\n",
      "Iteration 4041, Average Reward: -120.6\n",
      "Iteration 4042, Average Reward: -115.8\n",
      "Iteration 4043, Average Reward: -136.88\n",
      "Iteration 4044, Average Reward: -144.73\n",
      "Iteration 4045, Average Reward: -140.73\n",
      "Iteration 4046, Average Reward: -134.48\n",
      "Iteration 4047, Average Reward: -144.71\n",
      "Iteration 4048, Average Reward: -127.14\n",
      "Iteration 4049, Average Reward: -128.47\n",
      "Iteration 4050, Average Reward: -111.06\n",
      "Iteration 4051, Average Reward: -101.05\n",
      "Iteration 4052, Average Reward: -124.71\n",
      "Iteration 4053, Average Reward: -132.0\n",
      "Iteration 4054, Average Reward: -117.41\n",
      "Iteration 4055, Average Reward: -123.42\n",
      "Iteration 4056, Average Reward: -120.26\n",
      "Iteration 4057, Average Reward: -133.37\n",
      "Iteration 4058, Average Reward: -141.5\n",
      "Iteration 4059, Average Reward: -141.88\n",
      "Iteration 4060, Average Reward: -137.34\n",
      "Iteration 4061, Average Reward: -142.66\n",
      "Iteration 4062, Average Reward: -106.12\n",
      "Iteration 4063, Average Reward: -107.83\n",
      "Iteration 4064, Average Reward: -99.06\n",
      "Iteration 4065, Average Reward: -111.19\n",
      "Iteration 4066, Average Reward: -106.39\n",
      "Iteration 4067, Average Reward: -125.82\n",
      "Iteration 4068, Average Reward: -134.52\n",
      "Iteration 4069, Average Reward: -136.97\n",
      "Iteration 4070, Average Reward: -148.04\n",
      "Iteration 4071, Average Reward: -147.97\n",
      "Iteration 4072, Average Reward: -140.0\n",
      "Iteration 4073, Average Reward: -128.58\n",
      "Iteration 4074, Average Reward: -130.1\n",
      "Iteration 4075, Average Reward: -125.55\n",
      "Iteration 4076, Average Reward: -120.04\n",
      "Iteration 4077, Average Reward: -118.99\n",
      "Iteration 4078, Average Reward: -112.0\n",
      "Iteration 4079, Average Reward: -104.35\n",
      "Iteration 4080, Average Reward: -123.34\n",
      "Iteration 4081, Average Reward: -121.53\n",
      "Iteration 4082, Average Reward: -126.0\n",
      "Iteration 4083, Average Reward: -134.29\n",
      "Iteration 4084, Average Reward: -133.05\n",
      "Iteration 4085, Average Reward: -119.47\n",
      "Iteration 4086, Average Reward: -101.96\n",
      "Iteration 4087, Average Reward: -96.39\n",
      "Iteration 4088, Average Reward: -98.69\n",
      "Iteration 4089, Average Reward: -112.6\n",
      "Iteration 4090, Average Reward: -133.98\n",
      "Iteration 4091, Average Reward: -130.92\n",
      "Iteration 4092, Average Reward: -130.73\n",
      "Iteration 4093, Average Reward: -131.38\n",
      "Iteration 4094, Average Reward: -118.3\n",
      "Iteration 4095, Average Reward: -112.5\n",
      "Iteration 4096, Average Reward: -112.7\n",
      "Iteration 4097, Average Reward: -109.29\n",
      "Iteration 4098, Average Reward: -101.99\n",
      "Iteration 4099, Average Reward: -110.01\n",
      "Iteration 4100, Average Reward: -103.25\n",
      "Iteration 4101, Average Reward: -117.17\n",
      "Iteration 4102, Average Reward: -123.01\n",
      "Iteration 4103, Average Reward: -109.92\n",
      "Iteration 4104, Average Reward: -126.46\n",
      "Iteration 4105, Average Reward: -138.26\n",
      "Iteration 4106, Average Reward: -127.78\n",
      "Iteration 4107, Average Reward: -112.7\n",
      "Iteration 4108, Average Reward: -106.2\n",
      "Iteration 4109, Average Reward: -97.99\n",
      "Iteration 4110, Average Reward: -125.39\n",
      "Iteration 4111, Average Reward: -126.21\n",
      "Iteration 4112, Average Reward: -121.23\n",
      "Iteration 4113, Average Reward: -139.9\n",
      "Iteration 4114, Average Reward: -149.54\n",
      "Iteration 4115, Average Reward: -127.55\n",
      "Iteration 4116, Average Reward: -106.87\n",
      "Iteration 4117, Average Reward: -123.03\n",
      "Iteration 4118, Average Reward: -129.26\n",
      "Iteration 4119, Average Reward: -120.51\n",
      "Iteration 4120, Average Reward: -126.35\n",
      "Iteration 4121, Average Reward: -140.85\n",
      "Iteration 4122, Average Reward: -128.44\n",
      "Iteration 4123, Average Reward: -126.81\n",
      "Iteration 4124, Average Reward: -144.83\n",
      "Iteration 4125, Average Reward: -132.89\n",
      "Iteration 4126, Average Reward: -125.93\n",
      "Iteration 4127, Average Reward: -122.8\n",
      "Iteration 4128, Average Reward: -126.5\n",
      "Iteration 4129, Average Reward: -123.81\n",
      "Iteration 4130, Average Reward: -120.48\n",
      "Iteration 4131, Average Reward: -123.4\n",
      "Iteration 4132, Average Reward: -114.13\n",
      "Iteration 4133, Average Reward: -115.99\n",
      "Iteration 4134, Average Reward: -98.34\n",
      "Iteration 4135, Average Reward: -119.96\n",
      "Iteration 4136, Average Reward: -126.53\n",
      "Iteration 4137, Average Reward: -120.61\n",
      "Iteration 4138, Average Reward: -112.83\n",
      "Iteration 4139, Average Reward: -124.22\n",
      "Iteration 4140, Average Reward: -125.85\n",
      "Iteration 4141, Average Reward: -124.11\n",
      "Iteration 4142, Average Reward: -119.34\n",
      "Iteration 4143, Average Reward: -120.99\n",
      "Iteration 4144, Average Reward: -127.53\n",
      "Iteration 4145, Average Reward: -121.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4146, Average Reward: -125.65\n",
      "Iteration 4147, Average Reward: -131.74\n",
      "Iteration 4148, Average Reward: -134.08\n",
      "Iteration 4149, Average Reward: -131.27\n",
      "Iteration 4150, Average Reward: -127.49\n",
      "Iteration 4151, Average Reward: -121.42\n",
      "Iteration 4152, Average Reward: -119.1\n",
      "Iteration 4153, Average Reward: -123.53\n",
      "Iteration 4154, Average Reward: -126.82\n",
      "Iteration 4155, Average Reward: -121.6\n",
      "Iteration 4156, Average Reward: -112.1\n",
      "Iteration 4157, Average Reward: -122.76\n",
      "Iteration 4158, Average Reward: -119.08\n",
      "Iteration 4159, Average Reward: -126.51\n",
      "Iteration 4160, Average Reward: -126.98\n",
      "Iteration 4161, Average Reward: -127.9\n",
      "Iteration 4162, Average Reward: -122.72\n",
      "Iteration 4163, Average Reward: -106.56\n",
      "Iteration 4164, Average Reward: -115.46\n",
      "Iteration 4165, Average Reward: -118.61\n",
      "Iteration 4166, Average Reward: -134.49\n",
      "Iteration 4167, Average Reward: -142.44\n",
      "Iteration 4168, Average Reward: -133.73\n",
      "Iteration 4169, Average Reward: -130.84\n",
      "Iteration 4170, Average Reward: -124.77\n",
      "Iteration 4171, Average Reward: -120.77\n",
      "Iteration 4172, Average Reward: -129.87\n",
      "Iteration 4173, Average Reward: -117.06\n",
      "Iteration 4174, Average Reward: -120.54\n",
      "Iteration 4175, Average Reward: -129.45\n",
      "Iteration 4176, Average Reward: -130.59\n",
      "Iteration 4177, Average Reward: -127.44\n",
      "Iteration 4178, Average Reward: -123.02\n",
      "Iteration 4179, Average Reward: -122.7\n",
      "Iteration 4180, Average Reward: -119.96\n",
      "Iteration 4181, Average Reward: -123.89\n",
      "Iteration 4182, Average Reward: -136.15\n",
      "Iteration 4183, Average Reward: -130.12\n",
      "Iteration 4184, Average Reward: -125.26\n",
      "Iteration 4185, Average Reward: -139.54\n",
      "Iteration 4186, Average Reward: -132.75\n",
      "Iteration 4187, Average Reward: -118.87\n",
      "Iteration 4188, Average Reward: -109.53\n",
      "Iteration 4189, Average Reward: -121.13\n",
      "Iteration 4190, Average Reward: -120.45\n",
      "Iteration 4191, Average Reward: -115.82\n",
      "Iteration 4192, Average Reward: -134.51\n",
      "Iteration 4193, Average Reward: -141.39\n",
      "Iteration 4194, Average Reward: -137.94\n",
      "Iteration 4195, Average Reward: -123.41\n",
      "Iteration 4196, Average Reward: -129.45\n",
      "Iteration 4197, Average Reward: -122.84\n",
      "Iteration 4198, Average Reward: -116.13\n",
      "Iteration 4199, Average Reward: -108.2\n",
      "Iteration 4200, Average Reward: -105.8\n",
      "Iteration 4201, Average Reward: -108.85\n",
      "Iteration 4202, Average Reward: -115.29\n",
      "Iteration 4203, Average Reward: -131.09\n",
      "Iteration 4204, Average Reward: -148.15\n",
      "Iteration 4205, Average Reward: -139.31\n",
      "Iteration 4206, Average Reward: -130.72\n",
      "Iteration 4207, Average Reward: -118.18\n",
      "Iteration 4208, Average Reward: -118.17\n",
      "Iteration 4209, Average Reward: -119.01\n",
      "Iteration 4210, Average Reward: -132.96\n",
      "Iteration 4211, Average Reward: -131.26\n",
      "Iteration 4212, Average Reward: -129.47\n",
      "Iteration 4213, Average Reward: -123.35\n",
      "Iteration 4214, Average Reward: -135.05\n",
      "Iteration 4215, Average Reward: -137.55\n",
      "Iteration 4216, Average Reward: -127.87\n",
      "Iteration 4217, Average Reward: -109.87\n",
      "Iteration 4218, Average Reward: -115.74\n",
      "Iteration 4219, Average Reward: -113.92\n",
      "Iteration 4220, Average Reward: -103.97\n",
      "Iteration 4221, Average Reward: -99.6\n",
      "Iteration 4222, Average Reward: -120.27\n",
      "Iteration 4223, Average Reward: -126.25\n",
      "Iteration 4224, Average Reward: -134.62\n",
      "Iteration 4225, Average Reward: -151.07\n",
      "Iteration 4226, Average Reward: -141.51\n",
      "Iteration 4227, Average Reward: -130.71\n",
      "Iteration 4228, Average Reward: -113.57\n",
      "Iteration 4229, Average Reward: -112.11\n",
      "Iteration 4230, Average Reward: -111.7\n",
      "Iteration 4231, Average Reward: -90.12\n",
      "Iteration 4232, Average Reward: -91.96\n",
      "Iteration 4233, Average Reward: -129.78\n",
      "Iteration 4234, Average Reward: -120.52\n",
      "Iteration 4235, Average Reward: -115.26\n",
      "Iteration 4236, Average Reward: -125.87\n",
      "Iteration 4237, Average Reward: -123.06\n",
      "Iteration 4238, Average Reward: -129.7\n",
      "Iteration 4239, Average Reward: -120.54\n",
      "Iteration 4240, Average Reward: -120.25\n",
      "Iteration 4241, Average Reward: -132.96\n",
      "Iteration 4242, Average Reward: -124.61\n",
      "Iteration 4243, Average Reward: -108.02\n",
      "Iteration 4244, Average Reward: -111.0\n",
      "Iteration 4245, Average Reward: -100.19\n",
      "Iteration 4246, Average Reward: -109.45\n",
      "Iteration 4247, Average Reward: -135.24\n",
      "Iteration 4248, Average Reward: -136.84\n",
      "Iteration 4249, Average Reward: -128.6\n",
      "Iteration 4250, Average Reward: -136.25\n",
      "Iteration 4251, Average Reward: -116.66\n",
      "Iteration 4252, Average Reward: -108.53\n",
      "Iteration 4253, Average Reward: -98.9\n",
      "Iteration 4254, Average Reward: -109.79\n",
      "Iteration 4255, Average Reward: -127.06\n",
      "Iteration 4256, Average Reward: -132.74\n",
      "Iteration 4257, Average Reward: -138.79\n",
      "Iteration 4258, Average Reward: -148.31\n",
      "Iteration 4259, Average Reward: -133.76\n",
      "Iteration 4260, Average Reward: -114.47\n",
      "Iteration 4261, Average Reward: -99.38\n",
      "Iteration 4262, Average Reward: -110.08\n",
      "Iteration 4263, Average Reward: -103.84\n",
      "Iteration 4264, Average Reward: -96.43\n",
      "Iteration 4265, Average Reward: -124.27\n",
      "Iteration 4266, Average Reward: -125.35\n",
      "Iteration 4267, Average Reward: -136.95\n",
      "Iteration 4268, Average Reward: -139.71\n",
      "Iteration 4269, Average Reward: -139.56\n",
      "Iteration 4270, Average Reward: -128.84\n",
      "Iteration 4271, Average Reward: -108.89\n",
      "Iteration 4272, Average Reward: -103.81\n",
      "Iteration 4273, Average Reward: -106.96\n",
      "Iteration 4274, Average Reward: -105.9\n",
      "Iteration 4275, Average Reward: -121.98\n",
      "Iteration 4276, Average Reward: -126.63\n",
      "Iteration 4277, Average Reward: -135.18\n",
      "Iteration 4278, Average Reward: -135.37\n",
      "Iteration 4279, Average Reward: -140.21\n",
      "Iteration 4280, Average Reward: -126.67\n",
      "Iteration 4281, Average Reward: -128.69\n",
      "Iteration 4282, Average Reward: -121.83\n",
      "Iteration 4283, Average Reward: -122.08\n",
      "Iteration 4284, Average Reward: -122.8\n",
      "Iteration 4285, Average Reward: -128.85\n",
      "Iteration 4286, Average Reward: -123.22\n",
      "Iteration 4287, Average Reward: -120.35\n",
      "Iteration 4288, Average Reward: -128.55\n",
      "Iteration 4289, Average Reward: -121.81\n",
      "Iteration 4290, Average Reward: -120.77\n",
      "Iteration 4291, Average Reward: -118.63\n",
      "Iteration 4292, Average Reward: -127.65\n",
      "Iteration 4293, Average Reward: -121.9\n",
      "Iteration 4294, Average Reward: -119.56\n",
      "Iteration 4295, Average Reward: -121.08\n",
      "Iteration 4296, Average Reward: -125.46\n",
      "Iteration 4297, Average Reward: -134.26\n",
      "Iteration 4298, Average Reward: -131.83\n",
      "Iteration 4299, Average Reward: -118.48\n",
      "Iteration 4300, Average Reward: -118.95\n",
      "Iteration 4301, Average Reward: -110.56\n",
      "Iteration 4302, Average Reward: -122.22\n",
      "Iteration 4303, Average Reward: -112.13\n",
      "Iteration 4304, Average Reward: -99.77\n",
      "Iteration 4305, Average Reward: -107.32\n",
      "Iteration 4306, Average Reward: -122.86\n",
      "Iteration 4307, Average Reward: -121.38\n",
      "Iteration 4308, Average Reward: -119.81\n",
      "Iteration 4309, Average Reward: -123.38\n",
      "Iteration 4310, Average Reward: -107.11\n",
      "Iteration 4311, Average Reward: -113.09\n",
      "Iteration 4312, Average Reward: -110.5\n",
      "Iteration 4313, Average Reward: -113.04\n",
      "Iteration 4314, Average Reward: -114.78\n",
      "Iteration 4315, Average Reward: -106.18\n",
      "Iteration 4316, Average Reward: -118.24\n",
      "Iteration 4317, Average Reward: -119.32\n",
      "Iteration 4318, Average Reward: -107.06\n",
      "Iteration 4319, Average Reward: -125.92\n",
      "Iteration 4320, Average Reward: -143.22\n",
      "Iteration 4321, Average Reward: -139.82\n",
      "Iteration 4322, Average Reward: -127.17\n",
      "Iteration 4323, Average Reward: -121.28\n",
      "Iteration 4324, Average Reward: -134.26\n",
      "Iteration 4325, Average Reward: -142.32\n",
      "Iteration 4326, Average Reward: -136.56\n",
      "Iteration 4327, Average Reward: -150.23\n",
      "Iteration 4328, Average Reward: -136.56\n",
      "Iteration 4329, Average Reward: -117.96\n",
      "Iteration 4330, Average Reward: -116.41\n",
      "Iteration 4331, Average Reward: -128.0\n",
      "Iteration 4332, Average Reward: -112.96\n",
      "Iteration 4333, Average Reward: -117.96\n",
      "Iteration 4334, Average Reward: -122.33\n",
      "Iteration 4335, Average Reward: -128.77\n",
      "Iteration 4336, Average Reward: -131.61\n",
      "Iteration 4337, Average Reward: -139.81\n",
      "Iteration 4338, Average Reward: -134.44\n",
      "Iteration 4339, Average Reward: -126.44\n",
      "Iteration 4340, Average Reward: -125.58\n",
      "Iteration 4341, Average Reward: -136.33\n",
      "Iteration 4342, Average Reward: -123.9\n",
      "Iteration 4343, Average Reward: -119.21\n",
      "Iteration 4344, Average Reward: -127.28\n",
      "Iteration 4345, Average Reward: -116.27\n",
      "Iteration 4346, Average Reward: -116.91\n",
      "Iteration 4347, Average Reward: -120.14\n",
      "Iteration 4348, Average Reward: -116.36\n",
      "Iteration 4349, Average Reward: -129.36\n",
      "Iteration 4350, Average Reward: -128.42\n",
      "Iteration 4351, Average Reward: -135.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4352, Average Reward: -120.6\n",
      "Iteration 4353, Average Reward: -111.15\n",
      "Iteration 4354, Average Reward: -117.78\n",
      "Iteration 4355, Average Reward: -122.22\n",
      "Iteration 4356, Average Reward: -121.12\n",
      "Iteration 4357, Average Reward: -120.71\n",
      "Iteration 4358, Average Reward: -127.13\n",
      "Iteration 4359, Average Reward: -102.04\n",
      "Iteration 4360, Average Reward: -107.46\n",
      "Iteration 4361, Average Reward: -125.95\n",
      "Iteration 4362, Average Reward: -128.07\n",
      "Iteration 4363, Average Reward: -108.86\n",
      "Iteration 4364, Average Reward: -117.0\n",
      "Iteration 4365, Average Reward: -117.06\n",
      "Iteration 4366, Average Reward: -94.01\n",
      "Iteration 4367, Average Reward: -98.12\n",
      "Iteration 4368, Average Reward: -123.82\n",
      "Iteration 4369, Average Reward: -142.57\n",
      "Iteration 4370, Average Reward: -150.02\n",
      "Iteration 4371, Average Reward: -149.29\n",
      "Iteration 4372, Average Reward: -144.53\n",
      "Iteration 4373, Average Reward: -124.13\n",
      "Iteration 4374, Average Reward: -116.25\n",
      "Iteration 4375, Average Reward: -98.31\n",
      "Iteration 4376, Average Reward: -88.2\n",
      "Iteration 4377, Average Reward: -103.28\n",
      "Iteration 4378, Average Reward: -104.61\n",
      "Iteration 4379, Average Reward: -119.3\n",
      "Iteration 4380, Average Reward: -133.25\n",
      "Iteration 4381, Average Reward: -127.99\n",
      "Iteration 4382, Average Reward: -138.91\n",
      "Iteration 4383, Average Reward: -131.78\n",
      "Iteration 4384, Average Reward: -128.01\n",
      "Iteration 4385, Average Reward: -134.1\n",
      "Iteration 4386, Average Reward: -124.82\n",
      "Iteration 4387, Average Reward: -114.27\n",
      "Iteration 4388, Average Reward: -110.7\n",
      "Iteration 4389, Average Reward: -118.58\n",
      "Iteration 4390, Average Reward: -105.17\n",
      "Iteration 4391, Average Reward: -108.97\n",
      "Iteration 4392, Average Reward: -128.2\n",
      "Iteration 4393, Average Reward: -137.22\n",
      "Iteration 4394, Average Reward: -148.4\n",
      "Iteration 4395, Average Reward: -143.67\n",
      "Iteration 4396, Average Reward: -124.29\n",
      "Iteration 4397, Average Reward: -125.96\n",
      "Iteration 4398, Average Reward: -110.54\n",
      "Iteration 4399, Average Reward: -114.49\n",
      "Iteration 4400, Average Reward: -110.61\n",
      "Iteration 4401, Average Reward: -131.9\n",
      "Iteration 4402, Average Reward: -134.16\n",
      "Iteration 4403, Average Reward: -122.44\n",
      "Iteration 4404, Average Reward: -118.13\n",
      "Iteration 4405, Average Reward: -139.33\n",
      "Iteration 4406, Average Reward: -140.45\n",
      "Iteration 4407, Average Reward: -119.19\n",
      "Iteration 4408, Average Reward: -118.6\n",
      "Iteration 4409, Average Reward: -122.72\n",
      "Iteration 4410, Average Reward: -107.14\n",
      "Iteration 4411, Average Reward: -115.96\n",
      "Iteration 4412, Average Reward: -108.3\n",
      "Iteration 4413, Average Reward: -114.13\n",
      "Iteration 4414, Average Reward: -121.33\n",
      "Iteration 4415, Average Reward: -125.11\n",
      "Iteration 4416, Average Reward: -121.46\n",
      "Iteration 4417, Average Reward: -138.07\n",
      "Iteration 4418, Average Reward: -146.01\n",
      "Iteration 4419, Average Reward: -143.52\n",
      "Iteration 4420, Average Reward: -141.49\n",
      "Iteration 4421, Average Reward: -126.59\n",
      "Iteration 4422, Average Reward: -126.96\n",
      "Iteration 4423, Average Reward: -116.3\n",
      "Iteration 4424, Average Reward: -111.32\n",
      "Iteration 4425, Average Reward: -107.84\n",
      "Iteration 4426, Average Reward: -110.1\n",
      "Iteration 4427, Average Reward: -112.19\n",
      "Iteration 4428, Average Reward: -124.32\n",
      "Iteration 4429, Average Reward: -126.62\n",
      "Iteration 4430, Average Reward: -128.02\n",
      "Iteration 4431, Average Reward: -135.46\n",
      "Iteration 4432, Average Reward: -128.5\n",
      "Iteration 4433, Average Reward: -116.67\n",
      "Iteration 4434, Average Reward: -130.23\n",
      "Iteration 4435, Average Reward: -141.25\n",
      "Iteration 4436, Average Reward: -112.81\n",
      "Iteration 4437, Average Reward: -131.08\n",
      "Iteration 4438, Average Reward: -138.29\n",
      "Iteration 4439, Average Reward: -139.73\n",
      "Iteration 4440, Average Reward: -138.47\n",
      "Iteration 4441, Average Reward: -133.4\n",
      "Iteration 4442, Average Reward: -124.38\n",
      "Iteration 4443, Average Reward: -126.06\n",
      "Iteration 4444, Average Reward: -107.28\n",
      "Iteration 4445, Average Reward: -128.69\n",
      "Iteration 4446, Average Reward: -128.28\n",
      "Iteration 4447, Average Reward: -125.1\n",
      "Iteration 4448, Average Reward: -103.99\n",
      "Iteration 4449, Average Reward: -113.67\n",
      "Iteration 4450, Average Reward: -138.87\n",
      "Iteration 4451, Average Reward: -135.75\n",
      "Iteration 4452, Average Reward: -136.54\n",
      "Iteration 4453, Average Reward: -136.05\n",
      "Iteration 4454, Average Reward: -128.56\n",
      "Iteration 4455, Average Reward: -125.97\n",
      "Iteration 4456, Average Reward: -136.27\n",
      "Iteration 4457, Average Reward: -122.55\n",
      "Iteration 4458, Average Reward: -121.48\n",
      "Iteration 4459, Average Reward: -109.03\n",
      "Iteration 4460, Average Reward: -128.43\n",
      "Iteration 4461, Average Reward: -135.48\n",
      "Iteration 4462, Average Reward: -138.61\n",
      "Iteration 4463, Average Reward: -136.99\n",
      "Iteration 4464, Average Reward: -130.01\n",
      "Iteration 4465, Average Reward: -120.21\n",
      "Iteration 4466, Average Reward: -103.21\n",
      "Iteration 4467, Average Reward: -120.32\n",
      "Iteration 4468, Average Reward: -132.96\n",
      "Iteration 4469, Average Reward: -125.65\n",
      "Iteration 4470, Average Reward: -123.81\n",
      "Iteration 4471, Average Reward: -127.04\n",
      "Iteration 4472, Average Reward: -114.63\n",
      "Iteration 4473, Average Reward: -113.59\n",
      "Iteration 4474, Average Reward: -117.05\n",
      "Iteration 4475, Average Reward: -105.87\n",
      "Iteration 4476, Average Reward: -95.56\n",
      "Iteration 4477, Average Reward: -109.21\n",
      "Iteration 4478, Average Reward: -113.76\n",
      "Iteration 4479, Average Reward: -124.22\n",
      "Iteration 4480, Average Reward: -131.19\n",
      "Iteration 4481, Average Reward: -154.27\n",
      "Iteration 4482, Average Reward: -132.49\n",
      "Iteration 4483, Average Reward: -123.49\n",
      "Iteration 4484, Average Reward: -125.85\n",
      "Iteration 4485, Average Reward: -107.44\n",
      "Iteration 4486, Average Reward: -108.8\n",
      "Iteration 4487, Average Reward: -127.51\n",
      "Iteration 4488, Average Reward: -139.03\n",
      "Iteration 4489, Average Reward: -127.62\n",
      "Iteration 4490, Average Reward: -135.71\n",
      "Iteration 4491, Average Reward: -139.95\n",
      "Iteration 4492, Average Reward: -146.46\n",
      "Iteration 4493, Average Reward: -127.52\n",
      "Iteration 4494, Average Reward: -126.15\n",
      "Iteration 4495, Average Reward: -125.16\n",
      "Iteration 4496, Average Reward: -112.2\n",
      "Iteration 4497, Average Reward: -123.23\n",
      "Iteration 4498, Average Reward: -110.39\n",
      "Iteration 4499, Average Reward: -106.03\n",
      "Iteration 4500, Average Reward: -124.13\n",
      "Iteration 4501, Average Reward: -140.87\n",
      "Iteration 4502, Average Reward: -150.07\n",
      "Iteration 4503, Average Reward: -151.25\n",
      "Iteration 4504, Average Reward: -124.79\n",
      "Iteration 4505, Average Reward: -123.23\n",
      "Iteration 4506, Average Reward: -131.48\n",
      "Iteration 4507, Average Reward: -121.87\n",
      "Iteration 4508, Average Reward: -101.72\n",
      "Iteration 4509, Average Reward: -107.49\n",
      "Iteration 4510, Average Reward: -123.75\n",
      "Iteration 4511, Average Reward: -118.16\n",
      "Iteration 4512, Average Reward: -113.12\n",
      "Iteration 4513, Average Reward: -121.91\n",
      "Iteration 4514, Average Reward: -133.3\n",
      "Iteration 4515, Average Reward: -124.99\n",
      "Iteration 4516, Average Reward: -122.91\n",
      "Iteration 4517, Average Reward: -120.76\n",
      "Iteration 4518, Average Reward: -112.77\n",
      "Iteration 4519, Average Reward: -126.45\n",
      "Iteration 4520, Average Reward: -126.95\n",
      "Iteration 4521, Average Reward: -116.24\n",
      "Iteration 4522, Average Reward: -110.65\n",
      "Iteration 4523, Average Reward: -109.58\n",
      "Iteration 4524, Average Reward: -109.3\n",
      "Iteration 4525, Average Reward: -118.59\n",
      "Iteration 4526, Average Reward: -111.56\n",
      "Iteration 4527, Average Reward: -123.79\n",
      "Iteration 4528, Average Reward: -127.4\n",
      "Iteration 4529, Average Reward: -126.99\n",
      "Iteration 4530, Average Reward: -124.7\n",
      "Iteration 4531, Average Reward: -126.66\n",
      "Iteration 4532, Average Reward: -126.71\n",
      "Iteration 4533, Average Reward: -110.07\n",
      "Iteration 4534, Average Reward: -104.81\n",
      "Iteration 4535, Average Reward: -111.45\n",
      "Iteration 4536, Average Reward: -114.7\n",
      "Iteration 4537, Average Reward: -139.09\n",
      "Iteration 4538, Average Reward: -118.66\n",
      "Iteration 4539, Average Reward: -120.57\n",
      "Iteration 4540, Average Reward: -136.77\n",
      "Iteration 4541, Average Reward: -129.65\n",
      "Iteration 4542, Average Reward: -121.91\n",
      "Iteration 4543, Average Reward: -133.5\n",
      "Iteration 4544, Average Reward: -119.4\n",
      "Iteration 4545, Average Reward: -105.01\n",
      "Iteration 4546, Average Reward: -109.03\n",
      "Iteration 4547, Average Reward: -106.45\n",
      "Iteration 4548, Average Reward: -102.69\n",
      "Iteration 4549, Average Reward: -119.34\n",
      "Iteration 4550, Average Reward: -128.12\n",
      "Iteration 4551, Average Reward: -130.3\n",
      "Iteration 4552, Average Reward: -141.44\n",
      "Iteration 4553, Average Reward: -128.62\n",
      "Iteration 4554, Average Reward: -137.88\n",
      "Iteration 4555, Average Reward: -122.62\n",
      "Iteration 4556, Average Reward: -118.81\n",
      "Iteration 4557, Average Reward: -109.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4558, Average Reward: -110.86\n",
      "Iteration 4559, Average Reward: -115.15\n",
      "Iteration 4560, Average Reward: -123.94\n",
      "Iteration 4561, Average Reward: -118.11\n",
      "Iteration 4562, Average Reward: -132.01\n",
      "Iteration 4563, Average Reward: -142.6\n",
      "Iteration 4564, Average Reward: -117.06\n",
      "Iteration 4565, Average Reward: -129.42\n",
      "Iteration 4566, Average Reward: -133.11\n",
      "Iteration 4567, Average Reward: -115.54\n",
      "Iteration 4568, Average Reward: -108.76\n",
      "Iteration 4569, Average Reward: -111.89\n",
      "Iteration 4570, Average Reward: -111.7\n",
      "Iteration 4571, Average Reward: -104.27\n",
      "Iteration 4572, Average Reward: -111.4\n",
      "Iteration 4573, Average Reward: -114.04\n",
      "Iteration 4574, Average Reward: -124.85\n",
      "Iteration 4575, Average Reward: -130.68\n",
      "Iteration 4576, Average Reward: -141.39\n",
      "Iteration 4577, Average Reward: -121.24\n",
      "Iteration 4578, Average Reward: -115.88\n",
      "Iteration 4579, Average Reward: -124.16\n",
      "Iteration 4580, Average Reward: -118.67\n",
      "Iteration 4581, Average Reward: -134.35\n",
      "Iteration 4582, Average Reward: -128.19\n",
      "Iteration 4583, Average Reward: -127.43\n",
      "Iteration 4584, Average Reward: -121.42\n",
      "Iteration 4585, Average Reward: -116.4\n",
      "Iteration 4586, Average Reward: -109.42\n",
      "Iteration 4587, Average Reward: -116.54\n",
      "Iteration 4588, Average Reward: -119.15\n",
      "Iteration 4589, Average Reward: -123.63\n",
      "Iteration 4590, Average Reward: -124.42\n",
      "Iteration 4591, Average Reward: -142.13\n",
      "Iteration 4592, Average Reward: -140.42\n",
      "Iteration 4593, Average Reward: -131.91\n",
      "Iteration 4594, Average Reward: -114.25\n",
      "Iteration 4595, Average Reward: -97.94\n",
      "Iteration 4596, Average Reward: -88.85\n",
      "Iteration 4597, Average Reward: -99.74\n",
      "Iteration 4598, Average Reward: -107.96\n",
      "Iteration 4599, Average Reward: -116.01\n",
      "Iteration 4600, Average Reward: -134.82\n",
      "Iteration 4601, Average Reward: -157.09\n",
      "Iteration 4602, Average Reward: -144.07\n",
      "Iteration 4603, Average Reward: -146.46\n",
      "Iteration 4604, Average Reward: -137.15\n",
      "Iteration 4605, Average Reward: -111.22\n",
      "Iteration 4606, Average Reward: -106.69\n",
      "Iteration 4607, Average Reward: -98.29\n",
      "Iteration 4608, Average Reward: -100.96\n",
      "Iteration 4609, Average Reward: -124.88\n",
      "Iteration 4610, Average Reward: -125.68\n",
      "Iteration 4611, Average Reward: -115.14\n",
      "Iteration 4612, Average Reward: -123.32\n",
      "Iteration 4613, Average Reward: -128.88\n",
      "Iteration 4614, Average Reward: -141.95\n",
      "Iteration 4615, Average Reward: -125.89\n",
      "Iteration 4616, Average Reward: -127.77\n",
      "Iteration 4617, Average Reward: -119.3\n",
      "Iteration 4618, Average Reward: -116.71\n",
      "Iteration 4619, Average Reward: -111.77\n",
      "Iteration 4620, Average Reward: -118.22\n",
      "Iteration 4621, Average Reward: -134.27\n",
      "Iteration 4622, Average Reward: -135.29\n",
      "Iteration 4623, Average Reward: -131.78\n",
      "Iteration 4624, Average Reward: -139.71\n",
      "Iteration 4625, Average Reward: -131.4\n",
      "Iteration 4626, Average Reward: -133.27\n",
      "Iteration 4627, Average Reward: -125.77\n",
      "Iteration 4628, Average Reward: -94.83\n",
      "Iteration 4629, Average Reward: -99.75\n",
      "Iteration 4630, Average Reward: -110.0\n",
      "Iteration 4631, Average Reward: -113.36\n",
      "Iteration 4632, Average Reward: -111.21\n",
      "Iteration 4633, Average Reward: -113.28\n",
      "Iteration 4634, Average Reward: -133.28\n",
      "Iteration 4635, Average Reward: -113.57\n",
      "Iteration 4636, Average Reward: -119.72\n",
      "Iteration 4637, Average Reward: -133.31\n",
      "Iteration 4638, Average Reward: -133.32\n",
      "Iteration 4639, Average Reward: -118.43\n",
      "Iteration 4640, Average Reward: -133.71\n",
      "Iteration 4641, Average Reward: -114.03\n",
      "Iteration 4642, Average Reward: -125.79\n",
      "Iteration 4643, Average Reward: -112.9\n",
      "Iteration 4644, Average Reward: -129.6\n",
      "Iteration 4645, Average Reward: -122.39\n",
      "Iteration 4646, Average Reward: -127.92\n",
      "Iteration 4647, Average Reward: -128.26\n",
      "Iteration 4648, Average Reward: -118.98\n",
      "Iteration 4649, Average Reward: -141.35\n",
      "Iteration 4650, Average Reward: -145.95\n",
      "Iteration 4651, Average Reward: -140.38\n",
      "Iteration 4652, Average Reward: -130.23\n",
      "Iteration 4653, Average Reward: -118.63\n",
      "Iteration 4654, Average Reward: -123.25\n",
      "Iteration 4655, Average Reward: -121.33\n",
      "Iteration 4656, Average Reward: -113.65\n",
      "Iteration 4657, Average Reward: -121.53\n",
      "Iteration 4658, Average Reward: -128.7\n",
      "Iteration 4659, Average Reward: -124.17\n",
      "Iteration 4660, Average Reward: -118.97\n",
      "Iteration 4661, Average Reward: -107.7\n",
      "Iteration 4662, Average Reward: -119.24\n",
      "Iteration 4663, Average Reward: -127.94\n",
      "Iteration 4664, Average Reward: -124.31\n",
      "Iteration 4665, Average Reward: -124.41\n",
      "Iteration 4666, Average Reward: -123.28\n",
      "Iteration 4667, Average Reward: -112.23\n",
      "Iteration 4668, Average Reward: -108.33\n",
      "Iteration 4669, Average Reward: -119.0\n",
      "Iteration 4670, Average Reward: -116.1\n",
      "Iteration 4671, Average Reward: -118.43\n",
      "Iteration 4672, Average Reward: -126.89\n",
      "Iteration 4673, Average Reward: -138.55\n",
      "Iteration 4674, Average Reward: -126.23\n",
      "Iteration 4675, Average Reward: -130.93\n",
      "Iteration 4676, Average Reward: -138.5\n",
      "Iteration 4677, Average Reward: -145.38\n",
      "Iteration 4678, Average Reward: -129.41\n",
      "Iteration 4679, Average Reward: -117.73\n",
      "Iteration 4680, Average Reward: -112.62\n",
      "Iteration 4681, Average Reward: -140.78\n",
      "Iteration 4682, Average Reward: -145.11\n",
      "Iteration 4683, Average Reward: -147.0\n",
      "Iteration 4684, Average Reward: -152.56\n",
      "Iteration 4685, Average Reward: -142.31\n",
      "Iteration 4686, Average Reward: -124.79\n",
      "Iteration 4687, Average Reward: -99.85\n",
      "Iteration 4688, Average Reward: -118.46\n",
      "Iteration 4689, Average Reward: -136.66\n",
      "Iteration 4690, Average Reward: -132.57\n",
      "Iteration 4691, Average Reward: -131.1\n",
      "Iteration 4692, Average Reward: -132.93\n",
      "Iteration 4693, Average Reward: -127.76\n",
      "Iteration 4694, Average Reward: -107.75\n",
      "Iteration 4695, Average Reward: -117.95\n",
      "Iteration 4696, Average Reward: -118.1\n",
      "Iteration 4697, Average Reward: -119.54\n",
      "Iteration 4698, Average Reward: -113.26\n",
      "Iteration 4699, Average Reward: -126.7\n",
      "Iteration 4700, Average Reward: -128.5\n",
      "Iteration 4701, Average Reward: -124.51\n",
      "Iteration 4702, Average Reward: -118.63\n",
      "Iteration 4703, Average Reward: -136.85\n",
      "Iteration 4704, Average Reward: -119.91\n",
      "Iteration 4705, Average Reward: -127.09\n",
      "Iteration 4706, Average Reward: -131.41\n",
      "Iteration 4707, Average Reward: -128.23\n",
      "Iteration 4708, Average Reward: -99.3\n",
      "Iteration 4709, Average Reward: -111.9\n",
      "Iteration 4710, Average Reward: -116.53\n",
      "Iteration 4711, Average Reward: -126.73\n",
      "Iteration 4712, Average Reward: -112.14\n",
      "Iteration 4713, Average Reward: -124.8\n",
      "Iteration 4714, Average Reward: -129.1\n",
      "Iteration 4715, Average Reward: -119.3\n",
      "Iteration 4716, Average Reward: -102.96\n",
      "Iteration 4717, Average Reward: -105.37\n",
      "Iteration 4718, Average Reward: -125.08\n",
      "Iteration 4719, Average Reward: -113.26\n",
      "Iteration 4720, Average Reward: -129.09\n",
      "Iteration 4721, Average Reward: -116.07\n",
      "Iteration 4722, Average Reward: -125.5\n",
      "Iteration 4723, Average Reward: -136.21\n",
      "Iteration 4724, Average Reward: -135.22\n",
      "Iteration 4725, Average Reward: -138.57\n",
      "Iteration 4726, Average Reward: -126.46\n",
      "Iteration 4727, Average Reward: -129.03\n",
      "Iteration 4728, Average Reward: -113.59\n",
      "Iteration 4729, Average Reward: -128.85\n",
      "Iteration 4730, Average Reward: -138.71\n",
      "Iteration 4731, Average Reward: -141.05\n",
      "Iteration 4732, Average Reward: -139.23\n",
      "Iteration 4733, Average Reward: -137.54\n",
      "Iteration 4734, Average Reward: -120.76\n",
      "Iteration 4735, Average Reward: -121.49\n",
      "Iteration 4736, Average Reward: -116.12\n",
      "Iteration 4737, Average Reward: -118.5\n",
      "Iteration 4738, Average Reward: -116.69\n",
      "Iteration 4739, Average Reward: -124.18\n",
      "Iteration 4740, Average Reward: -121.91\n",
      "Iteration 4741, Average Reward: -117.13\n",
      "Iteration 4742, Average Reward: -105.52\n",
      "Iteration 4743, Average Reward: -129.59\n",
      "Iteration 4744, Average Reward: -121.49\n",
      "Iteration 4745, Average Reward: -109.14\n",
      "Iteration 4746, Average Reward: -108.62\n",
      "Iteration 4747, Average Reward: -124.44\n",
      "Iteration 4748, Average Reward: -118.73\n",
      "Iteration 4749, Average Reward: -126.85\n",
      "Iteration 4750, Average Reward: -125.51\n",
      "Iteration 4751, Average Reward: -102.94\n",
      "Iteration 4752, Average Reward: -104.83\n",
      "Iteration 4753, Average Reward: -122.8\n",
      "Iteration 4754, Average Reward: -132.77\n",
      "Iteration 4755, Average Reward: -120.85\n",
      "Iteration 4756, Average Reward: -123.83\n",
      "Iteration 4757, Average Reward: -133.19\n",
      "Iteration 4758, Average Reward: -117.21\n",
      "Iteration 4759, Average Reward: -111.28\n",
      "Iteration 4760, Average Reward: -112.26\n",
      "Iteration 4761, Average Reward: -98.98\n",
      "Iteration 4762, Average Reward: -104.83\n",
      "Iteration 4763, Average Reward: -122.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4764, Average Reward: -136.81\n",
      "Iteration 4765, Average Reward: -140.06\n",
      "Iteration 4766, Average Reward: -133.75\n",
      "Iteration 4767, Average Reward: -110.09\n",
      "Iteration 4768, Average Reward: -131.57\n",
      "Iteration 4769, Average Reward: -131.86\n",
      "Iteration 4770, Average Reward: -122.48\n",
      "Iteration 4771, Average Reward: -117.99\n",
      "Iteration 4772, Average Reward: -124.69\n",
      "Iteration 4773, Average Reward: -112.6\n",
      "Iteration 4774, Average Reward: -114.64\n",
      "Iteration 4775, Average Reward: -109.71\n",
      "Iteration 4776, Average Reward: -137.04\n",
      "Iteration 4777, Average Reward: -137.5\n",
      "Iteration 4778, Average Reward: -138.47\n",
      "Iteration 4779, Average Reward: -132.25\n",
      "Iteration 4780, Average Reward: -144.28\n",
      "Iteration 4781, Average Reward: -119.05\n",
      "Iteration 4782, Average Reward: -108.53\n",
      "Iteration 4783, Average Reward: -101.25\n",
      "Iteration 4784, Average Reward: -128.58\n",
      "Iteration 4785, Average Reward: -130.15\n",
      "Iteration 4786, Average Reward: -113.2\n",
      "Iteration 4787, Average Reward: -104.88\n",
      "Iteration 4788, Average Reward: -108.66\n",
      "Iteration 4789, Average Reward: -129.84\n",
      "Iteration 4790, Average Reward: -120.04\n",
      "Iteration 4791, Average Reward: -116.74\n",
      "Iteration 4792, Average Reward: -122.86\n",
      "Iteration 4793, Average Reward: -114.03\n",
      "Iteration 4794, Average Reward: -114.48\n",
      "Iteration 4795, Average Reward: -112.95\n",
      "Iteration 4796, Average Reward: -116.47\n",
      "Iteration 4797, Average Reward: -131.04\n",
      "Iteration 4798, Average Reward: -124.08\n",
      "Iteration 4799, Average Reward: -118.2\n",
      "Iteration 4800, Average Reward: -121.32\n",
      "Iteration 4801, Average Reward: -106.56\n",
      "Iteration 4802, Average Reward: -108.92\n",
      "Iteration 4803, Average Reward: -121.34\n",
      "Iteration 4804, Average Reward: -125.76\n",
      "Iteration 4805, Average Reward: -122.62\n",
      "Iteration 4806, Average Reward: -123.0\n",
      "Iteration 4807, Average Reward: -120.01\n",
      "Iteration 4808, Average Reward: -112.35\n",
      "Iteration 4809, Average Reward: -115.3\n",
      "Iteration 4810, Average Reward: -118.32\n",
      "Iteration 4811, Average Reward: -125.68\n",
      "Iteration 4812, Average Reward: -129.17\n",
      "Iteration 4813, Average Reward: -137.4\n",
      "Iteration 4814, Average Reward: -135.94\n",
      "Iteration 4815, Average Reward: -142.2\n",
      "Iteration 4816, Average Reward: -129.44\n",
      "Iteration 4817, Average Reward: -126.4\n",
      "Iteration 4818, Average Reward: -130.49\n",
      "Iteration 4819, Average Reward: -130.43\n",
      "Iteration 4820, Average Reward: -126.72\n",
      "Iteration 4821, Average Reward: -128.35\n",
      "Iteration 4822, Average Reward: -131.41\n",
      "Iteration 4823, Average Reward: -108.24\n",
      "Iteration 4824, Average Reward: -100.02\n",
      "Iteration 4825, Average Reward: -119.31\n",
      "Iteration 4826, Average Reward: -124.2\n",
      "Iteration 4827, Average Reward: -103.02\n",
      "Iteration 4828, Average Reward: -118.01\n",
      "Iteration 4829, Average Reward: -143.57\n",
      "Iteration 4830, Average Reward: -132.04\n",
      "Iteration 4831, Average Reward: -118.83\n",
      "Iteration 4832, Average Reward: -117.11\n",
      "Iteration 4833, Average Reward: -104.07\n",
      "Iteration 4834, Average Reward: -99.5\n",
      "Iteration 4835, Average Reward: -109.21\n",
      "Iteration 4836, Average Reward: -124.67\n",
      "Iteration 4837, Average Reward: -120.78\n",
      "Iteration 4838, Average Reward: -124.49\n",
      "Iteration 4839, Average Reward: -136.82\n",
      "Iteration 4840, Average Reward: -130.64\n",
      "Iteration 4841, Average Reward: -128.56\n",
      "Iteration 4842, Average Reward: -129.28\n",
      "Iteration 4843, Average Reward: -123.75\n",
      "Iteration 4844, Average Reward: -113.79\n",
      "Iteration 4845, Average Reward: -117.82\n",
      "Iteration 4846, Average Reward: -136.31\n",
      "Iteration 4847, Average Reward: -132.87\n",
      "Iteration 4848, Average Reward: -130.42\n",
      "Iteration 4849, Average Reward: -132.54\n",
      "Iteration 4850, Average Reward: -130.87\n",
      "Iteration 4851, Average Reward: -117.82\n",
      "Iteration 4852, Average Reward: -110.9\n",
      "Iteration 4853, Average Reward: -118.59\n",
      "Iteration 4854, Average Reward: -133.3\n",
      "Iteration 4855, Average Reward: -128.54\n",
      "Iteration 4856, Average Reward: -126.07\n",
      "Iteration 4857, Average Reward: -128.51\n",
      "Iteration 4858, Average Reward: -118.88\n",
      "Iteration 4859, Average Reward: -107.33\n",
      "Iteration 4860, Average Reward: -123.93\n",
      "Iteration 4861, Average Reward: -141.34\n",
      "Iteration 4862, Average Reward: -135.55\n",
      "Iteration 4863, Average Reward: -134.52\n",
      "Iteration 4864, Average Reward: -130.56\n",
      "Iteration 4865, Average Reward: -144.02\n",
      "Iteration 4866, Average Reward: -131.63\n",
      "Iteration 4867, Average Reward: -135.14\n",
      "Iteration 4868, Average Reward: -137.56\n",
      "Iteration 4869, Average Reward: -122.58\n",
      "Iteration 4870, Average Reward: -121.96\n",
      "Iteration 4871, Average Reward: -119.68\n",
      "Iteration 4872, Average Reward: -129.01\n",
      "Iteration 4873, Average Reward: -134.04\n",
      "Iteration 4874, Average Reward: -134.45\n",
      "Iteration 4875, Average Reward: -134.32\n",
      "Iteration 4876, Average Reward: -123.17\n",
      "Iteration 4877, Average Reward: -119.13\n",
      "Iteration 4878, Average Reward: -129.74\n",
      "Iteration 4879, Average Reward: -134.41\n",
      "Iteration 4880, Average Reward: -139.23\n",
      "Iteration 4881, Average Reward: -123.69\n",
      "Iteration 4882, Average Reward: -120.7\n",
      "Iteration 4883, Average Reward: -124.73\n",
      "Iteration 4884, Average Reward: -122.29\n",
      "Iteration 4885, Average Reward: -132.35\n",
      "Iteration 4886, Average Reward: -132.78\n",
      "Iteration 4887, Average Reward: -137.94\n",
      "Iteration 4888, Average Reward: -143.09\n",
      "Iteration 4889, Average Reward: -136.8\n",
      "Iteration 4890, Average Reward: -147.6\n",
      "Iteration 4891, Average Reward: -139.11\n",
      "Iteration 4892, Average Reward: -131.85\n",
      "Iteration 4893, Average Reward: -112.36\n",
      "Iteration 4894, Average Reward: -127.69\n",
      "Iteration 4895, Average Reward: -120.0\n",
      "Iteration 4896, Average Reward: -109.61\n",
      "Iteration 4897, Average Reward: -110.3\n",
      "Iteration 4898, Average Reward: -119.9\n",
      "Iteration 4899, Average Reward: -119.07\n",
      "Iteration 4900, Average Reward: -115.61\n",
      "Iteration 4901, Average Reward: -127.45\n",
      "Iteration 4902, Average Reward: -140.04\n",
      "Iteration 4903, Average Reward: -134.67\n",
      "Iteration 4904, Average Reward: -127.22\n",
      "Iteration 4905, Average Reward: -120.57\n",
      "Iteration 4906, Average Reward: -120.76\n",
      "Iteration 4907, Average Reward: -133.21\n",
      "Iteration 4908, Average Reward: -136.28\n",
      "Iteration 4909, Average Reward: -138.16\n",
      "Iteration 4910, Average Reward: -120.96\n",
      "Iteration 4911, Average Reward: -130.77\n",
      "Iteration 4912, Average Reward: -117.78\n",
      "Iteration 4913, Average Reward: -92.82\n",
      "Iteration 4914, Average Reward: -109.85\n",
      "Iteration 4915, Average Reward: -116.07\n",
      "Iteration 4916, Average Reward: -127.33\n",
      "Iteration 4917, Average Reward: -139.61\n",
      "Iteration 4918, Average Reward: -149.31\n",
      "Iteration 4919, Average Reward: -134.2\n",
      "Iteration 4920, Average Reward: -125.79\n",
      "Iteration 4921, Average Reward: -135.7\n",
      "Iteration 4922, Average Reward: -128.22\n",
      "Iteration 4923, Average Reward: -119.14\n",
      "Iteration 4924, Average Reward: -118.95\n",
      "Iteration 4925, Average Reward: -126.77\n",
      "Iteration 4926, Average Reward: -134.14\n",
      "Iteration 4927, Average Reward: -121.17\n",
      "Iteration 4928, Average Reward: -118.51\n",
      "Iteration 4929, Average Reward: -113.45\n",
      "Iteration 4930, Average Reward: -125.9\n",
      "Iteration 4931, Average Reward: -124.45\n",
      "Iteration 4932, Average Reward: -122.05\n",
      "Iteration 4933, Average Reward: -137.26\n",
      "Iteration 4934, Average Reward: -129.88\n",
      "Iteration 4935, Average Reward: -120.05\n",
      "Iteration 4936, Average Reward: -116.4\n",
      "Iteration 4937, Average Reward: -117.97\n",
      "Iteration 4938, Average Reward: -123.83\n",
      "Iteration 4939, Average Reward: -135.69\n",
      "Iteration 4940, Average Reward: -127.95\n",
      "Iteration 4941, Average Reward: -114.38\n",
      "Iteration 4942, Average Reward: -120.92\n",
      "Iteration 4943, Average Reward: -123.65\n",
      "Iteration 4944, Average Reward: -118.6\n",
      "Iteration 4945, Average Reward: -124.36\n",
      "Iteration 4946, Average Reward: -123.16\n",
      "Iteration 4947, Average Reward: -130.79\n",
      "Iteration 4948, Average Reward: -131.09\n",
      "Iteration 4949, Average Reward: -122.93\n",
      "Iteration 4950, Average Reward: -124.21\n",
      "Iteration 4951, Average Reward: -122.65\n",
      "Iteration 4952, Average Reward: -118.72\n",
      "Iteration 4953, Average Reward: -135.42\n",
      "Iteration 4954, Average Reward: -142.05\n",
      "Iteration 4955, Average Reward: -128.56\n",
      "Iteration 4956, Average Reward: -141.39\n",
      "Iteration 4957, Average Reward: -139.06\n",
      "Iteration 4958, Average Reward: -108.83\n",
      "Iteration 4959, Average Reward: -110.93\n",
      "Iteration 4960, Average Reward: -115.25\n",
      "Iteration 4961, Average Reward: -120.5\n",
      "Iteration 4962, Average Reward: -113.88\n",
      "Iteration 4963, Average Reward: -93.19\n",
      "Iteration 4964, Average Reward: -111.61\n",
      "Iteration 4965, Average Reward: -101.32\n",
      "Iteration 4966, Average Reward: -113.54\n",
      "Iteration 4967, Average Reward: -92.76\n",
      "Iteration 4968, Average Reward: -108.56\n",
      "Iteration 4969, Average Reward: -131.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4970, Average Reward: -107.68\n",
      "Iteration 4971, Average Reward: -116.99\n",
      "Iteration 4972, Average Reward: -116.84\n",
      "Iteration 4973, Average Reward: -109.86\n",
      "Iteration 4974, Average Reward: -115.77\n",
      "Iteration 4975, Average Reward: -139.63\n",
      "Iteration 4976, Average Reward: -137.44\n",
      "Iteration 4977, Average Reward: -113.55\n",
      "Iteration 4978, Average Reward: -124.71\n",
      "Iteration 4979, Average Reward: -137.91\n",
      "Iteration 4980, Average Reward: -120.61\n",
      "Iteration 4981, Average Reward: -99.45\n",
      "Iteration 4982, Average Reward: -103.94\n",
      "Iteration 4983, Average Reward: -108.84\n",
      "Iteration 4984, Average Reward: -115.03\n",
      "Iteration 4985, Average Reward: -116.35\n",
      "Iteration 4986, Average Reward: -124.14\n",
      "Iteration 4987, Average Reward: -124.55\n",
      "Iteration 4988, Average Reward: -133.49\n",
      "Iteration 4989, Average Reward: -152.75\n",
      "Iteration 4990, Average Reward: -146.02\n",
      "Iteration 4991, Average Reward: -125.53\n",
      "Iteration 4992, Average Reward: -140.89\n",
      "Iteration 4993, Average Reward: -139.37\n",
      "Iteration 4994, Average Reward: -122.52\n",
      "Iteration 4995, Average Reward: -110.66\n",
      "Iteration 4996, Average Reward: -119.41\n",
      "Iteration 4997, Average Reward: -121.35\n",
      "Iteration 4998, Average Reward: -115.29\n",
      "Iteration 4999, Average Reward: -119.41\n",
      "Iteration 5000, Average Reward: -131.37\n",
      "Iteration 5001, Average Reward: -135.0\n",
      "Iteration 5002, Average Reward: -135.22\n",
      "Iteration 5003, Average Reward: -131.28\n",
      "Iteration 5004, Average Reward: -116.98\n",
      "Iteration 5005, Average Reward: -105.21\n",
      "Iteration 5006, Average Reward: -113.01\n",
      "Iteration 5007, Average Reward: -117.46\n",
      "Iteration 5008, Average Reward: -136.14\n",
      "Iteration 5009, Average Reward: -142.39\n",
      "Iteration 5010, Average Reward: -130.29\n",
      "Iteration 5011, Average Reward: -127.77\n",
      "Iteration 5012, Average Reward: -137.25\n",
      "Iteration 5013, Average Reward: -130.19\n",
      "Iteration 5014, Average Reward: -114.84\n",
      "Iteration 5015, Average Reward: -116.45\n",
      "Iteration 5016, Average Reward: -122.46\n",
      "Iteration 5017, Average Reward: -120.59\n",
      "Iteration 5018, Average Reward: -121.39\n",
      "Iteration 5019, Average Reward: -121.81\n",
      "Iteration 5020, Average Reward: -109.78\n",
      "Iteration 5021, Average Reward: -112.87\n",
      "Iteration 5022, Average Reward: -128.63\n",
      "Iteration 5023, Average Reward: -123.43\n",
      "Iteration 5024, Average Reward: -116.5\n",
      "Iteration 5025, Average Reward: -122.08\n",
      "Iteration 5026, Average Reward: -128.61\n",
      "Iteration 5027, Average Reward: -120.01\n",
      "Iteration 5028, Average Reward: -141.27\n",
      "Iteration 5029, Average Reward: -132.81\n",
      "Iteration 5030, Average Reward: -142.39\n",
      "Iteration 5031, Average Reward: -152.41\n",
      "Iteration 5032, Average Reward: -130.54\n",
      "Iteration 5033, Average Reward: -118.56\n",
      "Iteration 5034, Average Reward: -117.04\n",
      "Iteration 5035, Average Reward: -123.69\n",
      "Iteration 5036, Average Reward: -125.7\n",
      "Iteration 5037, Average Reward: -127.7\n",
      "Iteration 5038, Average Reward: -142.7\n",
      "Iteration 5039, Average Reward: -139.49\n",
      "Iteration 5040, Average Reward: -126.85\n",
      "Iteration 5041, Average Reward: -118.38\n",
      "Iteration 5042, Average Reward: -125.31\n",
      "Iteration 5043, Average Reward: -122.27\n",
      "Iteration 5044, Average Reward: -113.46\n",
      "Iteration 5045, Average Reward: -139.92\n",
      "Iteration 5046, Average Reward: -140.38\n",
      "Iteration 5047, Average Reward: -136.54\n",
      "Iteration 5048, Average Reward: -135.51\n",
      "Iteration 5049, Average Reward: -131.96\n",
      "Iteration 5050, Average Reward: -125.9\n",
      "Iteration 5051, Average Reward: -134.27\n",
      "Iteration 5052, Average Reward: -104.43\n",
      "Iteration 5053, Average Reward: -91.52\n",
      "Iteration 5054, Average Reward: -124.2\n",
      "Iteration 5055, Average Reward: -132.25\n",
      "Iteration 5056, Average Reward: -117.84\n",
      "Iteration 5057, Average Reward: -112.3\n",
      "Iteration 5058, Average Reward: -132.53\n",
      "Iteration 5059, Average Reward: -136.26\n",
      "Iteration 5060, Average Reward: -138.18\n",
      "Iteration 5061, Average Reward: -143.66\n",
      "Iteration 5062, Average Reward: -145.03\n",
      "Iteration 5063, Average Reward: -128.31\n",
      "Iteration 5064, Average Reward: -123.86\n",
      "Iteration 5065, Average Reward: -117.42\n",
      "Iteration 5066, Average Reward: -105.95\n",
      "Iteration 5067, Average Reward: -107.87\n",
      "Iteration 5068, Average Reward: -91.92\n",
      "Iteration 5069, Average Reward: -116.45\n",
      "Iteration 5070, Average Reward: -138.92\n",
      "Iteration 5071, Average Reward: -128.89\n",
      "Iteration 5072, Average Reward: -135.07\n",
      "Iteration 5073, Average Reward: -138.29\n",
      "Iteration 5074, Average Reward: -109.55\n",
      "Iteration 5075, Average Reward: -115.85\n",
      "Iteration 5076, Average Reward: -121.69\n",
      "Iteration 5077, Average Reward: -114.07\n",
      "Iteration 5078, Average Reward: -127.38\n",
      "Iteration 5079, Average Reward: -120.94\n",
      "Iteration 5080, Average Reward: -130.08\n",
      "Iteration 5081, Average Reward: -116.45\n",
      "Iteration 5082, Average Reward: -114.61\n",
      "Iteration 5083, Average Reward: -124.94\n",
      "Iteration 5084, Average Reward: -128.24\n",
      "Iteration 5085, Average Reward: -126.55\n",
      "Iteration 5086, Average Reward: -122.39\n",
      "Iteration 5087, Average Reward: -120.79\n",
      "Iteration 5088, Average Reward: -112.22\n",
      "Iteration 5089, Average Reward: -115.69\n",
      "Iteration 5090, Average Reward: -117.97\n",
      "Iteration 5091, Average Reward: -125.25\n",
      "Iteration 5092, Average Reward: -131.23\n",
      "Iteration 5093, Average Reward: -124.71\n",
      "Iteration 5094, Average Reward: -112.1\n",
      "Iteration 5095, Average Reward: -108.18\n",
      "Iteration 5096, Average Reward: -114.5\n",
      "Iteration 5097, Average Reward: -127.42\n",
      "Iteration 5098, Average Reward: -118.67\n",
      "Iteration 5099, Average Reward: -130.66\n",
      "Iteration 5100, Average Reward: -131.13\n",
      "Iteration 5101, Average Reward: -134.72\n",
      "Iteration 5102, Average Reward: -120.55\n",
      "Iteration 5103, Average Reward: -110.22\n",
      "Iteration 5104, Average Reward: -108.9\n",
      "Iteration 5105, Average Reward: -108.3\n",
      "Iteration 5106, Average Reward: -117.74\n",
      "Iteration 5107, Average Reward: -128.0\n",
      "Iteration 5108, Average Reward: -107.57\n",
      "Iteration 5109, Average Reward: -111.67\n",
      "Iteration 5110, Average Reward: -135.34\n",
      "Iteration 5111, Average Reward: -138.03\n",
      "Iteration 5112, Average Reward: -114.73\n",
      "Iteration 5113, Average Reward: -117.05\n",
      "Iteration 5114, Average Reward: -120.16\n",
      "Iteration 5115, Average Reward: -107.99\n",
      "Iteration 5116, Average Reward: -102.31\n",
      "Iteration 5117, Average Reward: -119.5\n",
      "Iteration 5118, Average Reward: -102.09\n",
      "Iteration 5119, Average Reward: -111.55\n",
      "Iteration 5120, Average Reward: -107.77\n",
      "Iteration 5121, Average Reward: -110.1\n",
      "Iteration 5122, Average Reward: -122.04\n",
      "Iteration 5123, Average Reward: -132.1\n",
      "Iteration 5124, Average Reward: -137.35\n",
      "Iteration 5125, Average Reward: -134.04\n",
      "Iteration 5126, Average Reward: -128.88\n",
      "Iteration 5127, Average Reward: -122.59\n",
      "Iteration 5128, Average Reward: -119.7\n",
      "Iteration 5129, Average Reward: -120.58\n",
      "Iteration 5130, Average Reward: -116.24\n",
      "Iteration 5131, Average Reward: -122.14\n",
      "Iteration 5132, Average Reward: -120.85\n",
      "Iteration 5133, Average Reward: -125.74\n",
      "Iteration 5134, Average Reward: -136.36\n",
      "Iteration 5135, Average Reward: -131.71\n",
      "Iteration 5136, Average Reward: -129.11\n",
      "Iteration 5137, Average Reward: -120.05\n",
      "Iteration 5138, Average Reward: -121.14\n",
      "Iteration 5139, Average Reward: -120.12\n",
      "Iteration 5140, Average Reward: -109.05\n",
      "Iteration 5141, Average Reward: -103.41\n",
      "Iteration 5142, Average Reward: -105.97\n",
      "Iteration 5143, Average Reward: -95.68\n",
      "Iteration 5144, Average Reward: -101.4\n",
      "Iteration 5145, Average Reward: -116.89\n",
      "Iteration 5146, Average Reward: -99.82\n",
      "Iteration 5147, Average Reward: -107.71\n",
      "Iteration 5148, Average Reward: -130.1\n",
      "Iteration 5149, Average Reward: -137.42\n",
      "Iteration 5150, Average Reward: -118.64\n",
      "Iteration 5151, Average Reward: -128.89\n",
      "Iteration 5152, Average Reward: -123.64\n",
      "Iteration 5153, Average Reward: -124.83\n",
      "Iteration 5154, Average Reward: -125.49\n",
      "Iteration 5155, Average Reward: -121.87\n",
      "Iteration 5156, Average Reward: -122.68\n",
      "Iteration 5157, Average Reward: -123.47\n",
      "Iteration 5158, Average Reward: -115.55\n",
      "Iteration 5159, Average Reward: -118.55\n",
      "Iteration 5160, Average Reward: -131.81\n",
      "Iteration 5161, Average Reward: -110.46\n",
      "Iteration 5162, Average Reward: -100.13\n",
      "Iteration 5163, Average Reward: -136.34\n",
      "Iteration 5164, Average Reward: -139.75\n",
      "Iteration 5165, Average Reward: -131.45\n",
      "Iteration 5166, Average Reward: -124.41\n",
      "Iteration 5167, Average Reward: -113.44\n",
      "Iteration 5168, Average Reward: -118.89\n",
      "Iteration 5169, Average Reward: -108.32\n",
      "Iteration 5170, Average Reward: -106.66\n",
      "Iteration 5171, Average Reward: -124.25\n",
      "Iteration 5172, Average Reward: -133.67\n",
      "Iteration 5173, Average Reward: -148.1\n",
      "Iteration 5174, Average Reward: -138.82\n",
      "Iteration 5175, Average Reward: -134.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5176, Average Reward: -136.48\n",
      "Iteration 5177, Average Reward: -122.17\n",
      "Iteration 5178, Average Reward: -122.59\n",
      "Iteration 5179, Average Reward: -94.45\n",
      "Iteration 5180, Average Reward: -105.76\n",
      "Iteration 5181, Average Reward: -115.89\n",
      "Iteration 5182, Average Reward: -121.83\n",
      "Iteration 5183, Average Reward: -132.89\n",
      "Iteration 5184, Average Reward: -144.96\n",
      "Iteration 5185, Average Reward: -116.8\n",
      "Iteration 5186, Average Reward: -108.22\n",
      "Iteration 5187, Average Reward: -120.22\n",
      "Iteration 5188, Average Reward: -108.9\n",
      "Iteration 5189, Average Reward: -109.55\n",
      "Iteration 5190, Average Reward: -108.63\n",
      "Iteration 5191, Average Reward: -109.08\n",
      "Iteration 5192, Average Reward: -115.63\n",
      "Iteration 5193, Average Reward: -115.62\n",
      "Iteration 5194, Average Reward: -118.95\n",
      "Iteration 5195, Average Reward: -111.16\n",
      "Iteration 5196, Average Reward: -97.14\n",
      "Iteration 5197, Average Reward: -116.69\n",
      "Iteration 5198, Average Reward: -118.94\n",
      "Iteration 5199, Average Reward: -129.29\n",
      "Iteration 5200, Average Reward: -124.4\n",
      "Iteration 5201, Average Reward: -102.37\n",
      "Iteration 5202, Average Reward: -114.35\n",
      "Iteration 5203, Average Reward: -121.85\n",
      "Iteration 5204, Average Reward: -127.88\n",
      "Iteration 5205, Average Reward: -127.99\n",
      "Iteration 5206, Average Reward: -119.85\n",
      "Iteration 5207, Average Reward: -106.49\n",
      "Iteration 5208, Average Reward: -116.14\n",
      "Iteration 5209, Average Reward: -105.57\n",
      "Iteration 5210, Average Reward: -114.83\n",
      "Iteration 5211, Average Reward: -124.06\n",
      "Iteration 5212, Average Reward: -133.38\n",
      "Iteration 5213, Average Reward: -128.18\n",
      "Iteration 5214, Average Reward: -124.64\n",
      "Iteration 5215, Average Reward: -131.78\n",
      "Iteration 5216, Average Reward: -116.18\n",
      "Iteration 5217, Average Reward: -111.14\n",
      "Iteration 5218, Average Reward: -117.76\n",
      "Iteration 5219, Average Reward: -113.68\n",
      "Iteration 5220, Average Reward: -115.72\n",
      "Iteration 5221, Average Reward: -98.53\n",
      "Iteration 5222, Average Reward: -113.71\n",
      "Iteration 5223, Average Reward: -136.79\n",
      "Iteration 5224, Average Reward: -133.36\n",
      "Iteration 5225, Average Reward: -136.37\n",
      "Iteration 5226, Average Reward: -126.23\n",
      "Iteration 5227, Average Reward: -124.94\n",
      "Iteration 5228, Average Reward: -117.18\n",
      "Iteration 5229, Average Reward: -109.95\n",
      "Iteration 5230, Average Reward: -117.13\n",
      "Iteration 5231, Average Reward: -125.0\n",
      "Iteration 5232, Average Reward: -110.94\n",
      "Iteration 5233, Average Reward: -122.21\n",
      "Iteration 5234, Average Reward: -116.5\n",
      "Iteration 5235, Average Reward: -113.73\n",
      "Iteration 5236, Average Reward: -118.03\n",
      "Iteration 5237, Average Reward: -114.83\n",
      "Iteration 5238, Average Reward: -127.14\n",
      "Iteration 5239, Average Reward: -126.22\n",
      "Iteration 5240, Average Reward: -133.62\n",
      "Iteration 5241, Average Reward: -133.96\n",
      "Iteration 5242, Average Reward: -128.15\n",
      "Iteration 5243, Average Reward: -134.0\n",
      "Iteration 5244, Average Reward: -126.62\n",
      "Iteration 5245, Average Reward: -118.27\n",
      "Iteration 5246, Average Reward: -110.45\n",
      "Iteration 5247, Average Reward: -97.09\n",
      "Iteration 5248, Average Reward: -117.02\n",
      "Iteration 5249, Average Reward: -117.37\n",
      "Iteration 5250, Average Reward: -130.33\n",
      "Iteration 5251, Average Reward: -135.72\n",
      "Iteration 5252, Average Reward: -142.45\n",
      "Iteration 5253, Average Reward: -123.04\n",
      "Iteration 5254, Average Reward: -110.72\n",
      "Iteration 5255, Average Reward: -126.09\n",
      "Iteration 5256, Average Reward: -125.05\n",
      "Iteration 5257, Average Reward: -106.82\n",
      "Iteration 5258, Average Reward: -140.7\n",
      "Iteration 5259, Average Reward: -146.36\n",
      "Iteration 5260, Average Reward: -132.46\n",
      "Iteration 5261, Average Reward: -138.82\n",
      "Iteration 5262, Average Reward: -156.12\n",
      "Iteration 5263, Average Reward: -139.82\n",
      "Iteration 5264, Average Reward: -124.19\n",
      "Iteration 5265, Average Reward: -107.95\n",
      "Iteration 5266, Average Reward: -103.53\n",
      "Iteration 5267, Average Reward: -97.51\n",
      "Iteration 5268, Average Reward: -107.04\n",
      "Iteration 5269, Average Reward: -123.14\n",
      "Iteration 5270, Average Reward: -126.83\n",
      "Iteration 5271, Average Reward: -132.83\n",
      "Iteration 5272, Average Reward: -128.83\n",
      "Iteration 5273, Average Reward: -114.84\n",
      "Iteration 5274, Average Reward: -114.78\n",
      "Iteration 5275, Average Reward: -122.91\n",
      "Iteration 5276, Average Reward: -120.27\n",
      "Iteration 5277, Average Reward: -111.59\n",
      "Iteration 5278, Average Reward: -127.57\n",
      "Iteration 5279, Average Reward: -118.38\n",
      "Iteration 5280, Average Reward: -108.51\n",
      "Iteration 5281, Average Reward: -108.41\n",
      "Iteration 5282, Average Reward: -109.86\n",
      "Iteration 5283, Average Reward: -121.15\n",
      "Iteration 5284, Average Reward: -118.22\n",
      "Iteration 5285, Average Reward: -115.68\n",
      "Iteration 5286, Average Reward: -141.4\n",
      "Iteration 5287, Average Reward: -143.01\n",
      "Iteration 5288, Average Reward: -126.66\n",
      "Iteration 5289, Average Reward: -121.75\n",
      "Iteration 5290, Average Reward: -123.83\n",
      "Iteration 5291, Average Reward: -100.06\n",
      "Iteration 5292, Average Reward: -106.02\n",
      "Iteration 5293, Average Reward: -126.73\n",
      "Iteration 5294, Average Reward: -122.2\n",
      "Iteration 5295, Average Reward: -125.68\n",
      "Iteration 5296, Average Reward: -129.5\n",
      "Iteration 5297, Average Reward: -130.95\n",
      "Iteration 5298, Average Reward: -119.64\n",
      "Iteration 5299, Average Reward: -120.42\n",
      "Iteration 5300, Average Reward: -115.63\n",
      "Iteration 5301, Average Reward: -116.16\n",
      "Iteration 5302, Average Reward: -107.88\n",
      "Iteration 5303, Average Reward: -108.25\n",
      "Iteration 5304, Average Reward: -115.59\n",
      "Iteration 5305, Average Reward: -127.1\n",
      "Iteration 5306, Average Reward: -115.05\n",
      "Iteration 5307, Average Reward: -110.01\n",
      "Iteration 5308, Average Reward: -130.73\n",
      "Iteration 5309, Average Reward: -141.61\n",
      "Iteration 5310, Average Reward: -123.04\n",
      "Iteration 5311, Average Reward: -130.85\n",
      "Iteration 5312, Average Reward: -118.55\n",
      "Iteration 5313, Average Reward: -105.59\n",
      "Iteration 5314, Average Reward: -96.77\n",
      "Iteration 5315, Average Reward: -103.98\n",
      "Iteration 5316, Average Reward: -103.76\n",
      "Iteration 5317, Average Reward: -103.96\n",
      "Iteration 5318, Average Reward: -125.46\n",
      "Iteration 5319, Average Reward: -119.77\n",
      "Iteration 5320, Average Reward: -121.1\n",
      "Iteration 5321, Average Reward: -135.6\n",
      "Iteration 5322, Average Reward: -120.34\n",
      "Iteration 5323, Average Reward: -123.31\n",
      "Iteration 5324, Average Reward: -143.11\n",
      "Iteration 5325, Average Reward: -126.91\n",
      "Iteration 5326, Average Reward: -128.28\n",
      "Iteration 5327, Average Reward: -120.9\n",
      "Iteration 5328, Average Reward: -99.2\n",
      "Iteration 5329, Average Reward: -111.56\n",
      "Iteration 5330, Average Reward: -111.07\n",
      "Iteration 5331, Average Reward: -117.82\n",
      "Iteration 5332, Average Reward: -145.6\n",
      "Iteration 5333, Average Reward: -149.9\n",
      "Iteration 5334, Average Reward: -116.96\n",
      "Iteration 5335, Average Reward: -117.32\n",
      "Iteration 5336, Average Reward: -121.89\n",
      "Iteration 5337, Average Reward: -104.05\n",
      "Iteration 5338, Average Reward: -81.58\n",
      "Iteration 5339, Average Reward: -91.54\n",
      "Iteration 5340, Average Reward: -126.26\n",
      "Iteration 5341, Average Reward: -131.81\n",
      "Iteration 5342, Average Reward: -135.98\n",
      "Iteration 5343, Average Reward: -137.01\n",
      "Iteration 5344, Average Reward: -129.63\n",
      "Iteration 5345, Average Reward: -124.01\n",
      "Iteration 5346, Average Reward: -120.13\n",
      "Iteration 5347, Average Reward: -119.27\n",
      "Iteration 5348, Average Reward: -124.33\n",
      "Iteration 5349, Average Reward: -124.47\n",
      "Iteration 5350, Average Reward: -119.14\n",
      "Iteration 5351, Average Reward: -108.89\n",
      "Iteration 5352, Average Reward: -114.36\n",
      "Iteration 5353, Average Reward: -123.03\n",
      "Iteration 5354, Average Reward: -129.7\n",
      "Iteration 5355, Average Reward: -130.9\n",
      "Iteration 5356, Average Reward: -121.83\n",
      "Iteration 5357, Average Reward: -117.81\n",
      "Iteration 5358, Average Reward: -107.62\n",
      "Iteration 5359, Average Reward: -111.51\n",
      "Iteration 5360, Average Reward: -118.94\n",
      "Iteration 5361, Average Reward: -108.1\n",
      "Iteration 5362, Average Reward: -120.29\n",
      "Iteration 5363, Average Reward: -140.36\n",
      "Iteration 5364, Average Reward: -130.95\n",
      "Iteration 5365, Average Reward: -123.48\n",
      "Iteration 5366, Average Reward: -128.1\n",
      "Iteration 5367, Average Reward: -129.2\n",
      "Iteration 5368, Average Reward: -125.21\n",
      "Iteration 5369, Average Reward: -133.35\n",
      "Iteration 5370, Average Reward: -119.12\n",
      "Iteration 5371, Average Reward: -124.19\n",
      "Iteration 5372, Average Reward: -118.79\n",
      "Iteration 5373, Average Reward: -103.42\n",
      "Iteration 5374, Average Reward: -116.53\n",
      "Iteration 5375, Average Reward: -122.02\n",
      "Iteration 5376, Average Reward: -125.41\n",
      "Iteration 5377, Average Reward: -137.35\n",
      "Iteration 5378, Average Reward: -128.57\n",
      "Iteration 5379, Average Reward: -125.15\n",
      "Iteration 5380, Average Reward: -141.17\n",
      "Iteration 5381, Average Reward: -133.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5382, Average Reward: -114.48\n",
      "Iteration 5383, Average Reward: -104.52\n",
      "Iteration 5384, Average Reward: -134.62\n",
      "Iteration 5385, Average Reward: -134.9\n",
      "Iteration 5386, Average Reward: -108.12\n",
      "Iteration 5387, Average Reward: -109.89\n",
      "Iteration 5388, Average Reward: -117.66\n",
      "Iteration 5389, Average Reward: -132.41\n",
      "Iteration 5390, Average Reward: -117.52\n",
      "Iteration 5391, Average Reward: -105.25\n",
      "Iteration 5392, Average Reward: -112.08\n",
      "Iteration 5393, Average Reward: -108.11\n",
      "Iteration 5394, Average Reward: -121.39\n",
      "Iteration 5395, Average Reward: -118.65\n",
      "Iteration 5396, Average Reward: -120.26\n",
      "Iteration 5397, Average Reward: -127.66\n",
      "Iteration 5398, Average Reward: -136.16\n",
      "Iteration 5399, Average Reward: -123.97\n",
      "Iteration 5400, Average Reward: -110.19\n",
      "Iteration 5401, Average Reward: -117.22\n",
      "Iteration 5402, Average Reward: -122.97\n",
      "Iteration 5403, Average Reward: -120.3\n",
      "Iteration 5404, Average Reward: -107.51\n",
      "Iteration 5405, Average Reward: -114.09\n",
      "Iteration 5406, Average Reward: -121.67\n",
      "Iteration 5407, Average Reward: -121.23\n",
      "Iteration 5408, Average Reward: -113.8\n",
      "Iteration 5409, Average Reward: -102.27\n",
      "Iteration 5410, Average Reward: -89.42\n",
      "Iteration 5411, Average Reward: -100.45\n",
      "Iteration 5412, Average Reward: -114.37\n",
      "Iteration 5413, Average Reward: -130.75\n",
      "Iteration 5414, Average Reward: -141.76\n",
      "Iteration 5415, Average Reward: -129.02\n",
      "Iteration 5416, Average Reward: -126.02\n",
      "Iteration 5417, Average Reward: -126.04\n",
      "Iteration 5418, Average Reward: -123.66\n",
      "Iteration 5419, Average Reward: -118.58\n",
      "Iteration 5420, Average Reward: -124.5\n",
      "Iteration 5421, Average Reward: -115.67\n",
      "Iteration 5422, Average Reward: -110.56\n",
      "Iteration 5423, Average Reward: -126.72\n",
      "Iteration 5424, Average Reward: -119.54\n",
      "Iteration 5425, Average Reward: -118.98\n",
      "Iteration 5426, Average Reward: -116.23\n",
      "Iteration 5427, Average Reward: -122.11\n",
      "Iteration 5428, Average Reward: -130.41\n",
      "Iteration 5429, Average Reward: -139.86\n",
      "Iteration 5430, Average Reward: -144.2\n",
      "Iteration 5431, Average Reward: -133.89\n",
      "Iteration 5432, Average Reward: -118.56\n",
      "Iteration 5433, Average Reward: -94.64\n",
      "Iteration 5434, Average Reward: -97.8\n",
      "Iteration 5435, Average Reward: -109.65\n",
      "Iteration 5436, Average Reward: -103.3\n",
      "Iteration 5437, Average Reward: -115.25\n",
      "Iteration 5438, Average Reward: -143.15\n",
      "Iteration 5439, Average Reward: -143.24\n",
      "Iteration 5440, Average Reward: -128.2\n",
      "Iteration 5441, Average Reward: -124.05\n",
      "Iteration 5442, Average Reward: -123.25\n",
      "Iteration 5443, Average Reward: -110.89\n",
      "Iteration 5444, Average Reward: -102.81\n",
      "Iteration 5445, Average Reward: -116.23\n",
      "Iteration 5446, Average Reward: -137.52\n",
      "Iteration 5447, Average Reward: -128.81\n",
      "Iteration 5448, Average Reward: -141.43\n",
      "Iteration 5449, Average Reward: -132.36\n",
      "Iteration 5450, Average Reward: -125.18\n",
      "Iteration 5451, Average Reward: -107.05\n",
      "Iteration 5452, Average Reward: -119.74\n",
      "Iteration 5453, Average Reward: -108.41\n",
      "Iteration 5454, Average Reward: -117.28\n",
      "Iteration 5455, Average Reward: -125.67\n",
      "Iteration 5456, Average Reward: -134.11\n",
      "Iteration 5457, Average Reward: -144.4\n",
      "Iteration 5458, Average Reward: -125.38\n",
      "Iteration 5459, Average Reward: -119.16\n",
      "Iteration 5460, Average Reward: -126.43\n",
      "Iteration 5461, Average Reward: -126.26\n",
      "Iteration 5462, Average Reward: -114.73\n",
      "Iteration 5463, Average Reward: -100.91\n",
      "Iteration 5464, Average Reward: -120.41\n",
      "Iteration 5465, Average Reward: -136.34\n",
      "Iteration 5466, Average Reward: -130.78\n",
      "Iteration 5467, Average Reward: -132.26\n",
      "Iteration 5468, Average Reward: -132.21\n",
      "Iteration 5469, Average Reward: -131.56\n",
      "Iteration 5470, Average Reward: -128.74\n",
      "Iteration 5471, Average Reward: -121.29\n",
      "Iteration 5472, Average Reward: -117.81\n",
      "Iteration 5473, Average Reward: -125.41\n",
      "Iteration 5474, Average Reward: -123.59\n",
      "Iteration 5475, Average Reward: -114.97\n",
      "Iteration 5476, Average Reward: -115.67\n",
      "Iteration 5477, Average Reward: -115.48\n",
      "Iteration 5478, Average Reward: -121.89\n",
      "Iteration 5479, Average Reward: -137.86\n",
      "Iteration 5480, Average Reward: -141.71\n",
      "Iteration 5481, Average Reward: -133.85\n",
      "Iteration 5482, Average Reward: -127.34\n",
      "Iteration 5483, Average Reward: -127.27\n",
      "Iteration 5484, Average Reward: -101.98\n",
      "Iteration 5485, Average Reward: -117.78\n",
      "Iteration 5486, Average Reward: -138.06\n",
      "Iteration 5487, Average Reward: -111.9\n",
      "Iteration 5488, Average Reward: -106.47\n",
      "Iteration 5489, Average Reward: -112.03\n",
      "Iteration 5490, Average Reward: -114.68\n",
      "Iteration 5491, Average Reward: -120.84\n",
      "Iteration 5492, Average Reward: -120.14\n",
      "Iteration 5493, Average Reward: -125.46\n",
      "Iteration 5494, Average Reward: -117.85\n",
      "Iteration 5495, Average Reward: -101.9\n",
      "Iteration 5496, Average Reward: -108.26\n",
      "Iteration 5497, Average Reward: -122.32\n",
      "Iteration 5498, Average Reward: -118.11\n",
      "Iteration 5499, Average Reward: -112.66\n",
      "Iteration 5500, Average Reward: -100.78\n",
      "Iteration 5501, Average Reward: -117.72\n",
      "Iteration 5502, Average Reward: -109.28\n",
      "Iteration 5503, Average Reward: -118.79\n",
      "Iteration 5504, Average Reward: -129.22\n",
      "Iteration 5505, Average Reward: -139.09\n",
      "Iteration 5506, Average Reward: -129.33\n",
      "Iteration 5507, Average Reward: -120.6\n",
      "Iteration 5508, Average Reward: -112.11\n",
      "Iteration 5509, Average Reward: -112.44\n",
      "Iteration 5510, Average Reward: -110.76\n",
      "Iteration 5511, Average Reward: -123.25\n",
      "Iteration 5512, Average Reward: -106.95\n",
      "Iteration 5513, Average Reward: -102.97\n",
      "Iteration 5514, Average Reward: -121.06\n",
      "Iteration 5515, Average Reward: -128.9\n",
      "Iteration 5516, Average Reward: -130.76\n",
      "Iteration 5517, Average Reward: -147.48\n",
      "Iteration 5518, Average Reward: -144.42\n",
      "Iteration 5519, Average Reward: -111.55\n",
      "Iteration 5520, Average Reward: -107.56\n",
      "Iteration 5521, Average Reward: -120.23\n",
      "Iteration 5522, Average Reward: -112.52\n",
      "Iteration 5523, Average Reward: -111.07\n",
      "Iteration 5524, Average Reward: -118.89\n",
      "Iteration 5525, Average Reward: -122.87\n",
      "Iteration 5526, Average Reward: -127.44\n",
      "Iteration 5527, Average Reward: -119.07\n",
      "Iteration 5528, Average Reward: -96.26\n",
      "Iteration 5529, Average Reward: -110.31\n",
      "Iteration 5530, Average Reward: -114.2\n",
      "Iteration 5531, Average Reward: -109.6\n",
      "Iteration 5532, Average Reward: -111.15\n",
      "Iteration 5533, Average Reward: -130.76\n",
      "Iteration 5534, Average Reward: -122.68\n",
      "Iteration 5535, Average Reward: -118.63\n",
      "Iteration 5536, Average Reward: -123.28\n",
      "Iteration 5537, Average Reward: -101.9\n",
      "Iteration 5538, Average Reward: -109.92\n",
      "Iteration 5539, Average Reward: -123.07\n",
      "Iteration 5540, Average Reward: -106.84\n",
      "Iteration 5541, Average Reward: -109.42\n",
      "Iteration 5542, Average Reward: -130.65\n",
      "Iteration 5543, Average Reward: -139.98\n",
      "Iteration 5544, Average Reward: -122.18\n",
      "Iteration 5545, Average Reward: -138.67\n",
      "Iteration 5546, Average Reward: -134.22\n",
      "Iteration 5547, Average Reward: -129.65\n",
      "Iteration 5548, Average Reward: -107.9\n",
      "Iteration 5549, Average Reward: -107.53\n",
      "Iteration 5550, Average Reward: -135.67\n",
      "Iteration 5551, Average Reward: -139.62\n",
      "Iteration 5552, Average Reward: -119.43\n",
      "Iteration 5553, Average Reward: -111.65\n",
      "Iteration 5554, Average Reward: -104.58\n",
      "Iteration 5555, Average Reward: -105.94\n",
      "Iteration 5556, Average Reward: -106.21\n",
      "Iteration 5557, Average Reward: -121.74\n",
      "Iteration 5558, Average Reward: -128.71\n",
      "Iteration 5559, Average Reward: -130.03\n",
      "Iteration 5560, Average Reward: -132.51\n",
      "Iteration 5561, Average Reward: -130.71\n",
      "Iteration 5562, Average Reward: -121.57\n",
      "Iteration 5563, Average Reward: -125.59\n",
      "Iteration 5564, Average Reward: -132.03\n",
      "Iteration 5565, Average Reward: -141.61\n",
      "Iteration 5566, Average Reward: -123.29\n",
      "Iteration 5567, Average Reward: -116.23\n",
      "Iteration 5568, Average Reward: -121.76\n",
      "Iteration 5569, Average Reward: -126.9\n",
      "Iteration 5570, Average Reward: -120.49\n",
      "Iteration 5571, Average Reward: -122.43\n",
      "Iteration 5572, Average Reward: -130.42\n",
      "Iteration 5573, Average Reward: -134.16\n",
      "Iteration 5574, Average Reward: -134.71\n",
      "Iteration 5575, Average Reward: -123.38\n",
      "Iteration 5576, Average Reward: -141.21\n",
      "Iteration 5577, Average Reward: -138.2\n",
      "Iteration 5578, Average Reward: -129.22\n",
      "Iteration 5579, Average Reward: -127.05\n",
      "Iteration 5580, Average Reward: -112.06\n",
      "Iteration 5581, Average Reward: -106.08\n",
      "Iteration 5582, Average Reward: -102.51\n",
      "Iteration 5583, Average Reward: -117.69\n",
      "Iteration 5584, Average Reward: -102.24\n",
      "Iteration 5585, Average Reward: -119.49\n",
      "Iteration 5586, Average Reward: -131.47\n",
      "Iteration 5587, Average Reward: -133.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5588, Average Reward: -139.47\n",
      "Iteration 5589, Average Reward: -138.55\n",
      "Iteration 5590, Average Reward: -143.81\n",
      "Iteration 5591, Average Reward: -123.99\n",
      "Iteration 5592, Average Reward: -115.47\n",
      "Iteration 5593, Average Reward: -123.27\n",
      "Iteration 5594, Average Reward: -129.87\n",
      "Iteration 5595, Average Reward: -113.81\n",
      "Iteration 5596, Average Reward: -113.77\n",
      "Iteration 5597, Average Reward: -120.58\n",
      "Iteration 5598, Average Reward: -125.18\n",
      "Iteration 5599, Average Reward: -100.05\n",
      "Iteration 5600, Average Reward: -123.34\n",
      "Iteration 5601, Average Reward: -124.93\n",
      "Iteration 5602, Average Reward: -117.67\n",
      "Iteration 5603, Average Reward: -127.4\n",
      "Iteration 5604, Average Reward: -122.94\n",
      "Iteration 5605, Average Reward: -110.85\n",
      "Iteration 5606, Average Reward: -117.66\n",
      "Iteration 5607, Average Reward: -111.21\n",
      "Iteration 5608, Average Reward: -111.38\n",
      "Iteration 5609, Average Reward: -114.8\n",
      "Iteration 5610, Average Reward: -144.42\n",
      "Iteration 5611, Average Reward: -136.85\n",
      "Iteration 5612, Average Reward: -123.32\n",
      "Iteration 5613, Average Reward: -107.97\n",
      "Iteration 5614, Average Reward: -107.98\n",
      "Iteration 5615, Average Reward: -116.44\n",
      "Iteration 5616, Average Reward: -106.58\n",
      "Iteration 5617, Average Reward: -101.66\n",
      "Iteration 5618, Average Reward: -109.36\n",
      "Iteration 5619, Average Reward: -131.17\n",
      "Iteration 5620, Average Reward: -128.44\n",
      "Iteration 5621, Average Reward: -133.55\n",
      "Iteration 5622, Average Reward: -148.71\n",
      "Iteration 5623, Average Reward: -135.24\n",
      "Iteration 5624, Average Reward: -120.6\n",
      "Iteration 5625, Average Reward: -106.61\n",
      "Iteration 5626, Average Reward: -103.72\n",
      "Iteration 5627, Average Reward: -113.39\n",
      "Iteration 5628, Average Reward: -125.77\n",
      "Iteration 5629, Average Reward: -113.04\n",
      "Iteration 5630, Average Reward: -114.68\n",
      "Iteration 5631, Average Reward: -139.78\n",
      "Iteration 5632, Average Reward: -133.6\n",
      "Iteration 5633, Average Reward: -126.09\n",
      "Iteration 5634, Average Reward: -126.56\n",
      "Iteration 5635, Average Reward: -136.28\n",
      "Iteration 5636, Average Reward: -121.19\n",
      "Iteration 5637, Average Reward: -125.75\n",
      "Iteration 5638, Average Reward: -130.64\n",
      "Iteration 5639, Average Reward: -115.48\n",
      "Iteration 5640, Average Reward: -116.93\n",
      "Iteration 5641, Average Reward: -114.0\n",
      "Iteration 5642, Average Reward: -109.13\n",
      "Iteration 5643, Average Reward: -117.02\n",
      "Iteration 5644, Average Reward: -136.49\n",
      "Iteration 5645, Average Reward: -124.78\n",
      "Iteration 5646, Average Reward: -125.97\n",
      "Iteration 5647, Average Reward: -127.6\n",
      "Iteration 5648, Average Reward: -124.82\n",
      "Iteration 5649, Average Reward: -112.3\n",
      "Iteration 5650, Average Reward: -112.22\n",
      "Iteration 5651, Average Reward: -118.19\n",
      "Iteration 5652, Average Reward: -111.53\n",
      "Iteration 5653, Average Reward: -98.1\n",
      "Iteration 5654, Average Reward: -105.22\n",
      "Iteration 5655, Average Reward: -129.41\n",
      "Iteration 5656, Average Reward: -121.61\n",
      "Iteration 5657, Average Reward: -134.75\n",
      "Iteration 5658, Average Reward: -140.27\n",
      "Iteration 5659, Average Reward: -120.01\n",
      "Iteration 5660, Average Reward: -122.45\n",
      "Iteration 5661, Average Reward: -103.45\n",
      "Iteration 5662, Average Reward: -82.27\n",
      "Iteration 5663, Average Reward: -107.93\n",
      "Iteration 5664, Average Reward: -115.44\n",
      "Iteration 5665, Average Reward: -129.36\n",
      "Iteration 5666, Average Reward: -129.18\n",
      "Iteration 5667, Average Reward: -123.16\n",
      "Iteration 5668, Average Reward: -134.06\n",
      "Iteration 5669, Average Reward: -135.4\n",
      "Iteration 5670, Average Reward: -121.88\n",
      "Iteration 5671, Average Reward: -131.24\n",
      "Iteration 5672, Average Reward: -114.93\n",
      "Iteration 5673, Average Reward: -125.28\n",
      "Iteration 5674, Average Reward: -119.08\n",
      "Iteration 5675, Average Reward: -113.41\n",
      "Iteration 5676, Average Reward: -122.18\n",
      "Iteration 5677, Average Reward: -141.6\n",
      "Iteration 5678, Average Reward: -134.16\n",
      "Iteration 5679, Average Reward: -131.29\n",
      "Iteration 5680, Average Reward: -132.49\n",
      "Iteration 5681, Average Reward: -131.13\n",
      "Iteration 5682, Average Reward: -122.96\n",
      "Iteration 5683, Average Reward: -111.38\n",
      "Iteration 5684, Average Reward: -102.48\n",
      "Iteration 5685, Average Reward: -103.14\n",
      "Iteration 5686, Average Reward: -120.66\n",
      "Iteration 5687, Average Reward: -129.91\n",
      "Iteration 5688, Average Reward: -118.07\n",
      "Iteration 5689, Average Reward: -122.44\n",
      "Iteration 5690, Average Reward: -137.62\n",
      "Iteration 5691, Average Reward: -132.44\n",
      "Iteration 5692, Average Reward: -139.86\n",
      "Iteration 5693, Average Reward: -137.28\n",
      "Iteration 5694, Average Reward: -131.16\n",
      "Iteration 5695, Average Reward: -129.52\n",
      "Iteration 5696, Average Reward: -133.79\n",
      "Iteration 5697, Average Reward: -116.33\n",
      "Iteration 5698, Average Reward: -117.77\n",
      "Iteration 5699, Average Reward: -128.47\n",
      "Iteration 5700, Average Reward: -123.13\n",
      "Iteration 5701, Average Reward: -113.07\n",
      "Iteration 5702, Average Reward: -116.8\n",
      "Iteration 5703, Average Reward: -127.62\n",
      "Iteration 5704, Average Reward: -137.19\n",
      "Iteration 5705, Average Reward: -138.12\n",
      "Iteration 5706, Average Reward: -134.75\n",
      "Iteration 5707, Average Reward: -132.24\n",
      "Iteration 5708, Average Reward: -104.66\n",
      "Iteration 5709, Average Reward: -90.8\n",
      "Iteration 5710, Average Reward: -109.23\n",
      "Iteration 5711, Average Reward: -122.12\n",
      "Iteration 5712, Average Reward: -122.9\n",
      "Iteration 5713, Average Reward: -146.72\n",
      "Iteration 5714, Average Reward: -156.94\n",
      "Iteration 5715, Average Reward: -126.63\n",
      "Iteration 5716, Average Reward: -110.02\n",
      "Iteration 5717, Average Reward: -102.4\n",
      "Iteration 5718, Average Reward: -107.92\n",
      "Iteration 5719, Average Reward: -104.32\n",
      "Iteration 5720, Average Reward: -116.75\n",
      "Iteration 5721, Average Reward: -144.81\n",
      "Iteration 5722, Average Reward: -131.87\n",
      "Iteration 5723, Average Reward: -102.32\n",
      "Iteration 5724, Average Reward: -131.93\n",
      "Iteration 5725, Average Reward: -132.59\n",
      "Iteration 5726, Average Reward: -121.32\n",
      "Iteration 5727, Average Reward: -143.19\n",
      "Iteration 5728, Average Reward: -109.56\n",
      "Iteration 5729, Average Reward: -117.92\n",
      "Iteration 5730, Average Reward: -127.24\n",
      "Iteration 5731, Average Reward: -126.57\n",
      "Iteration 5732, Average Reward: -132.45\n",
      "Iteration 5733, Average Reward: -139.38\n",
      "Iteration 5734, Average Reward: -133.75\n",
      "Iteration 5735, Average Reward: -126.07\n",
      "Iteration 5736, Average Reward: -117.93\n",
      "Iteration 5737, Average Reward: -117.41\n",
      "Iteration 5738, Average Reward: -113.68\n",
      "Iteration 5739, Average Reward: -111.37\n",
      "Iteration 5740, Average Reward: -101.01\n",
      "Iteration 5741, Average Reward: -109.0\n",
      "Iteration 5742, Average Reward: -126.72\n",
      "Iteration 5743, Average Reward: -132.45\n",
      "Iteration 5744, Average Reward: -118.86\n",
      "Iteration 5745, Average Reward: -110.86\n",
      "Iteration 5746, Average Reward: -107.03\n",
      "Iteration 5747, Average Reward: -102.56\n",
      "Iteration 5748, Average Reward: -114.89\n",
      "Iteration 5749, Average Reward: -137.35\n",
      "Iteration 5750, Average Reward: -121.2\n",
      "Iteration 5751, Average Reward: -113.1\n",
      "Iteration 5752, Average Reward: -126.13\n",
      "Iteration 5753, Average Reward: -130.14\n",
      "Iteration 5754, Average Reward: -131.04\n",
      "Iteration 5755, Average Reward: -136.56\n",
      "Iteration 5756, Average Reward: -125.12\n",
      "Iteration 5757, Average Reward: -94.15\n",
      "Iteration 5758, Average Reward: -104.53\n",
      "Iteration 5759, Average Reward: -108.27\n",
      "Iteration 5760, Average Reward: -110.82\n",
      "Iteration 5761, Average Reward: -123.52\n",
      "Iteration 5762, Average Reward: -133.96\n",
      "Iteration 5763, Average Reward: -133.37\n",
      "Iteration 5764, Average Reward: -122.45\n",
      "Iteration 5765, Average Reward: -126.92\n",
      "Iteration 5766, Average Reward: -115.48\n",
      "Iteration 5767, Average Reward: -111.91\n",
      "Iteration 5768, Average Reward: -116.61\n",
      "Iteration 5769, Average Reward: -122.31\n",
      "Iteration 5770, Average Reward: -113.9\n",
      "Iteration 5771, Average Reward: -114.89\n",
      "Iteration 5772, Average Reward: -104.33\n",
      "Iteration 5773, Average Reward: -110.97\n",
      "Iteration 5774, Average Reward: -122.52\n",
      "Iteration 5775, Average Reward: -119.9\n",
      "Iteration 5776, Average Reward: -115.69\n",
      "Iteration 5777, Average Reward: -113.12\n",
      "Iteration 5778, Average Reward: -116.8\n",
      "Iteration 5779, Average Reward: -109.88\n",
      "Iteration 5780, Average Reward: -99.35\n",
      "Iteration 5781, Average Reward: -115.4\n",
      "Iteration 5782, Average Reward: -124.75\n",
      "Iteration 5783, Average Reward: -135.09\n",
      "Iteration 5784, Average Reward: -128.73\n",
      "Iteration 5785, Average Reward: -107.63\n",
      "Iteration 5786, Average Reward: -107.99\n",
      "Iteration 5787, Average Reward: -136.5\n",
      "Iteration 5788, Average Reward: -132.34\n",
      "Iteration 5789, Average Reward: -143.62\n",
      "Iteration 5790, Average Reward: -132.37\n",
      "Iteration 5791, Average Reward: -134.64\n",
      "Iteration 5792, Average Reward: -118.71\n",
      "Iteration 5793, Average Reward: -113.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5794, Average Reward: -124.28\n",
      "Iteration 5795, Average Reward: -130.02\n",
      "Iteration 5796, Average Reward: -130.92\n",
      "Iteration 5797, Average Reward: -116.87\n",
      "Iteration 5798, Average Reward: -109.79\n",
      "Iteration 5799, Average Reward: -111.62\n",
      "Iteration 5800, Average Reward: -108.35\n",
      "Iteration 5801, Average Reward: -101.77\n",
      "Iteration 5802, Average Reward: -105.18\n",
      "Iteration 5803, Average Reward: -107.95\n",
      "Iteration 5804, Average Reward: -116.21\n",
      "Iteration 5805, Average Reward: -121.66\n",
      "Iteration 5806, Average Reward: -140.34\n",
      "Iteration 5807, Average Reward: -136.74\n",
      "Iteration 5808, Average Reward: -124.83\n",
      "Iteration 5809, Average Reward: -109.99\n",
      "Iteration 5810, Average Reward: -96.61\n",
      "Iteration 5811, Average Reward: -114.89\n",
      "Iteration 5812, Average Reward: -122.89\n",
      "Iteration 5813, Average Reward: -124.43\n",
      "Iteration 5814, Average Reward: -112.35\n",
      "Iteration 5815, Average Reward: -117.28\n",
      "Iteration 5816, Average Reward: -122.8\n",
      "Iteration 5817, Average Reward: -118.2\n",
      "Iteration 5818, Average Reward: -113.57\n",
      "Iteration 5819, Average Reward: -121.2\n",
      "Iteration 5820, Average Reward: -114.63\n",
      "Iteration 5821, Average Reward: -117.01\n",
      "Iteration 5822, Average Reward: -138.31\n",
      "Iteration 5823, Average Reward: -126.87\n",
      "Iteration 5824, Average Reward: -116.84\n",
      "Iteration 5825, Average Reward: -138.11\n",
      "Iteration 5826, Average Reward: -134.59\n",
      "Iteration 5827, Average Reward: -128.97\n",
      "Iteration 5828, Average Reward: -123.79\n",
      "Iteration 5829, Average Reward: -123.05\n",
      "Iteration 5830, Average Reward: -129.54\n",
      "Iteration 5831, Average Reward: -116.97\n",
      "Iteration 5832, Average Reward: -113.69\n",
      "Iteration 5833, Average Reward: -128.88\n",
      "Iteration 5834, Average Reward: -129.8\n",
      "Iteration 5835, Average Reward: -130.3\n",
      "Iteration 5836, Average Reward: -121.83\n",
      "Iteration 5837, Average Reward: -126.82\n",
      "Iteration 5838, Average Reward: -108.03\n",
      "Iteration 5839, Average Reward: -99.37\n",
      "Iteration 5840, Average Reward: -111.73\n",
      "Iteration 5841, Average Reward: -115.76\n",
      "Iteration 5842, Average Reward: -133.89\n",
      "Iteration 5843, Average Reward: -121.61\n",
      "Iteration 5844, Average Reward: -120.01\n",
      "Iteration 5845, Average Reward: -130.39\n",
      "Iteration 5846, Average Reward: -130.85\n",
      "Iteration 5847, Average Reward: -130.14\n",
      "Iteration 5848, Average Reward: -134.78\n",
      "Iteration 5849, Average Reward: -127.21\n",
      "Iteration 5850, Average Reward: -130.25\n",
      "Iteration 5851, Average Reward: -112.47\n",
      "Iteration 5852, Average Reward: -114.13\n",
      "Iteration 5853, Average Reward: -124.76\n",
      "Iteration 5854, Average Reward: -124.67\n",
      "Iteration 5855, Average Reward: -140.86\n",
      "Iteration 5856, Average Reward: -128.68\n",
      "Iteration 5857, Average Reward: -132.53\n",
      "Iteration 5858, Average Reward: -139.18\n",
      "Iteration 5859, Average Reward: -139.0\n",
      "Iteration 5860, Average Reward: -128.3\n",
      "Iteration 5861, Average Reward: -130.92\n",
      "Iteration 5862, Average Reward: -131.26\n",
      "Iteration 5863, Average Reward: -125.82\n",
      "Iteration 5864, Average Reward: -134.12\n",
      "Iteration 5865, Average Reward: -138.74\n",
      "Iteration 5866, Average Reward: -128.32\n",
      "Iteration 5867, Average Reward: -141.02\n",
      "Iteration 5868, Average Reward: -145.46\n",
      "Iteration 5869, Average Reward: -123.97\n",
      "Iteration 5870, Average Reward: -121.02\n",
      "Iteration 5871, Average Reward: -119.96\n",
      "Iteration 5872, Average Reward: -124.69\n",
      "Iteration 5873, Average Reward: -133.21\n",
      "Iteration 5874, Average Reward: -149.04\n",
      "Iteration 5875, Average Reward: -127.8\n",
      "Iteration 5876, Average Reward: -119.71\n",
      "Iteration 5877, Average Reward: -121.87\n",
      "Iteration 5878, Average Reward: -106.9\n",
      "Iteration 5879, Average Reward: -113.55\n",
      "Iteration 5880, Average Reward: -112.61\n",
      "Iteration 5881, Average Reward: -116.74\n",
      "Iteration 5882, Average Reward: -131.05\n",
      "Iteration 5883, Average Reward: -132.35\n",
      "Iteration 5884, Average Reward: -137.62\n",
      "Iteration 5885, Average Reward: -125.54\n",
      "Iteration 5886, Average Reward: -117.11\n",
      "Iteration 5887, Average Reward: -119.73\n",
      "Iteration 5888, Average Reward: -133.57\n",
      "Iteration 5889, Average Reward: -144.93\n",
      "Iteration 5890, Average Reward: -123.06\n",
      "Iteration 5891, Average Reward: -113.49\n",
      "Iteration 5892, Average Reward: -120.9\n",
      "Iteration 5893, Average Reward: -120.62\n",
      "Iteration 5894, Average Reward: -113.58\n",
      "Iteration 5895, Average Reward: -118.58\n",
      "Iteration 5896, Average Reward: -117.96\n",
      "Iteration 5897, Average Reward: -116.93\n",
      "Iteration 5898, Average Reward: -127.19\n",
      "Iteration 5899, Average Reward: -131.43\n",
      "Iteration 5900, Average Reward: -124.51\n",
      "Iteration 5901, Average Reward: -122.82\n",
      "Iteration 5902, Average Reward: -133.68\n",
      "Iteration 5903, Average Reward: -125.66\n",
      "Iteration 5904, Average Reward: -126.59\n",
      "Iteration 5905, Average Reward: -121.81\n",
      "Iteration 5906, Average Reward: -113.15\n",
      "Iteration 5907, Average Reward: -116.16\n",
      "Iteration 5908, Average Reward: -102.56\n",
      "Iteration 5909, Average Reward: -109.2\n",
      "Iteration 5910, Average Reward: -124.39\n",
      "Iteration 5911, Average Reward: -138.39\n",
      "Iteration 5912, Average Reward: -142.27\n",
      "Iteration 5913, Average Reward: -143.53\n",
      "Iteration 5914, Average Reward: -136.19\n",
      "Iteration 5915, Average Reward: -123.62\n",
      "Iteration 5916, Average Reward: -108.31\n",
      "Iteration 5917, Average Reward: -125.37\n",
      "Iteration 5918, Average Reward: -112.99\n",
      "Iteration 5919, Average Reward: -108.39\n",
      "Iteration 5920, Average Reward: -122.04\n",
      "Iteration 5921, Average Reward: -120.85\n",
      "Iteration 5922, Average Reward: -124.3\n",
      "Iteration 5923, Average Reward: -134.41\n",
      "Iteration 5924, Average Reward: -128.7\n",
      "Iteration 5925, Average Reward: -134.53\n",
      "Iteration 5926, Average Reward: -131.75\n",
      "Iteration 5927, Average Reward: -121.19\n",
      "Iteration 5928, Average Reward: -109.96\n",
      "Iteration 5929, Average Reward: -117.38\n",
      "Iteration 5930, Average Reward: -125.84\n",
      "Iteration 5931, Average Reward: -126.14\n",
      "Iteration 5932, Average Reward: -127.61\n",
      "Iteration 5933, Average Reward: -124.61\n",
      "Iteration 5934, Average Reward: -115.96\n",
      "Iteration 5935, Average Reward: -125.36\n",
      "Iteration 5936, Average Reward: -138.7\n",
      "Iteration 5937, Average Reward: -132.63\n",
      "Iteration 5938, Average Reward: -96.48\n",
      "Iteration 5939, Average Reward: -125.73\n",
      "Iteration 5940, Average Reward: -143.63\n",
      "Iteration 5941, Average Reward: -142.41\n",
      "Iteration 5942, Average Reward: -124.88\n",
      "Iteration 5943, Average Reward: -125.85\n",
      "Iteration 5944, Average Reward: -110.08\n",
      "Iteration 5945, Average Reward: -99.86\n",
      "Iteration 5946, Average Reward: -101.63\n",
      "Iteration 5947, Average Reward: -123.88\n",
      "Iteration 5948, Average Reward: -128.54\n",
      "Iteration 5949, Average Reward: -103.3\n",
      "Iteration 5950, Average Reward: -128.4\n",
      "Iteration 5951, Average Reward: -133.88\n",
      "Iteration 5952, Average Reward: -132.1\n",
      "Iteration 5953, Average Reward: -137.82\n",
      "Iteration 5954, Average Reward: -146.73\n",
      "Iteration 5955, Average Reward: -138.53\n",
      "Iteration 5956, Average Reward: -124.79\n",
      "Iteration 5957, Average Reward: -123.06\n",
      "Iteration 5958, Average Reward: -124.04\n",
      "Iteration 5959, Average Reward: -119.19\n",
      "Iteration 5960, Average Reward: -105.72\n",
      "Iteration 5961, Average Reward: -128.64\n",
      "Iteration 5962, Average Reward: -142.27\n",
      "Iteration 5963, Average Reward: -133.3\n",
      "Iteration 5964, Average Reward: -113.4\n",
      "Iteration 5965, Average Reward: -128.64\n",
      "Iteration 5966, Average Reward: -118.97\n",
      "Iteration 5967, Average Reward: -116.69\n",
      "Iteration 5968, Average Reward: -109.96\n",
      "Iteration 5969, Average Reward: -101.2\n",
      "Iteration 5970, Average Reward: -95.08\n",
      "Iteration 5971, Average Reward: -102.13\n",
      "Iteration 5972, Average Reward: -112.08\n",
      "Iteration 5973, Average Reward: -125.09\n",
      "Iteration 5974, Average Reward: -133.93\n",
      "Iteration 5975, Average Reward: -138.0\n",
      "Iteration 5976, Average Reward: -133.15\n",
      "Iteration 5977, Average Reward: -121.8\n",
      "Iteration 5978, Average Reward: -120.9\n",
      "Iteration 5979, Average Reward: -103.2\n",
      "Iteration 5980, Average Reward: -108.64\n",
      "Iteration 5981, Average Reward: -101.31\n",
      "Iteration 5982, Average Reward: -105.01\n",
      "Iteration 5983, Average Reward: -123.8\n",
      "Iteration 5984, Average Reward: -131.08\n",
      "Iteration 5985, Average Reward: -147.15\n",
      "Iteration 5986, Average Reward: -138.8\n",
      "Iteration 5987, Average Reward: -138.78\n",
      "Iteration 5988, Average Reward: -133.55\n",
      "Iteration 5989, Average Reward: -128.37\n",
      "Iteration 5990, Average Reward: -104.74\n",
      "Iteration 5991, Average Reward: -106.15\n",
      "Iteration 5992, Average Reward: -120.0\n",
      "Iteration 5993, Average Reward: -127.81\n",
      "Iteration 5994, Average Reward: -129.23\n",
      "Iteration 5995, Average Reward: -123.84\n",
      "Iteration 5996, Average Reward: -129.66\n",
      "Iteration 5997, Average Reward: -134.5\n",
      "Iteration 5998, Average Reward: -136.81\n",
      "Iteration 5999, Average Reward: -125.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6000, Average Reward: -120.03\n",
      "Iteration 6001, Average Reward: -111.19\n",
      "Iteration 6002, Average Reward: -124.97\n",
      "Iteration 6003, Average Reward: -108.75\n",
      "Iteration 6004, Average Reward: -136.0\n",
      "Iteration 6005, Average Reward: -137.38\n",
      "Iteration 6006, Average Reward: -123.55\n",
      "Iteration 6007, Average Reward: -125.41\n",
      "Iteration 6008, Average Reward: -101.87\n",
      "Iteration 6009, Average Reward: -111.44\n",
      "Iteration 6010, Average Reward: -130.17\n",
      "Iteration 6011, Average Reward: -123.78\n",
      "Iteration 6012, Average Reward: -106.9\n",
      "Iteration 6013, Average Reward: -109.23\n",
      "Iteration 6014, Average Reward: -127.48\n",
      "Iteration 6015, Average Reward: -125.28\n",
      "Iteration 6016, Average Reward: -128.34\n",
      "Iteration 6017, Average Reward: -108.91\n",
      "Iteration 6018, Average Reward: -119.07\n",
      "Iteration 6019, Average Reward: -120.51\n",
      "Iteration 6020, Average Reward: -111.76\n",
      "Iteration 6021, Average Reward: -118.47\n",
      "Iteration 6022, Average Reward: -127.83\n",
      "Iteration 6023, Average Reward: -122.99\n",
      "Iteration 6024, Average Reward: -112.77\n",
      "Iteration 6025, Average Reward: -131.48\n",
      "Iteration 6026, Average Reward: -118.0\n",
      "Iteration 6027, Average Reward: -119.96\n",
      "Iteration 6028, Average Reward: -114.43\n",
      "Iteration 6029, Average Reward: -126.44\n",
      "Iteration 6030, Average Reward: -115.13\n",
      "Iteration 6031, Average Reward: -116.87\n",
      "Iteration 6032, Average Reward: -108.85\n",
      "Iteration 6033, Average Reward: -114.51\n",
      "Iteration 6034, Average Reward: -129.09\n",
      "Iteration 6035, Average Reward: -141.11\n",
      "Iteration 6036, Average Reward: -142.58\n",
      "Iteration 6037, Average Reward: -130.19\n",
      "Iteration 6038, Average Reward: -129.66\n",
      "Iteration 6039, Average Reward: -122.54\n",
      "Iteration 6040, Average Reward: -124.24\n",
      "Iteration 6041, Average Reward: -115.96\n",
      "Iteration 6042, Average Reward: -123.19\n",
      "Iteration 6043, Average Reward: -123.87\n",
      "Iteration 6044, Average Reward: -115.25\n",
      "Iteration 6045, Average Reward: -111.74\n",
      "Iteration 6046, Average Reward: -120.93\n",
      "Iteration 6047, Average Reward: -133.58\n",
      "Iteration 6048, Average Reward: -123.68\n",
      "Iteration 6049, Average Reward: -131.72\n",
      "Iteration 6050, Average Reward: -134.74\n",
      "Iteration 6051, Average Reward: -129.04\n",
      "Iteration 6052, Average Reward: -121.28\n",
      "Iteration 6053, Average Reward: -110.84\n",
      "Iteration 6054, Average Reward: -96.79\n",
      "Iteration 6055, Average Reward: -97.67\n",
      "Iteration 6056, Average Reward: -117.83\n",
      "Iteration 6057, Average Reward: -130.45\n",
      "Iteration 6058, Average Reward: -119.22\n",
      "Iteration 6059, Average Reward: -105.64\n",
      "Iteration 6060, Average Reward: -125.13\n",
      "Iteration 6061, Average Reward: -129.89\n",
      "Iteration 6062, Average Reward: -114.1\n",
      "Iteration 6063, Average Reward: -124.47\n",
      "Iteration 6064, Average Reward: -127.41\n",
      "Iteration 6065, Average Reward: -123.03\n",
      "Iteration 6066, Average Reward: -106.71\n",
      "Iteration 6067, Average Reward: -107.48\n",
      "Iteration 6068, Average Reward: -106.07\n",
      "Iteration 6069, Average Reward: -106.39\n",
      "Iteration 6070, Average Reward: -113.01\n",
      "Iteration 6071, Average Reward: -124.0\n",
      "Iteration 6072, Average Reward: -146.96\n",
      "Iteration 6073, Average Reward: -149.21\n",
      "Iteration 6074, Average Reward: -121.84\n",
      "Iteration 6075, Average Reward: -120.28\n",
      "Iteration 6076, Average Reward: -103.23\n",
      "Iteration 6077, Average Reward: -126.15\n",
      "Iteration 6078, Average Reward: -131.8\n",
      "Iteration 6079, Average Reward: -134.95\n",
      "Iteration 6080, Average Reward: -124.76\n",
      "Iteration 6081, Average Reward: -127.5\n",
      "Iteration 6082, Average Reward: -119.11\n",
      "Iteration 6083, Average Reward: -114.37\n",
      "Iteration 6084, Average Reward: -121.63\n",
      "Iteration 6085, Average Reward: -132.34\n",
      "Iteration 6086, Average Reward: -148.73\n",
      "Iteration 6087, Average Reward: -134.44\n",
      "Iteration 6088, Average Reward: -136.08\n",
      "Iteration 6089, Average Reward: -131.64\n",
      "Iteration 6090, Average Reward: -130.95\n",
      "Iteration 6091, Average Reward: -133.48\n",
      "Iteration 6092, Average Reward: -114.79\n",
      "Iteration 6093, Average Reward: -108.07\n",
      "Iteration 6094, Average Reward: -114.83\n",
      "Iteration 6095, Average Reward: -116.33\n",
      "Iteration 6096, Average Reward: -125.9\n",
      "Iteration 6097, Average Reward: -129.01\n",
      "Iteration 6098, Average Reward: -126.68\n",
      "Iteration 6099, Average Reward: -130.72\n",
      "Iteration 6100, Average Reward: -127.12\n",
      "Iteration 6101, Average Reward: -103.92\n",
      "Iteration 6102, Average Reward: -106.02\n",
      "Iteration 6103, Average Reward: -108.9\n",
      "Iteration 6104, Average Reward: -99.24\n",
      "Iteration 6105, Average Reward: -112.79\n",
      "Iteration 6106, Average Reward: -133.3\n",
      "Iteration 6107, Average Reward: -123.29\n",
      "Iteration 6108, Average Reward: -127.35\n",
      "Iteration 6109, Average Reward: -133.09\n",
      "Iteration 6110, Average Reward: -131.72\n",
      "Iteration 6111, Average Reward: -127.13\n",
      "Iteration 6112, Average Reward: -135.06\n",
      "Iteration 6113, Average Reward: -122.09\n",
      "Iteration 6114, Average Reward: -109.03\n",
      "Iteration 6115, Average Reward: -109.26\n",
      "Iteration 6116, Average Reward: -116.03\n",
      "Iteration 6117, Average Reward: -108.52\n",
      "Iteration 6118, Average Reward: -116.55\n",
      "Iteration 6119, Average Reward: -114.54\n",
      "Iteration 6120, Average Reward: -124.63\n",
      "Iteration 6121, Average Reward: -125.95\n",
      "Iteration 6122, Average Reward: -130.11\n",
      "Iteration 6123, Average Reward: -130.84\n",
      "Iteration 6124, Average Reward: -124.57\n",
      "Iteration 6125, Average Reward: -123.46\n",
      "Iteration 6126, Average Reward: -111.2\n",
      "Iteration 6127, Average Reward: -125.82\n",
      "Iteration 6128, Average Reward: -132.65\n",
      "Iteration 6129, Average Reward: -116.32\n",
      "Iteration 6130, Average Reward: -105.13\n",
      "Iteration 6131, Average Reward: -125.73\n",
      "Iteration 6132, Average Reward: -118.1\n",
      "Iteration 6133, Average Reward: -110.05\n",
      "Iteration 6134, Average Reward: -110.61\n",
      "Iteration 6135, Average Reward: -124.85\n",
      "Iteration 6136, Average Reward: -120.21\n",
      "Iteration 6137, Average Reward: -134.02\n",
      "Iteration 6138, Average Reward: -134.88\n",
      "Iteration 6139, Average Reward: -123.8\n",
      "Iteration 6140, Average Reward: -113.82\n",
      "Iteration 6141, Average Reward: -113.56\n",
      "Iteration 6142, Average Reward: -113.58\n",
      "Iteration 6143, Average Reward: -112.37\n",
      "Iteration 6144, Average Reward: -112.54\n",
      "Iteration 6145, Average Reward: -118.07\n",
      "Iteration 6146, Average Reward: -114.01\n",
      "Iteration 6147, Average Reward: -116.6\n",
      "Iteration 6148, Average Reward: -118.95\n",
      "Iteration 6149, Average Reward: -119.45\n",
      "Iteration 6150, Average Reward: -115.9\n",
      "Iteration 6151, Average Reward: -118.69\n",
      "Iteration 6152, Average Reward: -112.19\n",
      "Iteration 6153, Average Reward: -127.95\n",
      "Iteration 6154, Average Reward: -118.89\n",
      "Iteration 6155, Average Reward: -110.3\n",
      "Iteration 6156, Average Reward: -120.09\n",
      "Iteration 6157, Average Reward: -117.61\n",
      "Iteration 6158, Average Reward: -126.24\n",
      "Iteration 6159, Average Reward: -137.51\n",
      "Iteration 6160, Average Reward: -153.35\n",
      "Iteration 6161, Average Reward: -143.63\n",
      "Iteration 6162, Average Reward: -122.86\n",
      "Iteration 6163, Average Reward: -109.58\n",
      "Iteration 6164, Average Reward: -107.77\n",
      "Iteration 6165, Average Reward: -106.74\n",
      "Iteration 6166, Average Reward: -96.83\n",
      "Iteration 6167, Average Reward: -120.06\n",
      "Iteration 6168, Average Reward: -126.68\n",
      "Iteration 6169, Average Reward: -130.43\n",
      "Iteration 6170, Average Reward: -139.95\n",
      "Iteration 6171, Average Reward: -139.23\n",
      "Iteration 6172, Average Reward: -125.74\n",
      "Iteration 6173, Average Reward: -128.6\n",
      "Iteration 6174, Average Reward: -122.66\n",
      "Iteration 6175, Average Reward: -115.38\n",
      "Iteration 6176, Average Reward: -112.36\n",
      "Iteration 6177, Average Reward: -129.52\n",
      "Iteration 6178, Average Reward: -135.03\n",
      "Iteration 6179, Average Reward: -124.38\n",
      "Iteration 6180, Average Reward: -131.48\n",
      "Iteration 6181, Average Reward: -132.91\n",
      "Iteration 6182, Average Reward: -130.6\n",
      "Iteration 6183, Average Reward: -126.7\n",
      "Iteration 6184, Average Reward: -123.2\n",
      "Iteration 6185, Average Reward: -119.48\n",
      "Iteration 6186, Average Reward: -112.42\n",
      "Iteration 6187, Average Reward: -118.57\n",
      "Iteration 6188, Average Reward: -101.79\n",
      "Iteration 6189, Average Reward: -126.47\n",
      "Iteration 6190, Average Reward: -130.78\n",
      "Iteration 6191, Average Reward: -118.82\n",
      "Iteration 6192, Average Reward: -126.3\n",
      "Iteration 6193, Average Reward: -119.44\n",
      "Iteration 6194, Average Reward: -127.11\n",
      "Iteration 6195, Average Reward: -124.09\n",
      "Iteration 6196, Average Reward: -121.52\n",
      "Iteration 6197, Average Reward: -116.35\n",
      "Iteration 6198, Average Reward: -106.97\n",
      "Iteration 6199, Average Reward: -108.37\n",
      "Iteration 6200, Average Reward: -114.02\n",
      "Iteration 6201, Average Reward: -122.82\n",
      "Iteration 6202, Average Reward: -127.51\n",
      "Iteration 6203, Average Reward: -131.74\n",
      "Iteration 6204, Average Reward: -130.05\n",
      "Iteration 6205, Average Reward: -134.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6206, Average Reward: -134.58\n",
      "Iteration 6207, Average Reward: -129.32\n",
      "Iteration 6208, Average Reward: -121.37\n",
      "Iteration 6209, Average Reward: -114.69\n",
      "Iteration 6210, Average Reward: -129.87\n",
      "Iteration 6211, Average Reward: -126.79\n",
      "Iteration 6212, Average Reward: -120.03\n",
      "Iteration 6213, Average Reward: -119.77\n",
      "Iteration 6214, Average Reward: -118.5\n",
      "Iteration 6215, Average Reward: -125.57\n",
      "Iteration 6216, Average Reward: -115.2\n",
      "Iteration 6217, Average Reward: -124.12\n",
      "Iteration 6218, Average Reward: -102.53\n",
      "Iteration 6219, Average Reward: -102.28\n",
      "Iteration 6220, Average Reward: -104.13\n",
      "Iteration 6221, Average Reward: -124.86\n",
      "Iteration 6222, Average Reward: -131.56\n",
      "Iteration 6223, Average Reward: -118.02\n",
      "Iteration 6224, Average Reward: -141.15\n",
      "Iteration 6225, Average Reward: -139.91\n",
      "Iteration 6226, Average Reward: -136.81\n",
      "Iteration 6227, Average Reward: -139.51\n",
      "Iteration 6228, Average Reward: -119.84\n",
      "Iteration 6229, Average Reward: -118.65\n",
      "Iteration 6230, Average Reward: -124.58\n",
      "Iteration 6231, Average Reward: -113.94\n",
      "Iteration 6232, Average Reward: -118.7\n",
      "Iteration 6233, Average Reward: -118.67\n",
      "Iteration 6234, Average Reward: -123.82\n",
      "Iteration 6235, Average Reward: -137.65\n",
      "Iteration 6236, Average Reward: -136.54\n",
      "Iteration 6237, Average Reward: -125.69\n",
      "Iteration 6238, Average Reward: -116.38\n",
      "Iteration 6239, Average Reward: -126.82\n",
      "Iteration 6240, Average Reward: -124.27\n",
      "Iteration 6241, Average Reward: -118.57\n",
      "Iteration 6242, Average Reward: -114.22\n",
      "Iteration 6243, Average Reward: -106.85\n",
      "Iteration 6244, Average Reward: -116.24\n",
      "Iteration 6245, Average Reward: -118.76\n",
      "Iteration 6246, Average Reward: -125.97\n",
      "Iteration 6247, Average Reward: -120.27\n",
      "Iteration 6248, Average Reward: -127.77\n",
      "Iteration 6249, Average Reward: -122.06\n",
      "Iteration 6250, Average Reward: -113.12\n",
      "Iteration 6251, Average Reward: -97.39\n",
      "Iteration 6252, Average Reward: -106.77\n",
      "Iteration 6253, Average Reward: -102.38\n",
      "Iteration 6254, Average Reward: -109.93\n",
      "Iteration 6255, Average Reward: -113.76\n",
      "Iteration 6256, Average Reward: -122.44\n",
      "Iteration 6257, Average Reward: -134.7\n",
      "Iteration 6258, Average Reward: -146.99\n",
      "Iteration 6259, Average Reward: -139.78\n",
      "Iteration 6260, Average Reward: -152.26\n",
      "Iteration 6261, Average Reward: -142.43\n",
      "Iteration 6262, Average Reward: -120.87\n",
      "Iteration 6263, Average Reward: -121.49\n",
      "Iteration 6264, Average Reward: -114.95\n",
      "Iteration 6265, Average Reward: -116.36\n",
      "Iteration 6266, Average Reward: -116.15\n",
      "Iteration 6267, Average Reward: -110.97\n",
      "Iteration 6268, Average Reward: -125.6\n",
      "Iteration 6269, Average Reward: -114.35\n",
      "Iteration 6270, Average Reward: -130.87\n",
      "Iteration 6271, Average Reward: -131.39\n",
      "Iteration 6272, Average Reward: -110.84\n",
      "Iteration 6273, Average Reward: -123.2\n",
      "Iteration 6274, Average Reward: -132.78\n",
      "Iteration 6275, Average Reward: -124.15\n",
      "Iteration 6276, Average Reward: -119.0\n",
      "Iteration 6277, Average Reward: -109.04\n",
      "Iteration 6278, Average Reward: -112.55\n",
      "Iteration 6279, Average Reward: -125.75\n",
      "Iteration 6280, Average Reward: -135.1\n",
      "Iteration 6281, Average Reward: -136.8\n",
      "Iteration 6282, Average Reward: -139.94\n",
      "Iteration 6283, Average Reward: -121.37\n",
      "Iteration 6284, Average Reward: -124.96\n",
      "Iteration 6285, Average Reward: -129.7\n",
      "Iteration 6286, Average Reward: -129.82\n",
      "Iteration 6287, Average Reward: -115.22\n",
      "Iteration 6288, Average Reward: -99.56\n",
      "Iteration 6289, Average Reward: -107.31\n",
      "Iteration 6290, Average Reward: -108.01\n",
      "Iteration 6291, Average Reward: -112.42\n",
      "Iteration 6292, Average Reward: -124.34\n",
      "Iteration 6293, Average Reward: -142.0\n",
      "Iteration 6294, Average Reward: -133.51\n",
      "Iteration 6295, Average Reward: -128.8\n",
      "Iteration 6296, Average Reward: -118.36\n",
      "Iteration 6297, Average Reward: -118.84\n",
      "Iteration 6298, Average Reward: -123.27\n",
      "Iteration 6299, Average Reward: -105.81\n",
      "Iteration 6300, Average Reward: -119.28\n",
      "Iteration 6301, Average Reward: -140.61\n",
      "Iteration 6302, Average Reward: -141.04\n",
      "Iteration 6303, Average Reward: -125.18\n",
      "Iteration 6304, Average Reward: -121.61\n",
      "Iteration 6305, Average Reward: -124.94\n",
      "Iteration 6306, Average Reward: -143.25\n",
      "Iteration 6307, Average Reward: -144.22\n",
      "Iteration 6308, Average Reward: -127.87\n",
      "Iteration 6309, Average Reward: -132.48\n",
      "Iteration 6310, Average Reward: -133.42\n",
      "Iteration 6311, Average Reward: -111.64\n",
      "Iteration 6312, Average Reward: -108.25\n",
      "Iteration 6313, Average Reward: -118.62\n",
      "Iteration 6314, Average Reward: -133.34\n",
      "Iteration 6315, Average Reward: -149.44\n",
      "Iteration 6316, Average Reward: -144.8\n",
      "Iteration 6317, Average Reward: -128.38\n",
      "Iteration 6318, Average Reward: -132.46\n",
      "Iteration 6319, Average Reward: -126.3\n",
      "Iteration 6320, Average Reward: -112.99\n",
      "Iteration 6321, Average Reward: -110.47\n",
      "Iteration 6322, Average Reward: -104.73\n",
      "Iteration 6323, Average Reward: -117.43\n",
      "Iteration 6324, Average Reward: -103.99\n",
      "Iteration 6325, Average Reward: -114.59\n",
      "Iteration 6326, Average Reward: -129.8\n",
      "Iteration 6327, Average Reward: -142.22\n",
      "Iteration 6328, Average Reward: -126.98\n",
      "Iteration 6329, Average Reward: -134.03\n",
      "Iteration 6330, Average Reward: -127.04\n",
      "Iteration 6331, Average Reward: -128.54\n",
      "Iteration 6332, Average Reward: -132.53\n",
      "Iteration 6333, Average Reward: -117.15\n",
      "Iteration 6334, Average Reward: -117.32\n",
      "Iteration 6335, Average Reward: -114.17\n",
      "Iteration 6336, Average Reward: -116.1\n",
      "Iteration 6337, Average Reward: -107.85\n",
      "Iteration 6338, Average Reward: -125.85\n",
      "Iteration 6339, Average Reward: -125.53\n",
      "Iteration 6340, Average Reward: -137.92\n",
      "Iteration 6341, Average Reward: -129.52\n",
      "Iteration 6342, Average Reward: -125.89\n",
      "Iteration 6343, Average Reward: -135.81\n",
      "Iteration 6344, Average Reward: -131.1\n",
      "Iteration 6345, Average Reward: -125.66\n",
      "Iteration 6346, Average Reward: -137.63\n",
      "Iteration 6347, Average Reward: -109.21\n",
      "Iteration 6348, Average Reward: -115.7\n",
      "Iteration 6349, Average Reward: -128.35\n",
      "Iteration 6350, Average Reward: -129.42\n",
      "Iteration 6351, Average Reward: -136.56\n",
      "Iteration 6352, Average Reward: -143.1\n",
      "Iteration 6353, Average Reward: -131.87\n",
      "Iteration 6354, Average Reward: -126.24\n",
      "Iteration 6355, Average Reward: -112.94\n",
      "Iteration 6356, Average Reward: -99.19\n",
      "Iteration 6357, Average Reward: -115.01\n",
      "Iteration 6358, Average Reward: -137.29\n",
      "Iteration 6359, Average Reward: -135.1\n",
      "Iteration 6360, Average Reward: -122.37\n",
      "Iteration 6361, Average Reward: -116.84\n",
      "Iteration 6362, Average Reward: -120.57\n",
      "Iteration 6363, Average Reward: -116.22\n",
      "Iteration 6364, Average Reward: -110.49\n",
      "Iteration 6365, Average Reward: -120.26\n",
      "Iteration 6366, Average Reward: -141.58\n",
      "Iteration 6367, Average Reward: -135.32\n",
      "Iteration 6368, Average Reward: -129.57\n",
      "Iteration 6369, Average Reward: -128.08\n",
      "Iteration 6370, Average Reward: -109.44\n",
      "Iteration 6371, Average Reward: -96.05\n",
      "Iteration 6372, Average Reward: -99.54\n",
      "Iteration 6373, Average Reward: -125.52\n",
      "Iteration 6374, Average Reward: -121.38\n",
      "Iteration 6375, Average Reward: -127.5\n",
      "Iteration 6376, Average Reward: -143.15\n",
      "Iteration 6377, Average Reward: -151.71\n",
      "Iteration 6378, Average Reward: -138.88\n",
      "Iteration 6379, Average Reward: -129.39\n",
      "Iteration 6380, Average Reward: -133.32\n",
      "Iteration 6381, Average Reward: -116.38\n",
      "Iteration 6382, Average Reward: -120.05\n",
      "Iteration 6383, Average Reward: -125.81\n",
      "Iteration 6384, Average Reward: -132.93\n",
      "Iteration 6385, Average Reward: -133.38\n",
      "Iteration 6386, Average Reward: -128.2\n",
      "Iteration 6387, Average Reward: -113.89\n",
      "Iteration 6388, Average Reward: -107.15\n",
      "Iteration 6389, Average Reward: -114.31\n",
      "Iteration 6390, Average Reward: -128.94\n",
      "Iteration 6391, Average Reward: -127.88\n",
      "Iteration 6392, Average Reward: -124.96\n",
      "Iteration 6393, Average Reward: -133.06\n",
      "Iteration 6394, Average Reward: -137.95\n",
      "Iteration 6395, Average Reward: -122.98\n",
      "Iteration 6396, Average Reward: -122.4\n",
      "Iteration 6397, Average Reward: -107.27\n",
      "Iteration 6398, Average Reward: -109.12\n",
      "Iteration 6399, Average Reward: -133.84\n",
      "Iteration 6400, Average Reward: -137.69\n",
      "Iteration 6401, Average Reward: -125.94\n",
      "Iteration 6402, Average Reward: -123.36\n",
      "Iteration 6403, Average Reward: -136.91\n",
      "Iteration 6404, Average Reward: -127.83\n",
      "Iteration 6405, Average Reward: -132.82\n",
      "Iteration 6406, Average Reward: -125.83\n",
      "Iteration 6407, Average Reward: -124.15\n",
      "Iteration 6408, Average Reward: -110.44\n",
      "Iteration 6409, Average Reward: -113.46\n",
      "Iteration 6410, Average Reward: -117.06\n",
      "Iteration 6411, Average Reward: -119.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6412, Average Reward: -121.92\n",
      "Iteration 6413, Average Reward: -137.82\n",
      "Iteration 6414, Average Reward: -129.32\n",
      "Iteration 6415, Average Reward: -118.53\n",
      "Iteration 6416, Average Reward: -121.24\n",
      "Iteration 6417, Average Reward: -131.6\n",
      "Iteration 6418, Average Reward: -121.21\n",
      "Iteration 6419, Average Reward: -128.42\n",
      "Iteration 6420, Average Reward: -137.97\n",
      "Iteration 6421, Average Reward: -119.81\n",
      "Iteration 6422, Average Reward: -113.37\n",
      "Iteration 6423, Average Reward: -112.54\n",
      "Iteration 6424, Average Reward: -126.65\n",
      "Iteration 6425, Average Reward: -111.47\n",
      "Iteration 6426, Average Reward: -103.55\n",
      "Iteration 6427, Average Reward: -124.5\n",
      "Iteration 6428, Average Reward: -121.22\n",
      "Iteration 6429, Average Reward: -125.53\n",
      "Iteration 6430, Average Reward: -134.22\n",
      "Iteration 6431, Average Reward: -139.01\n",
      "Iteration 6432, Average Reward: -122.28\n",
      "Iteration 6433, Average Reward: -124.31\n",
      "Iteration 6434, Average Reward: -118.94\n",
      "Iteration 6435, Average Reward: -116.63\n",
      "Iteration 6436, Average Reward: -120.72\n",
      "Iteration 6437, Average Reward: -119.18\n",
      "Iteration 6438, Average Reward: -117.74\n",
      "Iteration 6439, Average Reward: -123.34\n",
      "Iteration 6440, Average Reward: -134.76\n",
      "Iteration 6441, Average Reward: -130.52\n",
      "Iteration 6442, Average Reward: -140.09\n",
      "Iteration 6443, Average Reward: -134.21\n",
      "Iteration 6444, Average Reward: -139.68\n",
      "Iteration 6445, Average Reward: -122.39\n",
      "Iteration 6446, Average Reward: -105.55\n",
      "Iteration 6447, Average Reward: -118.23\n",
      "Iteration 6448, Average Reward: -131.63\n",
      "Iteration 6449, Average Reward: -117.85\n",
      "Iteration 6450, Average Reward: -115.52\n",
      "Iteration 6451, Average Reward: -130.46\n",
      "Iteration 6452, Average Reward: -127.44\n",
      "Iteration 6453, Average Reward: -116.89\n",
      "Iteration 6454, Average Reward: -122.58\n",
      "Iteration 6455, Average Reward: -117.67\n",
      "Iteration 6456, Average Reward: -118.07\n",
      "Iteration 6457, Average Reward: -124.89\n",
      "Iteration 6458, Average Reward: -122.81\n",
      "Iteration 6459, Average Reward: -133.43\n",
      "Iteration 6460, Average Reward: -144.86\n",
      "Iteration 6461, Average Reward: -124.06\n",
      "Iteration 6462, Average Reward: -127.86\n",
      "Iteration 6463, Average Reward: -128.95\n",
      "Iteration 6464, Average Reward: -110.36\n",
      "Iteration 6465, Average Reward: -129.75\n",
      "Iteration 6466, Average Reward: -134.86\n",
      "Iteration 6467, Average Reward: -128.13\n",
      "Iteration 6468, Average Reward: -132.69\n",
      "Iteration 6469, Average Reward: -120.17\n",
      "Iteration 6470, Average Reward: -124.18\n",
      "Iteration 6471, Average Reward: -128.89\n",
      "Iteration 6472, Average Reward: -120.05\n",
      "Iteration 6473, Average Reward: -108.66\n",
      "Iteration 6474, Average Reward: -127.44\n",
      "Iteration 6475, Average Reward: -120.43\n",
      "Iteration 6476, Average Reward: -125.54\n",
      "Iteration 6477, Average Reward: -118.65\n",
      "Iteration 6478, Average Reward: -137.23\n",
      "Iteration 6479, Average Reward: -160.27\n",
      "Iteration 6480, Average Reward: -148.69\n",
      "Iteration 6481, Average Reward: -127.0\n",
      "Iteration 6482, Average Reward: -124.26\n",
      "Iteration 6483, Average Reward: -109.4\n",
      "Iteration 6484, Average Reward: -107.37\n",
      "Iteration 6485, Average Reward: -115.11\n",
      "Iteration 6486, Average Reward: -120.54\n",
      "Iteration 6487, Average Reward: -118.45\n",
      "Iteration 6488, Average Reward: -116.73\n",
      "Iteration 6489, Average Reward: -125.08\n",
      "Iteration 6490, Average Reward: -125.88\n",
      "Iteration 6491, Average Reward: -119.37\n",
      "Iteration 6492, Average Reward: -111.73\n",
      "Iteration 6493, Average Reward: -120.45\n",
      "Iteration 6494, Average Reward: -122.17\n",
      "Iteration 6495, Average Reward: -118.17\n",
      "Iteration 6496, Average Reward: -119.4\n",
      "Iteration 6497, Average Reward: -118.5\n",
      "Iteration 6498, Average Reward: -122.13\n",
      "Iteration 6499, Average Reward: -125.75\n",
      "Iteration 6500, Average Reward: -120.37\n",
      "Iteration 6501, Average Reward: -125.67\n",
      "Iteration 6502, Average Reward: -123.54\n",
      "Iteration 6503, Average Reward: -106.13\n",
      "Iteration 6504, Average Reward: -121.17\n",
      "Iteration 6505, Average Reward: -126.12\n",
      "Iteration 6506, Average Reward: -129.82\n",
      "Iteration 6507, Average Reward: -136.52\n",
      "Iteration 6508, Average Reward: -133.11\n",
      "Iteration 6509, Average Reward: -112.0\n",
      "Iteration 6510, Average Reward: -128.19\n",
      "Iteration 6511, Average Reward: -118.53\n",
      "Iteration 6512, Average Reward: -95.05\n",
      "Iteration 6513, Average Reward: -104.84\n",
      "Iteration 6514, Average Reward: -116.98\n",
      "Iteration 6515, Average Reward: -128.31\n",
      "Iteration 6516, Average Reward: -145.03\n",
      "Iteration 6517, Average Reward: -148.85\n",
      "Iteration 6518, Average Reward: -136.72\n",
      "Iteration 6519, Average Reward: -120.91\n",
      "Iteration 6520, Average Reward: -125.34\n",
      "Iteration 6521, Average Reward: -121.23\n",
      "Iteration 6522, Average Reward: -95.04\n",
      "Iteration 6523, Average Reward: -105.1\n",
      "Iteration 6524, Average Reward: -127.64\n",
      "Iteration 6525, Average Reward: -135.51\n",
      "Iteration 6526, Average Reward: -144.12\n",
      "Iteration 6527, Average Reward: -140.84\n",
      "Iteration 6528, Average Reward: -120.97\n",
      "Iteration 6529, Average Reward: -114.94\n",
      "Iteration 6530, Average Reward: -110.61\n",
      "Iteration 6531, Average Reward: -102.02\n",
      "Iteration 6532, Average Reward: -113.01\n",
      "Iteration 6533, Average Reward: -117.27\n",
      "Iteration 6534, Average Reward: -124.86\n",
      "Iteration 6535, Average Reward: -116.57\n",
      "Iteration 6536, Average Reward: -134.38\n",
      "Iteration 6537, Average Reward: -137.93\n",
      "Iteration 6538, Average Reward: -124.34\n",
      "Iteration 6539, Average Reward: -100.86\n",
      "Iteration 6540, Average Reward: -91.24\n",
      "Iteration 6541, Average Reward: -87.65\n",
      "Iteration 6542, Average Reward: -100.78\n",
      "Iteration 6543, Average Reward: -127.37\n",
      "Iteration 6544, Average Reward: -131.39\n",
      "Iteration 6545, Average Reward: -137.85\n",
      "Iteration 6546, Average Reward: -156.94\n",
      "Iteration 6547, Average Reward: -145.46\n",
      "Iteration 6548, Average Reward: -129.32\n",
      "Iteration 6549, Average Reward: -138.98\n",
      "Iteration 6550, Average Reward: -108.83\n",
      "Iteration 6551, Average Reward: -101.88\n",
      "Iteration 6552, Average Reward: -93.16\n",
      "Iteration 6553, Average Reward: -95.45\n",
      "Iteration 6554, Average Reward: -92.76\n",
      "Iteration 6555, Average Reward: -118.35\n",
      "Iteration 6556, Average Reward: -111.67\n",
      "Iteration 6557, Average Reward: -125.78\n",
      "Iteration 6558, Average Reward: -142.68\n",
      "Iteration 6559, Average Reward: -153.73\n",
      "Iteration 6560, Average Reward: -134.52\n",
      "Iteration 6561, Average Reward: -129.47\n",
      "Iteration 6562, Average Reward: -116.75\n",
      "Iteration 6563, Average Reward: -120.99\n",
      "Iteration 6564, Average Reward: -108.85\n",
      "Iteration 6565, Average Reward: -98.9\n",
      "Iteration 6566, Average Reward: -108.19\n",
      "Iteration 6567, Average Reward: -110.6\n",
      "Iteration 6568, Average Reward: -103.5\n",
      "Iteration 6569, Average Reward: -119.13\n",
      "Iteration 6570, Average Reward: -125.56\n",
      "Iteration 6571, Average Reward: -125.01\n",
      "Iteration 6572, Average Reward: -128.27\n",
      "Iteration 6573, Average Reward: -135.19\n",
      "Iteration 6574, Average Reward: -132.95\n",
      "Iteration 6575, Average Reward: -132.6\n",
      "Iteration 6576, Average Reward: -121.29\n",
      "Iteration 6577, Average Reward: -115.64\n",
      "Iteration 6578, Average Reward: -107.59\n",
      "Iteration 6579, Average Reward: -99.71\n",
      "Iteration 6580, Average Reward: -119.8\n",
      "Iteration 6581, Average Reward: -129.07\n",
      "Iteration 6582, Average Reward: -139.81\n",
      "Iteration 6583, Average Reward: -141.67\n",
      "Iteration 6584, Average Reward: -135.17\n",
      "Iteration 6585, Average Reward: -101.46\n",
      "Iteration 6586, Average Reward: -110.35\n",
      "Iteration 6587, Average Reward: -114.96\n",
      "Iteration 6588, Average Reward: -117.11\n",
      "Iteration 6589, Average Reward: -118.89\n",
      "Iteration 6590, Average Reward: -136.78\n",
      "Iteration 6591, Average Reward: -134.58\n",
      "Iteration 6592, Average Reward: -106.01\n",
      "Iteration 6593, Average Reward: -104.63\n",
      "Iteration 6594, Average Reward: -120.32\n",
      "Iteration 6595, Average Reward: -144.82\n",
      "Iteration 6596, Average Reward: -119.66\n",
      "Iteration 6597, Average Reward: -99.92\n",
      "Iteration 6598, Average Reward: -107.16\n",
      "Iteration 6599, Average Reward: -126.36\n",
      "Iteration 6600, Average Reward: -120.74\n",
      "Iteration 6601, Average Reward: -112.19\n",
      "Iteration 6602, Average Reward: -124.27\n",
      "Iteration 6603, Average Reward: -125.37\n",
      "Iteration 6604, Average Reward: -124.59\n",
      "Iteration 6605, Average Reward: -124.64\n",
      "Iteration 6606, Average Reward: -126.93\n",
      "Iteration 6607, Average Reward: -133.0\n",
      "Iteration 6608, Average Reward: -123.96\n",
      "Iteration 6609, Average Reward: -119.89\n",
      "Iteration 6610, Average Reward: -115.8\n",
      "Iteration 6611, Average Reward: -108.51\n",
      "Iteration 6612, Average Reward: -121.59\n",
      "Iteration 6613, Average Reward: -135.61\n",
      "Iteration 6614, Average Reward: -121.88\n",
      "Iteration 6615, Average Reward: -135.64\n",
      "Iteration 6616, Average Reward: -159.04\n",
      "Iteration 6617, Average Reward: -153.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6618, Average Reward: -123.02\n",
      "Iteration 6619, Average Reward: -103.74\n",
      "Iteration 6620, Average Reward: -109.07\n",
      "Iteration 6621, Average Reward: -124.44\n",
      "Iteration 6622, Average Reward: -120.11\n",
      "Iteration 6623, Average Reward: -118.16\n",
      "Iteration 6624, Average Reward: -128.75\n",
      "Iteration 6625, Average Reward: -133.93\n",
      "Iteration 6626, Average Reward: -139.5\n",
      "Iteration 6627, Average Reward: -127.99\n",
      "Iteration 6628, Average Reward: -127.37\n",
      "Iteration 6629, Average Reward: -109.86\n",
      "Iteration 6630, Average Reward: -102.54\n",
      "Iteration 6631, Average Reward: -111.1\n",
      "Iteration 6632, Average Reward: -121.87\n",
      "Iteration 6633, Average Reward: -134.94\n",
      "Iteration 6634, Average Reward: -150.71\n",
      "Iteration 6635, Average Reward: -146.12\n",
      "Iteration 6636, Average Reward: -140.15\n",
      "Iteration 6637, Average Reward: -120.4\n",
      "Iteration 6638, Average Reward: -111.27\n",
      "Iteration 6639, Average Reward: -123.61\n",
      "Iteration 6640, Average Reward: -133.97\n",
      "Iteration 6641, Average Reward: -124.4\n",
      "Iteration 6642, Average Reward: -143.48\n",
      "Iteration 6643, Average Reward: -122.53\n",
      "Iteration 6644, Average Reward: -129.9\n",
      "Iteration 6645, Average Reward: -120.52\n",
      "Iteration 6646, Average Reward: -113.99\n",
      "Iteration 6647, Average Reward: -115.42\n",
      "Iteration 6648, Average Reward: -137.14\n",
      "Iteration 6649, Average Reward: -114.14\n",
      "Iteration 6650, Average Reward: -114.74\n",
      "Iteration 6651, Average Reward: -122.24\n",
      "Iteration 6652, Average Reward: -134.93\n",
      "Iteration 6653, Average Reward: -140.33\n",
      "Iteration 6654, Average Reward: -142.63\n",
      "Iteration 6655, Average Reward: -134.69\n",
      "Iteration 6656, Average Reward: -130.67\n",
      "Iteration 6657, Average Reward: -139.96\n",
      "Iteration 6658, Average Reward: -133.28\n",
      "Iteration 6659, Average Reward: -117.07\n",
      "Iteration 6660, Average Reward: -109.4\n",
      "Iteration 6661, Average Reward: -109.22\n",
      "Iteration 6662, Average Reward: -117.92\n",
      "Iteration 6663, Average Reward: -125.95\n",
      "Iteration 6664, Average Reward: -146.02\n",
      "Iteration 6665, Average Reward: -137.0\n",
      "Iteration 6666, Average Reward: -126.78\n",
      "Iteration 6667, Average Reward: -123.17\n",
      "Iteration 6668, Average Reward: -107.33\n",
      "Iteration 6669, Average Reward: -117.42\n",
      "Iteration 6670, Average Reward: -110.24\n",
      "Iteration 6671, Average Reward: -107.22\n",
      "Iteration 6672, Average Reward: -116.58\n",
      "Iteration 6673, Average Reward: -142.89\n",
      "Iteration 6674, Average Reward: -155.29\n",
      "Iteration 6675, Average Reward: -130.78\n",
      "Iteration 6676, Average Reward: -119.4\n",
      "Iteration 6677, Average Reward: -116.97\n",
      "Iteration 6678, Average Reward: -135.32\n",
      "Iteration 6679, Average Reward: -129.24\n",
      "Iteration 6680, Average Reward: -130.75\n",
      "Iteration 6681, Average Reward: -127.86\n",
      "Iteration 6682, Average Reward: -130.35\n",
      "Iteration 6683, Average Reward: -131.26\n",
      "Iteration 6684, Average Reward: -128.74\n",
      "Iteration 6685, Average Reward: -120.23\n",
      "Iteration 6686, Average Reward: -116.96\n",
      "Iteration 6687, Average Reward: -117.75\n",
      "Iteration 6688, Average Reward: -111.14\n",
      "Iteration 6689, Average Reward: -123.74\n",
      "Iteration 6690, Average Reward: -123.84\n",
      "Iteration 6691, Average Reward: -116.55\n",
      "Iteration 6692, Average Reward: -99.67\n",
      "Iteration 6693, Average Reward: -123.56\n",
      "Iteration 6694, Average Reward: -138.96\n",
      "Iteration 6695, Average Reward: -121.83\n",
      "Iteration 6696, Average Reward: -109.77\n",
      "Iteration 6697, Average Reward: -123.59\n",
      "Iteration 6698, Average Reward: -139.86\n",
      "Iteration 6699, Average Reward: -129.53\n",
      "Iteration 6700, Average Reward: -130.64\n",
      "Iteration 6701, Average Reward: -141.54\n",
      "Iteration 6702, Average Reward: -132.15\n",
      "Iteration 6703, Average Reward: -127.35\n",
      "Iteration 6704, Average Reward: -137.31\n",
      "Iteration 6705, Average Reward: -123.9\n",
      "Iteration 6706, Average Reward: -122.86\n",
      "Iteration 6707, Average Reward: -126.05\n",
      "Iteration 6708, Average Reward: -135.97\n",
      "Iteration 6709, Average Reward: -132.02\n",
      "Iteration 6710, Average Reward: -136.69\n",
      "Iteration 6711, Average Reward: -134.98\n",
      "Iteration 6712, Average Reward: -122.77\n",
      "Iteration 6713, Average Reward: -116.25\n",
      "Iteration 6714, Average Reward: -115.14\n",
      "Iteration 6715, Average Reward: -115.93\n",
      "Iteration 6716, Average Reward: -127.26\n",
      "Iteration 6717, Average Reward: -126.02\n",
      "Iteration 6718, Average Reward: -106.33\n",
      "Iteration 6719, Average Reward: -108.39\n",
      "Iteration 6720, Average Reward: -138.4\n",
      "Iteration 6721, Average Reward: -125.03\n",
      "Iteration 6722, Average Reward: -106.2\n",
      "Iteration 6723, Average Reward: -118.17\n",
      "Iteration 6724, Average Reward: -122.94\n",
      "Iteration 6725, Average Reward: -122.45\n",
      "Iteration 6726, Average Reward: -119.35\n",
      "Iteration 6727, Average Reward: -128.9\n",
      "Iteration 6728, Average Reward: -122.98\n",
      "Iteration 6729, Average Reward: -121.02\n",
      "Iteration 6730, Average Reward: -112.53\n",
      "Iteration 6731, Average Reward: -117.05\n",
      "Iteration 6732, Average Reward: -117.41\n",
      "Iteration 6733, Average Reward: -122.84\n",
      "Iteration 6734, Average Reward: -108.18\n",
      "Iteration 6735, Average Reward: -113.73\n",
      "Iteration 6736, Average Reward: -112.2\n",
      "Iteration 6737, Average Reward: -122.09\n",
      "Iteration 6738, Average Reward: -106.77\n",
      "Iteration 6739, Average Reward: -119.02\n",
      "Iteration 6740, Average Reward: -109.89\n",
      "Iteration 6741, Average Reward: -126.37\n",
      "Iteration 6742, Average Reward: -128.52\n",
      "Iteration 6743, Average Reward: -139.43\n",
      "Iteration 6744, Average Reward: -128.76\n",
      "Iteration 6745, Average Reward: -118.59\n",
      "Iteration 6746, Average Reward: -124.79\n",
      "Iteration 6747, Average Reward: -135.61\n",
      "Iteration 6748, Average Reward: -130.13\n",
      "Iteration 6749, Average Reward: -120.43\n",
      "Iteration 6750, Average Reward: -130.42\n",
      "Iteration 6751, Average Reward: -137.7\n",
      "Iteration 6752, Average Reward: -132.15\n",
      "Iteration 6753, Average Reward: -138.08\n",
      "Iteration 6754, Average Reward: -124.78\n",
      "Iteration 6755, Average Reward: -116.42\n",
      "Iteration 6756, Average Reward: -125.75\n",
      "Iteration 6757, Average Reward: -122.36\n",
      "Iteration 6758, Average Reward: -116.75\n",
      "Iteration 6759, Average Reward: -117.77\n",
      "Iteration 6760, Average Reward: -104.52\n",
      "Iteration 6761, Average Reward: -114.91\n",
      "Iteration 6762, Average Reward: -117.47\n",
      "Iteration 6763, Average Reward: -119.87\n",
      "Iteration 6764, Average Reward: -123.17\n",
      "Iteration 6765, Average Reward: -116.78\n",
      "Iteration 6766, Average Reward: -117.74\n",
      "Iteration 6767, Average Reward: -131.89\n",
      "Iteration 6768, Average Reward: -129.29\n",
      "Iteration 6769, Average Reward: -128.69\n",
      "Iteration 6770, Average Reward: -135.96\n",
      "Iteration 6771, Average Reward: -130.78\n",
      "Iteration 6772, Average Reward: -120.23\n",
      "Iteration 6773, Average Reward: -127.81\n",
      "Iteration 6774, Average Reward: -154.98\n",
      "Iteration 6775, Average Reward: -137.36\n",
      "Iteration 6776, Average Reward: -118.19\n",
      "Iteration 6777, Average Reward: -114.12\n",
      "Iteration 6778, Average Reward: -113.84\n",
      "Iteration 6779, Average Reward: -111.18\n",
      "Iteration 6780, Average Reward: -115.48\n",
      "Iteration 6781, Average Reward: -122.36\n",
      "Iteration 6782, Average Reward: -118.3\n",
      "Iteration 6783, Average Reward: -120.09\n",
      "Iteration 6784, Average Reward: -115.48\n",
      "Iteration 6785, Average Reward: -113.27\n",
      "Iteration 6786, Average Reward: -115.05\n",
      "Iteration 6787, Average Reward: -132.31\n",
      "Iteration 6788, Average Reward: -139.13\n",
      "Iteration 6789, Average Reward: -136.26\n",
      "Iteration 6790, Average Reward: -136.53\n",
      "Iteration 6791, Average Reward: -124.9\n",
      "Iteration 6792, Average Reward: -118.75\n",
      "Iteration 6793, Average Reward: -129.32\n",
      "Iteration 6794, Average Reward: -122.23\n",
      "Iteration 6795, Average Reward: -124.12\n",
      "Iteration 6796, Average Reward: -123.1\n",
      "Iteration 6797, Average Reward: -105.88\n",
      "Iteration 6798, Average Reward: -115.42\n",
      "Iteration 6799, Average Reward: -120.41\n",
      "Iteration 6800, Average Reward: -117.88\n",
      "Iteration 6801, Average Reward: -118.16\n",
      "Iteration 6802, Average Reward: -127.53\n",
      "Iteration 6803, Average Reward: -122.46\n",
      "Iteration 6804, Average Reward: -114.2\n",
      "Iteration 6805, Average Reward: -122.95\n",
      "Iteration 6806, Average Reward: -116.72\n",
      "Iteration 6807, Average Reward: -97.0\n",
      "Iteration 6808, Average Reward: -111.11\n",
      "Iteration 6809, Average Reward: -135.58\n",
      "Iteration 6810, Average Reward: -120.11\n",
      "Iteration 6811, Average Reward: -119.56\n",
      "Iteration 6812, Average Reward: -122.38\n",
      "Iteration 6813, Average Reward: -114.8\n",
      "Iteration 6814, Average Reward: -110.69\n",
      "Iteration 6815, Average Reward: -125.07\n",
      "Iteration 6816, Average Reward: -125.11\n",
      "Iteration 6817, Average Reward: -105.96\n",
      "Iteration 6818, Average Reward: -99.09\n",
      "Iteration 6819, Average Reward: -112.22\n",
      "Iteration 6820, Average Reward: -131.75\n",
      "Iteration 6821, Average Reward: -120.63\n",
      "Iteration 6822, Average Reward: -110.92\n",
      "Iteration 6823, Average Reward: -114.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6824, Average Reward: -121.16\n",
      "Iteration 6825, Average Reward: -122.29\n",
      "Iteration 6826, Average Reward: -108.3\n",
      "Iteration 6827, Average Reward: -123.25\n",
      "Iteration 6828, Average Reward: -117.36\n",
      "Iteration 6829, Average Reward: -125.88\n",
      "Iteration 6830, Average Reward: -126.71\n",
      "Iteration 6831, Average Reward: -120.89\n",
      "Iteration 6832, Average Reward: -131.06\n",
      "Iteration 6833, Average Reward: -130.97\n",
      "Iteration 6834, Average Reward: -126.81\n",
      "Iteration 6835, Average Reward: -124.96\n",
      "Iteration 6836, Average Reward: -106.57\n",
      "Iteration 6837, Average Reward: -122.48\n",
      "Iteration 6838, Average Reward: -120.43\n",
      "Iteration 6839, Average Reward: -131.15\n",
      "Iteration 6840, Average Reward: -132.39\n",
      "Iteration 6841, Average Reward: -129.87\n",
      "Iteration 6842, Average Reward: -115.98\n",
      "Iteration 6843, Average Reward: -115.28\n",
      "Iteration 6844, Average Reward: -113.47\n",
      "Iteration 6845, Average Reward: -103.26\n",
      "Iteration 6846, Average Reward: -128.8\n",
      "Iteration 6847, Average Reward: -134.87\n",
      "Iteration 6848, Average Reward: -139.55\n",
      "Iteration 6849, Average Reward: -131.05\n",
      "Iteration 6850, Average Reward: -109.61\n",
      "Iteration 6851, Average Reward: -110.17\n",
      "Iteration 6852, Average Reward: -118.66\n",
      "Iteration 6853, Average Reward: -100.19\n",
      "Iteration 6854, Average Reward: -116.24\n",
      "Iteration 6855, Average Reward: -128.33\n",
      "Iteration 6856, Average Reward: -133.13\n",
      "Iteration 6857, Average Reward: -126.94\n",
      "Iteration 6858, Average Reward: -147.3\n",
      "Iteration 6859, Average Reward: -149.45\n",
      "Iteration 6860, Average Reward: -132.46\n",
      "Iteration 6861, Average Reward: -125.36\n",
      "Iteration 6862, Average Reward: -117.84\n",
      "Iteration 6863, Average Reward: -121.34\n",
      "Iteration 6864, Average Reward: -121.37\n",
      "Iteration 6865, Average Reward: -114.76\n",
      "Iteration 6866, Average Reward: -110.67\n",
      "Iteration 6867, Average Reward: -129.02\n",
      "Iteration 6868, Average Reward: -134.45\n",
      "Iteration 6869, Average Reward: -120.59\n",
      "Iteration 6870, Average Reward: -116.82\n",
      "Iteration 6871, Average Reward: -127.68\n",
      "Iteration 6872, Average Reward: -131.16\n",
      "Iteration 6873, Average Reward: -114.74\n",
      "Iteration 6874, Average Reward: -135.24\n",
      "Iteration 6875, Average Reward: -135.97\n",
      "Iteration 6876, Average Reward: -137.21\n",
      "Iteration 6877, Average Reward: -126.92\n",
      "Iteration 6878, Average Reward: -137.62\n",
      "Iteration 6879, Average Reward: -120.52\n",
      "Iteration 6880, Average Reward: -117.35\n",
      "Iteration 6881, Average Reward: -133.14\n",
      "Iteration 6882, Average Reward: -136.8\n",
      "Iteration 6883, Average Reward: -126.24\n",
      "Iteration 6884, Average Reward: -114.61\n",
      "Iteration 6885, Average Reward: -125.15\n",
      "Iteration 6886, Average Reward: -133.49\n",
      "Iteration 6887, Average Reward: -140.55\n",
      "Iteration 6888, Average Reward: -124.26\n",
      "Iteration 6889, Average Reward: -124.45\n",
      "Iteration 6890, Average Reward: -124.22\n",
      "Iteration 6891, Average Reward: -137.79\n",
      "Iteration 6892, Average Reward: -125.37\n",
      "Iteration 6893, Average Reward: -136.18\n",
      "Iteration 6894, Average Reward: -117.11\n",
      "Iteration 6895, Average Reward: -113.88\n",
      "Iteration 6896, Average Reward: -123.55\n",
      "Iteration 6897, Average Reward: -118.04\n",
      "Iteration 6898, Average Reward: -120.69\n",
      "Iteration 6899, Average Reward: -112.27\n",
      "Iteration 6900, Average Reward: -102.25\n",
      "Iteration 6901, Average Reward: -97.97\n",
      "Iteration 6902, Average Reward: -102.87\n",
      "Iteration 6903, Average Reward: -117.4\n",
      "Iteration 6904, Average Reward: -135.88\n",
      "Iteration 6905, Average Reward: -136.03\n",
      "Iteration 6906, Average Reward: -130.6\n",
      "Iteration 6907, Average Reward: -131.8\n",
      "Iteration 6908, Average Reward: -120.54\n",
      "Iteration 6909, Average Reward: -113.58\n",
      "Iteration 6910, Average Reward: -129.53\n",
      "Iteration 6911, Average Reward: -136.33\n",
      "Iteration 6912, Average Reward: -125.24\n",
      "Iteration 6913, Average Reward: -120.84\n",
      "Iteration 6914, Average Reward: -117.53\n",
      "Iteration 6915, Average Reward: -134.37\n",
      "Iteration 6916, Average Reward: -151.45\n",
      "Iteration 6917, Average Reward: -145.85\n",
      "Iteration 6918, Average Reward: -128.73\n",
      "Iteration 6919, Average Reward: -110.42\n",
      "Iteration 6920, Average Reward: -111.32\n",
      "Iteration 6921, Average Reward: -106.36\n",
      "Iteration 6922, Average Reward: -116.27\n",
      "Iteration 6923, Average Reward: -116.56\n",
      "Iteration 6924, Average Reward: -107.81\n",
      "Iteration 6925, Average Reward: -130.68\n",
      "Iteration 6926, Average Reward: -135.32\n",
      "Iteration 6927, Average Reward: -126.08\n",
      "Iteration 6928, Average Reward: -128.95\n",
      "Iteration 6929, Average Reward: -118.01\n",
      "Iteration 6930, Average Reward: -111.6\n",
      "Iteration 6931, Average Reward: -112.63\n",
      "Iteration 6932, Average Reward: -126.39\n",
      "Iteration 6933, Average Reward: -125.92\n",
      "Iteration 6934, Average Reward: -119.83\n",
      "Iteration 6935, Average Reward: -116.1\n",
      "Iteration 6936, Average Reward: -118.27\n",
      "Iteration 6937, Average Reward: -116.0\n",
      "Iteration 6938, Average Reward: -113.98\n",
      "Iteration 6939, Average Reward: -123.77\n",
      "Iteration 6940, Average Reward: -126.93\n",
      "Iteration 6941, Average Reward: -122.22\n",
      "Iteration 6942, Average Reward: -124.65\n",
      "Iteration 6943, Average Reward: -119.0\n",
      "Iteration 6944, Average Reward: -125.38\n",
      "Iteration 6945, Average Reward: -128.52\n",
      "Iteration 6946, Average Reward: -138.67\n",
      "Iteration 6947, Average Reward: -129.54\n",
      "Iteration 6948, Average Reward: -112.85\n",
      "Iteration 6949, Average Reward: -115.49\n",
      "Iteration 6950, Average Reward: -114.39\n",
      "Iteration 6951, Average Reward: -117.51\n",
      "Iteration 6952, Average Reward: -115.75\n",
      "Iteration 6953, Average Reward: -116.87\n",
      "Iteration 6954, Average Reward: -116.1\n",
      "Iteration 6955, Average Reward: -123.32\n",
      "Iteration 6956, Average Reward: -115.06\n",
      "Iteration 6957, Average Reward: -118.04\n",
      "Iteration 6958, Average Reward: -124.79\n",
      "Iteration 6959, Average Reward: -125.74\n",
      "Iteration 6960, Average Reward: -140.71\n",
      "Iteration 6961, Average Reward: -139.71\n",
      "Iteration 6962, Average Reward: -139.55\n",
      "Iteration 6963, Average Reward: -130.77\n",
      "Iteration 6964, Average Reward: -121.51\n",
      "Iteration 6965, Average Reward: -117.25\n",
      "Iteration 6966, Average Reward: -117.26\n",
      "Iteration 6967, Average Reward: -111.25\n",
      "Iteration 6968, Average Reward: -120.14\n",
      "Iteration 6969, Average Reward: -121.97\n",
      "Iteration 6970, Average Reward: -119.23\n",
      "Iteration 6971, Average Reward: -131.72\n",
      "Iteration 6972, Average Reward: -125.88\n",
      "Iteration 6973, Average Reward: -130.51\n",
      "Iteration 6974, Average Reward: -138.94\n",
      "Iteration 6975, Average Reward: -123.08\n",
      "Iteration 6976, Average Reward: -137.69\n",
      "Iteration 6977, Average Reward: -124.79\n",
      "Iteration 6978, Average Reward: -113.57\n",
      "Iteration 6979, Average Reward: -120.85\n",
      "Iteration 6980, Average Reward: -119.56\n",
      "Iteration 6981, Average Reward: -117.67\n",
      "Iteration 6982, Average Reward: -105.24\n",
      "Iteration 6983, Average Reward: -130.26\n",
      "Iteration 6984, Average Reward: -122.42\n",
      "Iteration 6985, Average Reward: -118.56\n",
      "Iteration 6986, Average Reward: -137.9\n",
      "Iteration 6987, Average Reward: -144.42\n",
      "Iteration 6988, Average Reward: -119.83\n",
      "Iteration 6989, Average Reward: -104.49\n",
      "Iteration 6990, Average Reward: -116.94\n",
      "Iteration 6991, Average Reward: -119.96\n",
      "Iteration 6992, Average Reward: -139.47\n",
      "Iteration 6993, Average Reward: -145.28\n",
      "Iteration 6994, Average Reward: -116.9\n",
      "Iteration 6995, Average Reward: -96.1\n",
      "Iteration 6996, Average Reward: -96.48\n",
      "Iteration 6997, Average Reward: -98.38\n",
      "Iteration 6998, Average Reward: -103.88\n",
      "Iteration 6999, Average Reward: -116.41\n",
      "Iteration 7000, Average Reward: -129.25\n",
      "Iteration 7001, Average Reward: -139.4\n",
      "Iteration 7002, Average Reward: -137.86\n",
      "Iteration 7003, Average Reward: -136.2\n",
      "Iteration 7004, Average Reward: -122.19\n",
      "Iteration 7005, Average Reward: -114.67\n",
      "Iteration 7006, Average Reward: -111.97\n",
      "Iteration 7007, Average Reward: -122.1\n",
      "Iteration 7008, Average Reward: -118.33\n",
      "Iteration 7009, Average Reward: -110.31\n",
      "Iteration 7010, Average Reward: -108.99\n",
      "Iteration 7011, Average Reward: -114.21\n",
      "Iteration 7012, Average Reward: -145.31\n",
      "Iteration 7013, Average Reward: -145.76\n",
      "Iteration 7014, Average Reward: -133.29\n",
      "Iteration 7015, Average Reward: -127.62\n",
      "Iteration 7016, Average Reward: -128.25\n",
      "Iteration 7017, Average Reward: -125.63\n",
      "Iteration 7018, Average Reward: -129.83\n",
      "Iteration 7019, Average Reward: -108.1\n",
      "Iteration 7020, Average Reward: -100.59\n",
      "Iteration 7021, Average Reward: -104.48\n",
      "Iteration 7022, Average Reward: -120.99\n",
      "Iteration 7023, Average Reward: -128.71\n",
      "Iteration 7024, Average Reward: -143.31\n",
      "Iteration 7025, Average Reward: -135.49\n",
      "Iteration 7026, Average Reward: -120.85\n",
      "Iteration 7027, Average Reward: -106.53\n",
      "Iteration 7028, Average Reward: -104.77\n",
      "Iteration 7029, Average Reward: -113.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7030, Average Reward: -117.58\n",
      "Iteration 7031, Average Reward: -121.18\n",
      "Iteration 7032, Average Reward: -136.17\n",
      "Iteration 7033, Average Reward: -144.05\n",
      "Iteration 7034, Average Reward: -147.22\n",
      "Iteration 7035, Average Reward: -131.09\n",
      "Iteration 7036, Average Reward: -104.88\n",
      "Iteration 7037, Average Reward: -103.7\n",
      "Iteration 7038, Average Reward: -109.01\n",
      "Iteration 7039, Average Reward: -125.29\n",
      "Iteration 7040, Average Reward: -133.99\n",
      "Iteration 7041, Average Reward: -136.83\n",
      "Iteration 7042, Average Reward: -139.48\n",
      "Iteration 7043, Average Reward: -126.46\n",
      "Iteration 7044, Average Reward: -128.17\n",
      "Iteration 7045, Average Reward: -120.13\n",
      "Iteration 7046, Average Reward: -119.19\n",
      "Iteration 7047, Average Reward: -117.78\n",
      "Iteration 7048, Average Reward: -133.31\n",
      "Iteration 7049, Average Reward: -133.66\n",
      "Iteration 7050, Average Reward: -135.66\n",
      "Iteration 7051, Average Reward: -146.72\n",
      "Iteration 7052, Average Reward: -138.17\n",
      "Iteration 7053, Average Reward: -127.18\n",
      "Iteration 7054, Average Reward: -124.16\n",
      "Iteration 7055, Average Reward: -115.2\n",
      "Iteration 7056, Average Reward: -114.71\n",
      "Iteration 7057, Average Reward: -120.19\n",
      "Iteration 7058, Average Reward: -130.52\n",
      "Iteration 7059, Average Reward: -144.66\n",
      "Iteration 7060, Average Reward: -126.97\n",
      "Iteration 7061, Average Reward: -118.12\n",
      "Iteration 7062, Average Reward: -120.95\n",
      "Iteration 7063, Average Reward: -100.8\n",
      "Iteration 7064, Average Reward: -94.74\n",
      "Iteration 7065, Average Reward: -115.01\n",
      "Iteration 7066, Average Reward: -127.87\n",
      "Iteration 7067, Average Reward: -134.29\n",
      "Iteration 7068, Average Reward: -134.94\n",
      "Iteration 7069, Average Reward: -136.53\n",
      "Iteration 7070, Average Reward: -134.5\n",
      "Iteration 7071, Average Reward: -136.13\n",
      "Iteration 7072, Average Reward: -128.69\n",
      "Iteration 7073, Average Reward: -132.79\n",
      "Iteration 7074, Average Reward: -126.34\n",
      "Iteration 7075, Average Reward: -126.02\n",
      "Iteration 7076, Average Reward: -106.62\n",
      "Iteration 7077, Average Reward: -112.09\n",
      "Iteration 7078, Average Reward: -112.81\n",
      "Iteration 7079, Average Reward: -127.17\n",
      "Iteration 7080, Average Reward: -113.08\n",
      "Iteration 7081, Average Reward: -123.09\n",
      "Iteration 7082, Average Reward: -128.52\n",
      "Iteration 7083, Average Reward: -126.94\n",
      "Iteration 7084, Average Reward: -125.34\n",
      "Iteration 7085, Average Reward: -127.98\n",
      "Iteration 7086, Average Reward: -116.81\n",
      "Iteration 7087, Average Reward: -112.6\n",
      "Iteration 7088, Average Reward: -130.39\n",
      "Iteration 7089, Average Reward: -130.75\n",
      "Iteration 7090, Average Reward: -134.14\n",
      "Iteration 7091, Average Reward: -137.0\n",
      "Iteration 7092, Average Reward: -131.14\n",
      "Iteration 7093, Average Reward: -129.22\n",
      "Iteration 7094, Average Reward: -122.91\n",
      "Iteration 7095, Average Reward: -119.66\n",
      "Iteration 7096, Average Reward: -114.8\n",
      "Iteration 7097, Average Reward: -126.13\n",
      "Iteration 7098, Average Reward: -121.23\n",
      "Iteration 7099, Average Reward: -121.55\n",
      "Iteration 7100, Average Reward: -118.19\n",
      "Iteration 7101, Average Reward: -117.9\n",
      "Iteration 7102, Average Reward: -134.07\n",
      "Iteration 7103, Average Reward: -132.12\n",
      "Iteration 7104, Average Reward: -130.19\n",
      "Iteration 7105, Average Reward: -114.6\n",
      "Iteration 7106, Average Reward: -112.78\n",
      "Iteration 7107, Average Reward: -115.97\n",
      "Iteration 7108, Average Reward: -124.49\n",
      "Iteration 7109, Average Reward: -119.89\n",
      "Iteration 7110, Average Reward: -122.25\n",
      "Iteration 7111, Average Reward: -131.09\n",
      "Iteration 7112, Average Reward: -102.4\n",
      "Iteration 7113, Average Reward: -97.95\n",
      "Iteration 7114, Average Reward: -118.37\n",
      "Iteration 7115, Average Reward: -118.52\n",
      "Iteration 7116, Average Reward: -109.79\n",
      "Iteration 7117, Average Reward: -111.02\n",
      "Iteration 7118, Average Reward: -129.52\n",
      "Iteration 7119, Average Reward: -140.76\n",
      "Iteration 7120, Average Reward: -147.14\n",
      "Iteration 7121, Average Reward: -142.37\n",
      "Iteration 7122, Average Reward: -138.77\n",
      "Iteration 7123, Average Reward: -118.88\n",
      "Iteration 7124, Average Reward: -112.05\n",
      "Iteration 7125, Average Reward: -105.17\n",
      "Iteration 7126, Average Reward: -113.66\n",
      "Iteration 7127, Average Reward: -110.51\n",
      "Iteration 7128, Average Reward: -118.28\n",
      "Iteration 7129, Average Reward: -119.26\n",
      "Iteration 7130, Average Reward: -127.92\n",
      "Iteration 7131, Average Reward: -142.4\n",
      "Iteration 7132, Average Reward: -131.77\n",
      "Iteration 7133, Average Reward: -126.48\n",
      "Iteration 7134, Average Reward: -126.29\n",
      "Iteration 7135, Average Reward: -115.67\n",
      "Iteration 7136, Average Reward: -119.41\n",
      "Iteration 7137, Average Reward: -113.14\n",
      "Iteration 7138, Average Reward: -122.83\n",
      "Iteration 7139, Average Reward: -125.0\n",
      "Iteration 7140, Average Reward: -118.19\n",
      "Iteration 7141, Average Reward: -135.33\n",
      "Iteration 7142, Average Reward: -136.83\n",
      "Iteration 7143, Average Reward: -129.83\n",
      "Iteration 7144, Average Reward: -151.75\n",
      "Iteration 7145, Average Reward: -141.73\n",
      "Iteration 7146, Average Reward: -126.67\n",
      "Iteration 7147, Average Reward: -115.16\n",
      "Iteration 7148, Average Reward: -104.87\n",
      "Iteration 7149, Average Reward: -110.43\n",
      "Iteration 7150, Average Reward: -112.8\n",
      "Iteration 7151, Average Reward: -115.44\n",
      "Iteration 7152, Average Reward: -120.33\n",
      "Iteration 7153, Average Reward: -130.92\n",
      "Iteration 7154, Average Reward: -129.26\n",
      "Iteration 7155, Average Reward: -121.22\n",
      "Iteration 7156, Average Reward: -107.23\n",
      "Iteration 7157, Average Reward: -121.94\n",
      "Iteration 7158, Average Reward: -123.32\n",
      "Iteration 7159, Average Reward: -121.41\n",
      "Iteration 7160, Average Reward: -123.98\n",
      "Iteration 7161, Average Reward: -133.86\n",
      "Iteration 7162, Average Reward: -124.76\n",
      "Iteration 7163, Average Reward: -107.93\n",
      "Iteration 7164, Average Reward: -116.6\n",
      "Iteration 7165, Average Reward: -134.36\n",
      "Iteration 7166, Average Reward: -139.74\n",
      "Iteration 7167, Average Reward: -128.81\n",
      "Iteration 7168, Average Reward: -111.02\n",
      "Iteration 7169, Average Reward: -120.48\n",
      "Iteration 7170, Average Reward: -120.95\n",
      "Iteration 7171, Average Reward: -113.49\n",
      "Iteration 7172, Average Reward: -112.56\n",
      "Iteration 7173, Average Reward: -125.39\n",
      "Iteration 7174, Average Reward: -110.6\n",
      "Iteration 7175, Average Reward: -113.27\n",
      "Iteration 7176, Average Reward: -118.25\n",
      "Iteration 7177, Average Reward: -123.12\n",
      "Iteration 7178, Average Reward: -132.35\n",
      "Iteration 7179, Average Reward: -159.37\n",
      "Iteration 7180, Average Reward: -149.29\n",
      "Iteration 7181, Average Reward: -134.74\n",
      "Iteration 7182, Average Reward: -118.91\n",
      "Iteration 7183, Average Reward: -111.42\n",
      "Iteration 7184, Average Reward: -112.62\n",
      "Iteration 7185, Average Reward: -111.62\n",
      "Iteration 7186, Average Reward: -114.3\n",
      "Iteration 7187, Average Reward: -117.52\n",
      "Iteration 7188, Average Reward: -121.78\n",
      "Iteration 7189, Average Reward: -129.36\n",
      "Iteration 7190, Average Reward: -146.1\n",
      "Iteration 7191, Average Reward: -158.45\n",
      "Iteration 7192, Average Reward: -138.58\n",
      "Iteration 7193, Average Reward: -120.28\n",
      "Iteration 7194, Average Reward: -114.96\n",
      "Iteration 7195, Average Reward: -103.55\n",
      "Iteration 7196, Average Reward: -90.56\n",
      "Iteration 7197, Average Reward: -97.53\n",
      "Iteration 7198, Average Reward: -123.15\n",
      "Iteration 7199, Average Reward: -124.07\n",
      "Iteration 7200, Average Reward: -138.54\n",
      "Iteration 7201, Average Reward: -144.5\n",
      "Iteration 7202, Average Reward: -125.59\n",
      "Iteration 7203, Average Reward: -136.29\n",
      "Iteration 7204, Average Reward: -131.11\n",
      "Iteration 7205, Average Reward: -112.9\n",
      "Iteration 7206, Average Reward: -117.3\n",
      "Iteration 7207, Average Reward: -123.02\n",
      "Iteration 7208, Average Reward: -117.69\n",
      "Iteration 7209, Average Reward: -122.21\n",
      "Iteration 7210, Average Reward: -128.74\n",
      "Iteration 7211, Average Reward: -144.98\n",
      "Iteration 7212, Average Reward: -129.98\n",
      "Iteration 7213, Average Reward: -119.5\n",
      "Iteration 7214, Average Reward: -128.68\n",
      "Iteration 7215, Average Reward: -110.22\n",
      "Iteration 7216, Average Reward: -103.86\n",
      "Iteration 7217, Average Reward: -111.07\n",
      "Iteration 7218, Average Reward: -110.86\n",
      "Iteration 7219, Average Reward: -138.06\n",
      "Iteration 7220, Average Reward: -137.5\n",
      "Iteration 7221, Average Reward: -133.92\n",
      "Iteration 7222, Average Reward: -133.72\n",
      "Iteration 7223, Average Reward: -131.54\n",
      "Iteration 7224, Average Reward: -130.74\n",
      "Iteration 7225, Average Reward: -129.4\n",
      "Iteration 7226, Average Reward: -127.36\n",
      "Iteration 7227, Average Reward: -115.03\n",
      "Iteration 7228, Average Reward: -113.81\n",
      "Iteration 7229, Average Reward: -127.85\n",
      "Iteration 7230, Average Reward: -137.53\n",
      "Iteration 7231, Average Reward: -124.4\n",
      "Iteration 7232, Average Reward: -130.8\n",
      "Iteration 7233, Average Reward: -139.2\n",
      "Iteration 7234, Average Reward: -128.38\n",
      "Iteration 7235, Average Reward: -125.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7236, Average Reward: -113.43\n",
      "Iteration 7237, Average Reward: -112.72\n",
      "Iteration 7238, Average Reward: -127.36\n",
      "Iteration 7239, Average Reward: -117.05\n",
      "Iteration 7240, Average Reward: -99.54\n",
      "Iteration 7241, Average Reward: -102.85\n",
      "Iteration 7242, Average Reward: -112.92\n",
      "Iteration 7243, Average Reward: -115.61\n",
      "Iteration 7244, Average Reward: -113.29\n",
      "Iteration 7245, Average Reward: -139.13\n",
      "Iteration 7246, Average Reward: -142.15\n",
      "Iteration 7247, Average Reward: -139.78\n",
      "Iteration 7248, Average Reward: -125.79\n",
      "Iteration 7249, Average Reward: -133.0\n",
      "Iteration 7250, Average Reward: -121.17\n",
      "Iteration 7251, Average Reward: -115.79\n",
      "Iteration 7252, Average Reward: -125.59\n",
      "Iteration 7253, Average Reward: -130.55\n",
      "Iteration 7254, Average Reward: -99.67\n",
      "Iteration 7255, Average Reward: -106.86\n",
      "Iteration 7256, Average Reward: -132.66\n",
      "Iteration 7257, Average Reward: -128.66\n",
      "Iteration 7258, Average Reward: -131.07\n",
      "Iteration 7259, Average Reward: -134.23\n",
      "Iteration 7260, Average Reward: -124.95\n",
      "Iteration 7261, Average Reward: -112.28\n",
      "Iteration 7262, Average Reward: -114.66\n",
      "Iteration 7263, Average Reward: -103.71\n",
      "Iteration 7264, Average Reward: -117.73\n",
      "Iteration 7265, Average Reward: -134.97\n",
      "Iteration 7266, Average Reward: -133.41\n",
      "Iteration 7267, Average Reward: -119.28\n",
      "Iteration 7268, Average Reward: -125.51\n",
      "Iteration 7269, Average Reward: -127.47\n",
      "Iteration 7270, Average Reward: -123.53\n",
      "Iteration 7271, Average Reward: -112.02\n",
      "Iteration 7272, Average Reward: -99.23\n",
      "Iteration 7273, Average Reward: -114.01\n",
      "Iteration 7274, Average Reward: -128.01\n",
      "Iteration 7275, Average Reward: -127.15\n",
      "Iteration 7276, Average Reward: -140.91\n",
      "Iteration 7277, Average Reward: -127.22\n",
      "Iteration 7278, Average Reward: -111.54\n",
      "Iteration 7279, Average Reward: -121.82\n",
      "Iteration 7280, Average Reward: -113.98\n",
      "Iteration 7281, Average Reward: -122.04\n",
      "Iteration 7282, Average Reward: -124.89\n",
      "Iteration 7283, Average Reward: -128.59\n",
      "Iteration 7284, Average Reward: -129.86\n",
      "Iteration 7285, Average Reward: -128.68\n",
      "Iteration 7286, Average Reward: -133.5\n",
      "Iteration 7287, Average Reward: -114.39\n",
      "Iteration 7288, Average Reward: -111.9\n",
      "Iteration 7289, Average Reward: -111.23\n",
      "Iteration 7290, Average Reward: -105.12\n",
      "Iteration 7291, Average Reward: -119.66\n",
      "Iteration 7292, Average Reward: -129.69\n",
      "Iteration 7293, Average Reward: -124.22\n",
      "Iteration 7294, Average Reward: -121.66\n",
      "Iteration 7295, Average Reward: -112.74\n",
      "Iteration 7296, Average Reward: -105.94\n",
      "Iteration 7297, Average Reward: -119.01\n",
      "Iteration 7298, Average Reward: -123.23\n",
      "Iteration 7299, Average Reward: -110.69\n",
      "Iteration 7300, Average Reward: -107.96\n",
      "Iteration 7301, Average Reward: -120.85\n",
      "Iteration 7302, Average Reward: -114.96\n",
      "Iteration 7303, Average Reward: -126.6\n",
      "Iteration 7304, Average Reward: -119.78\n",
      "Iteration 7305, Average Reward: -123.11\n",
      "Iteration 7306, Average Reward: -119.64\n",
      "Iteration 7307, Average Reward: -116.63\n",
      "Iteration 7308, Average Reward: -116.06\n",
      "Iteration 7309, Average Reward: -101.13\n",
      "Iteration 7310, Average Reward: -101.55\n",
      "Iteration 7311, Average Reward: -115.82\n",
      "Iteration 7312, Average Reward: -141.71\n",
      "Iteration 7313, Average Reward: -139.94\n",
      "Iteration 7314, Average Reward: -155.07\n",
      "Iteration 7315, Average Reward: -159.41\n",
      "Iteration 7316, Average Reward: -145.75\n",
      "Iteration 7317, Average Reward: -130.38\n",
      "Iteration 7318, Average Reward: -107.94\n",
      "Iteration 7319, Average Reward: -104.67\n",
      "Iteration 7320, Average Reward: -102.53\n",
      "Iteration 7321, Average Reward: -110.69\n",
      "Iteration 7322, Average Reward: -114.92\n",
      "Iteration 7323, Average Reward: -121.68\n",
      "Iteration 7324, Average Reward: -135.37\n",
      "Iteration 7325, Average Reward: -144.45\n",
      "Iteration 7326, Average Reward: -130.93\n",
      "Iteration 7327, Average Reward: -111.3\n",
      "Iteration 7328, Average Reward: -96.75\n",
      "Iteration 7329, Average Reward: -107.87\n",
      "Iteration 7330, Average Reward: -133.89\n",
      "Iteration 7331, Average Reward: -133.89\n",
      "Iteration 7332, Average Reward: -126.13\n",
      "Iteration 7333, Average Reward: -110.54\n",
      "Iteration 7334, Average Reward: -91.39\n",
      "Iteration 7335, Average Reward: -116.03\n",
      "Iteration 7336, Average Reward: -117.63\n",
      "Iteration 7337, Average Reward: -115.43\n",
      "Iteration 7338, Average Reward: -136.57\n",
      "Iteration 7339, Average Reward: -137.55\n",
      "Iteration 7340, Average Reward: -121.25\n",
      "Iteration 7341, Average Reward: -119.39\n",
      "Iteration 7342, Average Reward: -134.97\n",
      "Iteration 7343, Average Reward: -135.35\n",
      "Iteration 7344, Average Reward: -129.9\n",
      "Iteration 7345, Average Reward: -132.74\n",
      "Iteration 7346, Average Reward: -125.09\n",
      "Iteration 7347, Average Reward: -105.73\n",
      "Iteration 7348, Average Reward: -104.84\n",
      "Iteration 7349, Average Reward: -102.71\n",
      "Iteration 7350, Average Reward: -118.49\n",
      "Iteration 7351, Average Reward: -131.22\n",
      "Iteration 7352, Average Reward: -121.16\n",
      "Iteration 7353, Average Reward: -125.71\n",
      "Iteration 7354, Average Reward: -129.5\n",
      "Iteration 7355, Average Reward: -127.92\n",
      "Iteration 7356, Average Reward: -119.14\n",
      "Iteration 7357, Average Reward: -129.01\n",
      "Iteration 7358, Average Reward: -120.61\n",
      "Iteration 7359, Average Reward: -101.34\n",
      "Iteration 7360, Average Reward: -117.6\n",
      "Iteration 7361, Average Reward: -122.84\n",
      "Iteration 7362, Average Reward: -128.97\n",
      "Iteration 7363, Average Reward: -141.68\n",
      "Iteration 7364, Average Reward: -133.69\n",
      "Iteration 7365, Average Reward: -133.45\n",
      "Iteration 7366, Average Reward: -123.18\n",
      "Iteration 7367, Average Reward: -115.01\n",
      "Iteration 7368, Average Reward: -127.73\n",
      "Iteration 7369, Average Reward: -130.1\n",
      "Iteration 7370, Average Reward: -114.24\n",
      "Iteration 7371, Average Reward: -115.03\n",
      "Iteration 7372, Average Reward: -125.85\n",
      "Iteration 7373, Average Reward: -139.99\n",
      "Iteration 7374, Average Reward: -132.44\n",
      "Iteration 7375, Average Reward: -128.07\n",
      "Iteration 7376, Average Reward: -113.81\n",
      "Iteration 7377, Average Reward: -100.81\n",
      "Iteration 7378, Average Reward: -117.4\n",
      "Iteration 7379, Average Reward: -134.49\n",
      "Iteration 7380, Average Reward: -133.31\n",
      "Iteration 7381, Average Reward: -123.79\n",
      "Iteration 7382, Average Reward: -124.88\n",
      "Iteration 7383, Average Reward: -130.3\n",
      "Iteration 7384, Average Reward: -119.2\n",
      "Iteration 7385, Average Reward: -117.87\n",
      "Iteration 7386, Average Reward: -128.15\n",
      "Iteration 7387, Average Reward: -122.97\n",
      "Iteration 7388, Average Reward: -131.67\n",
      "Iteration 7389, Average Reward: -127.91\n",
      "Iteration 7390, Average Reward: -122.94\n",
      "Iteration 7391, Average Reward: -119.47\n",
      "Iteration 7392, Average Reward: -125.24\n",
      "Iteration 7393, Average Reward: -119.2\n",
      "Iteration 7394, Average Reward: -112.43\n",
      "Iteration 7395, Average Reward: -120.38\n",
      "Iteration 7396, Average Reward: -134.33\n",
      "Iteration 7397, Average Reward: -138.52\n",
      "Iteration 7398, Average Reward: -141.74\n",
      "Iteration 7399, Average Reward: -153.27\n",
      "Iteration 7400, Average Reward: -134.93\n",
      "Iteration 7401, Average Reward: -123.78\n",
      "Iteration 7402, Average Reward: -125.52\n",
      "Iteration 7403, Average Reward: -124.38\n",
      "Iteration 7404, Average Reward: -115.98\n",
      "Iteration 7405, Average Reward: -118.87\n",
      "Iteration 7406, Average Reward: -129.79\n",
      "Iteration 7407, Average Reward: -122.05\n",
      "Iteration 7408, Average Reward: -128.21\n",
      "Iteration 7409, Average Reward: -134.21\n",
      "Iteration 7410, Average Reward: -121.92\n",
      "Iteration 7411, Average Reward: -106.13\n",
      "Iteration 7412, Average Reward: -104.29\n",
      "Iteration 7413, Average Reward: -111.45\n",
      "Iteration 7414, Average Reward: -129.72\n",
      "Iteration 7415, Average Reward: -124.82\n",
      "Iteration 7416, Average Reward: -129.5\n",
      "Iteration 7417, Average Reward: -107.47\n",
      "Iteration 7418, Average Reward: -111.28\n",
      "Iteration 7419, Average Reward: -119.19\n",
      "Iteration 7420, Average Reward: -107.97\n",
      "Iteration 7421, Average Reward: -110.88\n",
      "Iteration 7422, Average Reward: -122.21\n",
      "Iteration 7423, Average Reward: -122.59\n",
      "Iteration 7424, Average Reward: -114.43\n",
      "Iteration 7425, Average Reward: -111.41\n",
      "Iteration 7426, Average Reward: -127.33\n",
      "Iteration 7427, Average Reward: -131.12\n",
      "Iteration 7428, Average Reward: -134.69\n",
      "Iteration 7429, Average Reward: -128.14\n",
      "Iteration 7430, Average Reward: -107.97\n",
      "Iteration 7431, Average Reward: -112.98\n",
      "Iteration 7432, Average Reward: -121.37\n",
      "Iteration 7433, Average Reward: -126.62\n",
      "Iteration 7434, Average Reward: -112.54\n",
      "Iteration 7435, Average Reward: -111.55\n",
      "Iteration 7436, Average Reward: -115.63\n",
      "Iteration 7437, Average Reward: -112.73\n",
      "Iteration 7438, Average Reward: -121.9\n",
      "Iteration 7439, Average Reward: -123.64\n",
      "Iteration 7440, Average Reward: -122.21\n",
      "Iteration 7441, Average Reward: -136.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7442, Average Reward: -145.15\n",
      "Iteration 7443, Average Reward: -134.19\n",
      "Iteration 7444, Average Reward: -125.5\n",
      "Iteration 7445, Average Reward: -125.5\n",
      "Iteration 7446, Average Reward: -118.55\n",
      "Iteration 7447, Average Reward: -123.58\n",
      "Iteration 7448, Average Reward: -116.09\n",
      "Iteration 7449, Average Reward: -108.8\n",
      "Iteration 7450, Average Reward: -116.41\n",
      "Iteration 7451, Average Reward: -118.19\n",
      "Iteration 7452, Average Reward: -135.96\n",
      "Iteration 7453, Average Reward: -121.48\n",
      "Iteration 7454, Average Reward: -125.85\n",
      "Iteration 7455, Average Reward: -130.42\n",
      "Iteration 7456, Average Reward: -110.21\n",
      "Iteration 7457, Average Reward: -108.97\n",
      "Iteration 7458, Average Reward: -119.69\n",
      "Iteration 7459, Average Reward: -94.36\n",
      "Iteration 7460, Average Reward: -91.24\n",
      "Iteration 7461, Average Reward: -119.94\n",
      "Iteration 7462, Average Reward: -134.24\n",
      "Iteration 7463, Average Reward: -127.67\n",
      "Iteration 7464, Average Reward: -115.19\n",
      "Iteration 7465, Average Reward: -130.85\n",
      "Iteration 7466, Average Reward: -136.77\n",
      "Iteration 7467, Average Reward: -130.28\n",
      "Iteration 7468, Average Reward: -116.44\n",
      "Iteration 7469, Average Reward: -101.15\n",
      "Iteration 7470, Average Reward: -98.8\n",
      "Iteration 7471, Average Reward: -115.93\n",
      "Iteration 7472, Average Reward: -118.11\n",
      "Iteration 7473, Average Reward: -118.75\n",
      "Iteration 7474, Average Reward: -129.15\n",
      "Iteration 7475, Average Reward: -137.29\n",
      "Iteration 7476, Average Reward: -119.57\n",
      "Iteration 7477, Average Reward: -135.05\n",
      "Iteration 7478, Average Reward: -145.17\n",
      "Iteration 7479, Average Reward: -124.03\n",
      "Iteration 7480, Average Reward: -100.69\n",
      "Iteration 7481, Average Reward: -105.59\n",
      "Iteration 7482, Average Reward: -107.79\n",
      "Iteration 7483, Average Reward: -116.88\n",
      "Iteration 7484, Average Reward: -116.51\n",
      "Iteration 7485, Average Reward: -125.13\n",
      "Iteration 7486, Average Reward: -129.24\n",
      "Iteration 7487, Average Reward: -121.6\n",
      "Iteration 7488, Average Reward: -123.38\n",
      "Iteration 7489, Average Reward: -138.66\n",
      "Iteration 7490, Average Reward: -117.57\n",
      "Iteration 7491, Average Reward: -125.77\n",
      "Iteration 7492, Average Reward: -117.96\n",
      "Iteration 7493, Average Reward: -121.51\n",
      "Iteration 7494, Average Reward: -123.04\n",
      "Iteration 7495, Average Reward: -130.65\n",
      "Iteration 7496, Average Reward: -114.94\n",
      "Iteration 7497, Average Reward: -122.13\n",
      "Iteration 7498, Average Reward: -125.2\n",
      "Iteration 7499, Average Reward: -119.11\n",
      "Iteration 7500, Average Reward: -112.71\n",
      "Iteration 7501, Average Reward: -123.68\n",
      "Iteration 7502, Average Reward: -144.82\n",
      "Iteration 7503, Average Reward: -145.87\n",
      "Iteration 7504, Average Reward: -150.74\n",
      "Iteration 7505, Average Reward: -140.25\n",
      "Iteration 7506, Average Reward: -146.05\n",
      "Iteration 7507, Average Reward: -129.02\n",
      "Iteration 7508, Average Reward: -102.05\n",
      "Iteration 7509, Average Reward: -108.89\n",
      "Iteration 7510, Average Reward: -116.02\n",
      "Iteration 7511, Average Reward: -110.37\n",
      "Iteration 7512, Average Reward: -111.75\n",
      "Iteration 7513, Average Reward: -137.68\n",
      "Iteration 7514, Average Reward: -123.65\n",
      "Iteration 7515, Average Reward: -137.81\n",
      "Iteration 7516, Average Reward: -160.26\n",
      "Iteration 7517, Average Reward: -138.31\n",
      "Iteration 7518, Average Reward: -135.66\n",
      "Iteration 7519, Average Reward: -125.72\n",
      "Iteration 7520, Average Reward: -110.07\n",
      "Iteration 7521, Average Reward: -97.66\n",
      "Iteration 7522, Average Reward: -103.9\n",
      "Iteration 7523, Average Reward: -128.19\n",
      "Iteration 7524, Average Reward: -117.59\n",
      "Iteration 7525, Average Reward: -124.5\n",
      "Iteration 7526, Average Reward: -123.28\n",
      "Iteration 7527, Average Reward: -124.18\n",
      "Iteration 7528, Average Reward: -124.76\n",
      "Iteration 7529, Average Reward: -118.24\n",
      "Iteration 7530, Average Reward: -122.89\n",
      "Iteration 7531, Average Reward: -131.38\n",
      "Iteration 7532, Average Reward: -133.73\n",
      "Iteration 7533, Average Reward: -126.89\n",
      "Iteration 7534, Average Reward: -127.32\n",
      "Iteration 7535, Average Reward: -122.7\n",
      "Iteration 7536, Average Reward: -114.83\n",
      "Iteration 7537, Average Reward: -129.26\n",
      "Iteration 7538, Average Reward: -125.56\n",
      "Iteration 7539, Average Reward: -116.89\n",
      "Iteration 7540, Average Reward: -123.4\n",
      "Iteration 7541, Average Reward: -132.04\n",
      "Iteration 7542, Average Reward: -123.71\n",
      "Iteration 7543, Average Reward: -110.89\n",
      "Iteration 7544, Average Reward: -135.73\n",
      "Iteration 7545, Average Reward: -108.76\n",
      "Iteration 7546, Average Reward: -105.68\n",
      "Iteration 7547, Average Reward: -132.2\n",
      "Iteration 7548, Average Reward: -136.54\n",
      "Iteration 7549, Average Reward: -134.51\n",
      "Iteration 7550, Average Reward: -130.36\n",
      "Iteration 7551, Average Reward: -123.28\n",
      "Iteration 7552, Average Reward: -105.92\n",
      "Iteration 7553, Average Reward: -120.42\n",
      "Iteration 7554, Average Reward: -115.88\n",
      "Iteration 7555, Average Reward: -102.29\n",
      "Iteration 7556, Average Reward: -120.21\n",
      "Iteration 7557, Average Reward: -125.64\n",
      "Iteration 7558, Average Reward: -115.05\n",
      "Iteration 7559, Average Reward: -104.44\n",
      "Iteration 7560, Average Reward: -120.96\n",
      "Iteration 7561, Average Reward: -123.52\n",
      "Iteration 7562, Average Reward: -130.51\n",
      "Iteration 7563, Average Reward: -129.02\n",
      "Iteration 7564, Average Reward: -128.17\n",
      "Iteration 7565, Average Reward: -119.11\n",
      "Iteration 7566, Average Reward: -131.25\n",
      "Iteration 7567, Average Reward: -111.4\n",
      "Iteration 7568, Average Reward: -111.39\n",
      "Iteration 7569, Average Reward: -111.72\n",
      "Iteration 7570, Average Reward: -130.5\n",
      "Iteration 7571, Average Reward: -127.55\n",
      "Iteration 7572, Average Reward: -123.09\n",
      "Iteration 7573, Average Reward: -123.2\n",
      "Iteration 7574, Average Reward: -152.75\n",
      "Iteration 7575, Average Reward: -155.2\n",
      "Iteration 7576, Average Reward: -135.8\n",
      "Iteration 7577, Average Reward: -129.22\n",
      "Iteration 7578, Average Reward: -131.66\n",
      "Iteration 7579, Average Reward: -127.31\n",
      "Iteration 7580, Average Reward: -111.66\n",
      "Iteration 7581, Average Reward: -138.26\n",
      "Iteration 7582, Average Reward: -145.48\n",
      "Iteration 7583, Average Reward: -127.19\n",
      "Iteration 7584, Average Reward: -120.86\n",
      "Iteration 7585, Average Reward: -122.07\n",
      "Iteration 7586, Average Reward: -110.1\n",
      "Iteration 7587, Average Reward: -120.0\n",
      "Iteration 7588, Average Reward: -133.52\n",
      "Iteration 7589, Average Reward: -127.25\n",
      "Iteration 7590, Average Reward: -129.15\n",
      "Iteration 7591, Average Reward: -122.06\n",
      "Iteration 7592, Average Reward: -112.82\n",
      "Iteration 7593, Average Reward: -126.36\n",
      "Iteration 7594, Average Reward: -133.6\n",
      "Iteration 7595, Average Reward: -122.46\n",
      "Iteration 7596, Average Reward: -131.73\n",
      "Iteration 7597, Average Reward: -153.62\n",
      "Iteration 7598, Average Reward: -142.63\n",
      "Iteration 7599, Average Reward: -124.26\n",
      "Iteration 7600, Average Reward: -121.87\n",
      "Iteration 7601, Average Reward: -127.69\n",
      "Iteration 7602, Average Reward: -118.76\n",
      "Iteration 7603, Average Reward: -123.41\n",
      "Iteration 7604, Average Reward: -126.2\n",
      "Iteration 7605, Average Reward: -119.24\n",
      "Iteration 7606, Average Reward: -105.71\n",
      "Iteration 7607, Average Reward: -101.87\n",
      "Iteration 7608, Average Reward: -119.88\n",
      "Iteration 7609, Average Reward: -127.82\n",
      "Iteration 7610, Average Reward: -133.61\n",
      "Iteration 7611, Average Reward: -125.36\n",
      "Iteration 7612, Average Reward: -130.62\n",
      "Iteration 7613, Average Reward: -128.28\n",
      "Iteration 7614, Average Reward: -115.34\n",
      "Iteration 7615, Average Reward: -116.08\n",
      "Iteration 7616, Average Reward: -132.31\n",
      "Iteration 7617, Average Reward: -126.77\n",
      "Iteration 7618, Average Reward: -128.68\n",
      "Iteration 7619, Average Reward: -117.28\n",
      "Iteration 7620, Average Reward: -112.38\n",
      "Iteration 7621, Average Reward: -116.29\n",
      "Iteration 7622, Average Reward: -113.39\n",
      "Iteration 7623, Average Reward: -134.04\n",
      "Iteration 7624, Average Reward: -145.85\n",
      "Iteration 7625, Average Reward: -131.29\n",
      "Iteration 7626, Average Reward: -120.43\n",
      "Iteration 7627, Average Reward: -103.26\n",
      "Iteration 7628, Average Reward: -106.63\n",
      "Iteration 7629, Average Reward: -110.5\n",
      "Iteration 7630, Average Reward: -109.45\n",
      "Iteration 7631, Average Reward: -123.64\n",
      "Iteration 7632, Average Reward: -133.83\n",
      "Iteration 7633, Average Reward: -124.66\n",
      "Iteration 7634, Average Reward: -107.22\n",
      "Iteration 7635, Average Reward: -101.19\n",
      "Iteration 7636, Average Reward: -115.99\n",
      "Iteration 7637, Average Reward: -117.0\n",
      "Iteration 7638, Average Reward: -105.65\n",
      "Iteration 7639, Average Reward: -119.9\n",
      "Iteration 7640, Average Reward: -136.92\n",
      "Iteration 7641, Average Reward: -127.9\n",
      "Iteration 7642, Average Reward: -96.03\n",
      "Iteration 7643, Average Reward: -110.45\n",
      "Iteration 7644, Average Reward: -125.4\n",
      "Iteration 7645, Average Reward: -128.15\n",
      "Iteration 7646, Average Reward: -104.19\n",
      "Iteration 7647, Average Reward: -118.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7648, Average Reward: -131.09\n",
      "Iteration 7649, Average Reward: -125.26\n",
      "Iteration 7650, Average Reward: -132.4\n",
      "Iteration 7651, Average Reward: -133.58\n",
      "Iteration 7652, Average Reward: -130.81\n",
      "Iteration 7653, Average Reward: -110.02\n",
      "Iteration 7654, Average Reward: -123.56\n",
      "Iteration 7655, Average Reward: -130.35\n",
      "Iteration 7656, Average Reward: -124.46\n",
      "Iteration 7657, Average Reward: -115.14\n",
      "Iteration 7658, Average Reward: -135.53\n",
      "Iteration 7659, Average Reward: -126.28\n",
      "Iteration 7660, Average Reward: -112.47\n",
      "Iteration 7661, Average Reward: -117.89\n",
      "Iteration 7662, Average Reward: -127.87\n",
      "Iteration 7663, Average Reward: -132.46\n",
      "Iteration 7664, Average Reward: -131.17\n",
      "Iteration 7665, Average Reward: -132.8\n",
      "Iteration 7666, Average Reward: -133.86\n",
      "Iteration 7667, Average Reward: -139.55\n",
      "Iteration 7668, Average Reward: -112.63\n",
      "Iteration 7669, Average Reward: -122.18\n",
      "Iteration 7670, Average Reward: -125.74\n",
      "Iteration 7671, Average Reward: -124.92\n",
      "Iteration 7672, Average Reward: -122.16\n",
      "Iteration 7673, Average Reward: -129.12\n",
      "Iteration 7674, Average Reward: -124.84\n",
      "Iteration 7675, Average Reward: -125.13\n",
      "Iteration 7676, Average Reward: -133.89\n",
      "Iteration 7677, Average Reward: -122.57\n",
      "Iteration 7678, Average Reward: -107.48\n",
      "Iteration 7679, Average Reward: -113.48\n",
      "Iteration 7680, Average Reward: -107.24\n",
      "Iteration 7681, Average Reward: -117.79\n",
      "Iteration 7682, Average Reward: -128.31\n",
      "Iteration 7683, Average Reward: -128.27\n",
      "Iteration 7684, Average Reward: -119.87\n",
      "Iteration 7685, Average Reward: -116.43\n",
      "Iteration 7686, Average Reward: -119.24\n",
      "Iteration 7687, Average Reward: -128.0\n",
      "Iteration 7688, Average Reward: -120.06\n",
      "Iteration 7689, Average Reward: -107.89\n",
      "Iteration 7690, Average Reward: -119.09\n",
      "Iteration 7691, Average Reward: -124.73\n",
      "Iteration 7692, Average Reward: -108.27\n",
      "Iteration 7693, Average Reward: -106.47\n",
      "Iteration 7694, Average Reward: -120.67\n",
      "Iteration 7695, Average Reward: -124.85\n",
      "Iteration 7696, Average Reward: -115.5\n",
      "Iteration 7697, Average Reward: -121.43\n",
      "Iteration 7698, Average Reward: -134.1\n",
      "Iteration 7699, Average Reward: -113.61\n",
      "Iteration 7700, Average Reward: -121.13\n",
      "Iteration 7701, Average Reward: -118.59\n",
      "Iteration 7702, Average Reward: -105.77\n",
      "Iteration 7703, Average Reward: -116.5\n",
      "Iteration 7704, Average Reward: -124.52\n",
      "Iteration 7705, Average Reward: -114.81\n",
      "Iteration 7706, Average Reward: -135.11\n",
      "Iteration 7707, Average Reward: -137.87\n",
      "Iteration 7708, Average Reward: -128.6\n",
      "Iteration 7709, Average Reward: -112.15\n",
      "Iteration 7710, Average Reward: -137.47\n",
      "Iteration 7711, Average Reward: -126.07\n",
      "Iteration 7712, Average Reward: -117.03\n",
      "Iteration 7713, Average Reward: -134.15\n",
      "Iteration 7714, Average Reward: -138.96\n",
      "Iteration 7715, Average Reward: -125.09\n",
      "Iteration 7716, Average Reward: -98.16\n",
      "Iteration 7717, Average Reward: -97.79\n",
      "Iteration 7718, Average Reward: -121.41\n",
      "Iteration 7719, Average Reward: -139.33\n",
      "Iteration 7720, Average Reward: -138.92\n",
      "Iteration 7721, Average Reward: -120.55\n",
      "Iteration 7722, Average Reward: -120.66\n",
      "Iteration 7723, Average Reward: -110.86\n",
      "Iteration 7724, Average Reward: -84.61\n",
      "Iteration 7725, Average Reward: -87.84\n",
      "Iteration 7726, Average Reward: -98.38\n",
      "Iteration 7727, Average Reward: -109.24\n",
      "Iteration 7728, Average Reward: -113.64\n",
      "Iteration 7729, Average Reward: -120.68\n",
      "Iteration 7730, Average Reward: -127.91\n",
      "Iteration 7731, Average Reward: -134.83\n",
      "Iteration 7732, Average Reward: -125.56\n",
      "Iteration 7733, Average Reward: -126.65\n",
      "Iteration 7734, Average Reward: -121.51\n",
      "Iteration 7735, Average Reward: -113.96\n",
      "Iteration 7736, Average Reward: -97.75\n",
      "Iteration 7737, Average Reward: -100.61\n",
      "Iteration 7738, Average Reward: -99.82\n",
      "Iteration 7739, Average Reward: -114.17\n",
      "Iteration 7740, Average Reward: -136.14\n",
      "Iteration 7741, Average Reward: -144.05\n",
      "Iteration 7742, Average Reward: -131.77\n",
      "Iteration 7743, Average Reward: -129.87\n",
      "Iteration 7744, Average Reward: -126.12\n",
      "Iteration 7745, Average Reward: -129.55\n",
      "Iteration 7746, Average Reward: -122.21\n",
      "Iteration 7747, Average Reward: -112.69\n",
      "Iteration 7748, Average Reward: -110.97\n",
      "Iteration 7749, Average Reward: -116.98\n",
      "Iteration 7750, Average Reward: -119.85\n",
      "Iteration 7751, Average Reward: -113.79\n",
      "Iteration 7752, Average Reward: -116.57\n",
      "Iteration 7753, Average Reward: -132.8\n",
      "Iteration 7754, Average Reward: -133.22\n",
      "Iteration 7755, Average Reward: -128.71\n",
      "Iteration 7756, Average Reward: -126.82\n",
      "Iteration 7757, Average Reward: -119.49\n",
      "Iteration 7758, Average Reward: -128.14\n",
      "Iteration 7759, Average Reward: -134.47\n",
      "Iteration 7760, Average Reward: -127.76\n",
      "Iteration 7761, Average Reward: -128.01\n",
      "Iteration 7762, Average Reward: -127.63\n",
      "Iteration 7763, Average Reward: -113.96\n",
      "Iteration 7764, Average Reward: -123.6\n",
      "Iteration 7765, Average Reward: -128.12\n",
      "Iteration 7766, Average Reward: -139.52\n",
      "Iteration 7767, Average Reward: -128.1\n",
      "Iteration 7768, Average Reward: -116.22\n",
      "Iteration 7769, Average Reward: -117.48\n",
      "Iteration 7770, Average Reward: -116.35\n",
      "Iteration 7771, Average Reward: -121.18\n",
      "Iteration 7772, Average Reward: -134.47\n",
      "Iteration 7773, Average Reward: -137.81\n",
      "Iteration 7774, Average Reward: -142.71\n",
      "Iteration 7775, Average Reward: -138.13\n",
      "Iteration 7776, Average Reward: -143.36\n",
      "Iteration 7777, Average Reward: -138.87\n",
      "Iteration 7778, Average Reward: -119.92\n",
      "Iteration 7779, Average Reward: -110.24\n",
      "Iteration 7780, Average Reward: -122.21\n",
      "Iteration 7781, Average Reward: -122.09\n",
      "Iteration 7782, Average Reward: -130.39\n",
      "Iteration 7783, Average Reward: -133.33\n",
      "Iteration 7784, Average Reward: -134.41\n",
      "Iteration 7785, Average Reward: -128.54\n",
      "Iteration 7786, Average Reward: -112.94\n",
      "Iteration 7787, Average Reward: -113.99\n",
      "Iteration 7788, Average Reward: -138.16\n",
      "Iteration 7789, Average Reward: -123.01\n",
      "Iteration 7790, Average Reward: -111.97\n",
      "Iteration 7791, Average Reward: -120.2\n",
      "Iteration 7792, Average Reward: -127.34\n",
      "Iteration 7793, Average Reward: -122.39\n",
      "Iteration 7794, Average Reward: -109.26\n",
      "Iteration 7795, Average Reward: -127.7\n",
      "Iteration 7796, Average Reward: -135.24\n",
      "Iteration 7797, Average Reward: -124.84\n",
      "Iteration 7798, Average Reward: -137.45\n",
      "Iteration 7799, Average Reward: -122.43\n",
      "Iteration 7800, Average Reward: -126.49\n",
      "Iteration 7801, Average Reward: -130.95\n",
      "Iteration 7802, Average Reward: -120.73\n",
      "Iteration 7803, Average Reward: -118.35\n",
      "Iteration 7804, Average Reward: -104.1\n",
      "Iteration 7805, Average Reward: -110.52\n",
      "Iteration 7806, Average Reward: -116.89\n",
      "Iteration 7807, Average Reward: -127.48\n",
      "Iteration 7808, Average Reward: -132.61\n",
      "Iteration 7809, Average Reward: -112.77\n",
      "Iteration 7810, Average Reward: -105.59\n",
      "Iteration 7811, Average Reward: -128.46\n",
      "Iteration 7812, Average Reward: -135.48\n",
      "Iteration 7813, Average Reward: -117.88\n",
      "Iteration 7814, Average Reward: -125.19\n",
      "Iteration 7815, Average Reward: -129.95\n",
      "Iteration 7816, Average Reward: -120.28\n",
      "Iteration 7817, Average Reward: -119.77\n",
      "Iteration 7818, Average Reward: -122.59\n",
      "Iteration 7819, Average Reward: -132.21\n",
      "Iteration 7820, Average Reward: -123.69\n",
      "Iteration 7821, Average Reward: -118.62\n",
      "Iteration 7822, Average Reward: -109.99\n",
      "Iteration 7823, Average Reward: -95.13\n",
      "Iteration 7824, Average Reward: -100.82\n",
      "Iteration 7825, Average Reward: -109.75\n",
      "Iteration 7826, Average Reward: -114.29\n",
      "Iteration 7827, Average Reward: -124.22\n",
      "Iteration 7828, Average Reward: -125.05\n",
      "Iteration 7829, Average Reward: -135.69\n",
      "Iteration 7830, Average Reward: -121.0\n",
      "Iteration 7831, Average Reward: -124.17\n",
      "Iteration 7832, Average Reward: -131.53\n",
      "Iteration 7833, Average Reward: -120.9\n",
      "Iteration 7834, Average Reward: -130.81\n",
      "Iteration 7835, Average Reward: -114.66\n",
      "Iteration 7836, Average Reward: -117.73\n",
      "Iteration 7837, Average Reward: -129.17\n",
      "Iteration 7838, Average Reward: -135.08\n",
      "Iteration 7839, Average Reward: -131.92\n",
      "Iteration 7840, Average Reward: -125.97\n",
      "Iteration 7841, Average Reward: -123.18\n",
      "Iteration 7842, Average Reward: -108.96\n",
      "Iteration 7843, Average Reward: -109.15\n",
      "Iteration 7844, Average Reward: -123.74\n",
      "Iteration 7845, Average Reward: -123.48\n",
      "Iteration 7846, Average Reward: -119.8\n",
      "Iteration 7847, Average Reward: -125.41\n",
      "Iteration 7848, Average Reward: -117.93\n",
      "Iteration 7849, Average Reward: -108.79\n",
      "Iteration 7850, Average Reward: -115.6\n",
      "Iteration 7851, Average Reward: -107.71\n",
      "Iteration 7852, Average Reward: -119.05\n",
      "Iteration 7853, Average Reward: -121.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7854, Average Reward: -117.63\n",
      "Iteration 7855, Average Reward: -124.39\n",
      "Iteration 7856, Average Reward: -127.44\n",
      "Iteration 7857, Average Reward: -126.81\n",
      "Iteration 7858, Average Reward: -121.34\n",
      "Iteration 7859, Average Reward: -136.53\n",
      "Iteration 7860, Average Reward: -125.84\n",
      "Iteration 7861, Average Reward: -115.01\n",
      "Iteration 7862, Average Reward: -113.5\n",
      "Iteration 7863, Average Reward: -110.39\n",
      "Iteration 7864, Average Reward: -109.19\n",
      "Iteration 7865, Average Reward: -125.69\n",
      "Iteration 7866, Average Reward: -147.83\n",
      "Iteration 7867, Average Reward: -148.5\n",
      "Iteration 7868, Average Reward: -144.32\n",
      "Iteration 7869, Average Reward: -128.79\n",
      "Iteration 7870, Average Reward: -119.65\n",
      "Iteration 7871, Average Reward: -105.24\n",
      "Iteration 7872, Average Reward: -105.87\n",
      "Iteration 7873, Average Reward: -101.36\n",
      "Iteration 7874, Average Reward: -98.08\n",
      "Iteration 7875, Average Reward: -118.5\n",
      "Iteration 7876, Average Reward: -134.8\n",
      "Iteration 7877, Average Reward: -130.12\n",
      "Iteration 7878, Average Reward: -126.01\n",
      "Iteration 7879, Average Reward: -144.4\n",
      "Iteration 7880, Average Reward: -145.77\n",
      "Iteration 7881, Average Reward: -131.79\n",
      "Iteration 7882, Average Reward: -124.71\n",
      "Iteration 7883, Average Reward: -122.79\n",
      "Iteration 7884, Average Reward: -124.27\n",
      "Iteration 7885, Average Reward: -114.16\n",
      "Iteration 7886, Average Reward: -115.74\n",
      "Iteration 7887, Average Reward: -128.01\n",
      "Iteration 7888, Average Reward: -137.48\n",
      "Iteration 7889, Average Reward: -135.74\n",
      "Iteration 7890, Average Reward: -130.76\n",
      "Iteration 7891, Average Reward: -129.75\n",
      "Iteration 7892, Average Reward: -98.95\n",
      "Iteration 7893, Average Reward: -105.87\n",
      "Iteration 7894, Average Reward: -111.93\n",
      "Iteration 7895, Average Reward: -123.05\n",
      "Iteration 7896, Average Reward: -119.46\n",
      "Iteration 7897, Average Reward: -116.19\n",
      "Iteration 7898, Average Reward: -104.16\n",
      "Iteration 7899, Average Reward: -109.56\n",
      "Iteration 7900, Average Reward: -119.99\n",
      "Iteration 7901, Average Reward: -136.84\n",
      "Iteration 7902, Average Reward: -114.01\n",
      "Iteration 7903, Average Reward: -110.19\n",
      "Iteration 7904, Average Reward: -120.07\n",
      "Iteration 7905, Average Reward: -136.55\n",
      "Iteration 7906, Average Reward: -126.59\n",
      "Iteration 7907, Average Reward: -130.92\n",
      "Iteration 7908, Average Reward: -125.3\n",
      "Iteration 7909, Average Reward: -118.23\n",
      "Iteration 7910, Average Reward: -114.01\n",
      "Iteration 7911, Average Reward: -126.16\n",
      "Iteration 7912, Average Reward: -137.18\n",
      "Iteration 7913, Average Reward: -124.94\n",
      "Iteration 7914, Average Reward: -110.01\n",
      "Iteration 7915, Average Reward: -131.76\n",
      "Iteration 7916, Average Reward: -142.09\n",
      "Iteration 7917, Average Reward: -130.12\n",
      "Iteration 7918, Average Reward: -128.58\n",
      "Iteration 7919, Average Reward: -119.81\n",
      "Iteration 7920, Average Reward: -106.81\n",
      "Iteration 7921, Average Reward: -102.82\n",
      "Iteration 7922, Average Reward: -110.75\n",
      "Iteration 7923, Average Reward: -116.69\n",
      "Iteration 7924, Average Reward: -127.02\n",
      "Iteration 7925, Average Reward: -141.23\n",
      "Iteration 7926, Average Reward: -129.79\n",
      "Iteration 7927, Average Reward: -125.57\n",
      "Iteration 7928, Average Reward: -121.08\n",
      "Iteration 7929, Average Reward: -119.88\n",
      "Iteration 7930, Average Reward: -112.25\n",
      "Iteration 7931, Average Reward: -119.88\n",
      "Iteration 7932, Average Reward: -130.83\n",
      "Iteration 7933, Average Reward: -128.69\n",
      "Iteration 7934, Average Reward: -118.46\n",
      "Iteration 7935, Average Reward: -117.4\n",
      "Iteration 7936, Average Reward: -122.27\n",
      "Iteration 7937, Average Reward: -118.59\n",
      "Iteration 7938, Average Reward: -125.59\n",
      "Iteration 7939, Average Reward: -143.22\n",
      "Iteration 7940, Average Reward: -125.69\n",
      "Iteration 7941, Average Reward: -107.83\n",
      "Iteration 7942, Average Reward: -124.08\n",
      "Iteration 7943, Average Reward: -127.33\n",
      "Iteration 7944, Average Reward: -128.34\n",
      "Iteration 7945, Average Reward: -122.33\n",
      "Iteration 7946, Average Reward: -97.7\n",
      "Iteration 7947, Average Reward: -88.27\n",
      "Iteration 7948, Average Reward: -120.0\n",
      "Iteration 7949, Average Reward: -120.96\n",
      "Iteration 7950, Average Reward: -124.49\n",
      "Iteration 7951, Average Reward: -124.94\n",
      "Iteration 7952, Average Reward: -131.8\n",
      "Iteration 7953, Average Reward: -129.06\n",
      "Iteration 7954, Average Reward: -121.91\n",
      "Iteration 7955, Average Reward: -132.52\n",
      "Iteration 7956, Average Reward: -116.44\n",
      "Iteration 7957, Average Reward: -122.81\n",
      "Iteration 7958, Average Reward: -133.07\n",
      "Iteration 7959, Average Reward: -132.77\n",
      "Iteration 7960, Average Reward: -114.7\n",
      "Iteration 7961, Average Reward: -103.33\n",
      "Iteration 7962, Average Reward: -106.52\n",
      "Iteration 7963, Average Reward: -102.31\n",
      "Iteration 7964, Average Reward: -109.55\n",
      "Iteration 7965, Average Reward: -117.33\n",
      "Iteration 7966, Average Reward: -116.31\n",
      "Iteration 7967, Average Reward: -123.95\n",
      "Iteration 7968, Average Reward: -127.88\n",
      "Iteration 7969, Average Reward: -137.54\n",
      "Iteration 7970, Average Reward: -139.96\n",
      "Iteration 7971, Average Reward: -136.46\n",
      "Iteration 7972, Average Reward: -121.04\n",
      "Iteration 7973, Average Reward: -116.62\n",
      "Iteration 7974, Average Reward: -114.27\n",
      "Iteration 7975, Average Reward: -137.78\n",
      "Iteration 7976, Average Reward: -137.85\n",
      "Iteration 7977, Average Reward: -139.71\n",
      "Iteration 7978, Average Reward: -137.38\n",
      "Iteration 7979, Average Reward: -126.62\n",
      "Iteration 7980, Average Reward: -122.83\n",
      "Iteration 7981, Average Reward: -113.54\n",
      "Iteration 7982, Average Reward: -117.56\n",
      "Iteration 7983, Average Reward: -124.39\n",
      "Iteration 7984, Average Reward: -118.76\n",
      "Iteration 7985, Average Reward: -100.53\n",
      "Iteration 7986, Average Reward: -95.3\n",
      "Iteration 7987, Average Reward: -98.8\n",
      "Iteration 7988, Average Reward: -104.44\n",
      "Iteration 7989, Average Reward: -111.04\n",
      "Iteration 7990, Average Reward: -119.34\n",
      "Iteration 7991, Average Reward: -129.7\n",
      "Iteration 7992, Average Reward: -125.0\n",
      "Iteration 7993, Average Reward: -115.77\n",
      "Iteration 7994, Average Reward: -112.02\n",
      "Iteration 7995, Average Reward: -109.47\n",
      "Iteration 7996, Average Reward: -118.2\n",
      "Iteration 7997, Average Reward: -121.5\n",
      "Iteration 7998, Average Reward: -129.67\n",
      "Iteration 7999, Average Reward: -130.09\n",
      "Iteration 8000, Average Reward: -135.83\n",
      "Iteration 8001, Average Reward: -124.58\n",
      "Iteration 8002, Average Reward: -121.31\n",
      "Iteration 8003, Average Reward: -123.33\n",
      "Iteration 8004, Average Reward: -117.07\n",
      "Iteration 8005, Average Reward: -123.11\n",
      "Iteration 8006, Average Reward: -115.74\n",
      "Iteration 8007, Average Reward: -124.57\n",
      "Iteration 8008, Average Reward: -107.77\n",
      "Iteration 8009, Average Reward: -111.51\n",
      "Iteration 8010, Average Reward: -106.35\n",
      "Iteration 8011, Average Reward: -115.22\n",
      "Iteration 8012, Average Reward: -136.53\n",
      "Iteration 8013, Average Reward: -133.87\n",
      "Iteration 8014, Average Reward: -120.15\n",
      "Iteration 8015, Average Reward: -123.71\n",
      "Iteration 8016, Average Reward: -126.5\n",
      "Iteration 8017, Average Reward: -128.35\n",
      "Iteration 8018, Average Reward: -125.08\n",
      "Iteration 8019, Average Reward: -124.27\n",
      "Iteration 8020, Average Reward: -120.07\n",
      "Iteration 8021, Average Reward: -92.33\n",
      "Iteration 8022, Average Reward: -110.6\n",
      "Iteration 8023, Average Reward: -108.85\n",
      "Iteration 8024, Average Reward: -117.0\n",
      "Iteration 8025, Average Reward: -126.37\n",
      "Iteration 8026, Average Reward: -126.63\n",
      "Iteration 8027, Average Reward: -122.77\n",
      "Iteration 8028, Average Reward: -131.75\n",
      "Iteration 8029, Average Reward: -128.94\n",
      "Iteration 8030, Average Reward: -127.07\n",
      "Iteration 8031, Average Reward: -113.24\n",
      "Iteration 8032, Average Reward: -112.8\n",
      "Iteration 8033, Average Reward: -104.98\n",
      "Iteration 8034, Average Reward: -113.96\n",
      "Iteration 8035, Average Reward: -114.74\n",
      "Iteration 8036, Average Reward: -125.33\n",
      "Iteration 8037, Average Reward: -131.99\n",
      "Iteration 8038, Average Reward: -129.76\n",
      "Iteration 8039, Average Reward: -138.43\n",
      "Iteration 8040, Average Reward: -126.19\n",
      "Iteration 8041, Average Reward: -105.66\n",
      "Iteration 8042, Average Reward: -96.37\n",
      "Iteration 8043, Average Reward: -93.77\n",
      "Iteration 8044, Average Reward: -109.77\n",
      "Iteration 8045, Average Reward: -126.13\n",
      "Iteration 8046, Average Reward: -125.45\n",
      "Iteration 8047, Average Reward: -145.14\n",
      "Iteration 8048, Average Reward: -137.15\n",
      "Iteration 8049, Average Reward: -135.44\n",
      "Iteration 8050, Average Reward: -131.68\n",
      "Iteration 8051, Average Reward: -128.95\n",
      "Iteration 8052, Average Reward: -125.42\n",
      "Iteration 8053, Average Reward: -107.73\n",
      "Iteration 8054, Average Reward: -97.59\n",
      "Iteration 8055, Average Reward: -114.14\n",
      "Iteration 8056, Average Reward: -133.52\n",
      "Iteration 8057, Average Reward: -125.94\n",
      "Iteration 8058, Average Reward: -130.51\n",
      "Iteration 8059, Average Reward: -134.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8060, Average Reward: -135.61\n",
      "Iteration 8061, Average Reward: -126.65\n",
      "Iteration 8062, Average Reward: -122.24\n",
      "Iteration 8063, Average Reward: -122.8\n",
      "Iteration 8064, Average Reward: -127.69\n",
      "Iteration 8065, Average Reward: -135.03\n",
      "Iteration 8066, Average Reward: -149.6\n",
      "Iteration 8067, Average Reward: -148.62\n",
      "Iteration 8068, Average Reward: -122.23\n",
      "Iteration 8069, Average Reward: -120.26\n",
      "Iteration 8070, Average Reward: -108.87\n",
      "Iteration 8071, Average Reward: -100.09\n",
      "Iteration 8072, Average Reward: -107.09\n",
      "Iteration 8073, Average Reward: -118.0\n",
      "Iteration 8074, Average Reward: -126.94\n",
      "Iteration 8075, Average Reward: -133.74\n",
      "Iteration 8076, Average Reward: -132.85\n",
      "Iteration 8077, Average Reward: -131.77\n",
      "Iteration 8078, Average Reward: -136.57\n",
      "Iteration 8079, Average Reward: -128.85\n",
      "Iteration 8080, Average Reward: -105.92\n",
      "Iteration 8081, Average Reward: -118.93\n",
      "Iteration 8082, Average Reward: -127.91\n",
      "Iteration 8083, Average Reward: -121.57\n",
      "Iteration 8084, Average Reward: -122.85\n",
      "Iteration 8085, Average Reward: -120.75\n",
      "Iteration 8086, Average Reward: -115.36\n",
      "Iteration 8087, Average Reward: -129.53\n",
      "Iteration 8088, Average Reward: -126.71\n",
      "Iteration 8089, Average Reward: -118.99\n",
      "Iteration 8090, Average Reward: -127.84\n",
      "Iteration 8091, Average Reward: -130.26\n",
      "Iteration 8092, Average Reward: -132.24\n",
      "Iteration 8093, Average Reward: -137.88\n",
      "Iteration 8094, Average Reward: -132.1\n",
      "Iteration 8095, Average Reward: -124.03\n",
      "Iteration 8096, Average Reward: -118.67\n",
      "Iteration 8097, Average Reward: -113.03\n",
      "Iteration 8098, Average Reward: -116.15\n",
      "Iteration 8099, Average Reward: -123.37\n",
      "Iteration 8100, Average Reward: -136.84\n",
      "Iteration 8101, Average Reward: -144.95\n",
      "Iteration 8102, Average Reward: -160.05\n",
      "Iteration 8103, Average Reward: -149.93\n",
      "Iteration 8104, Average Reward: -132.0\n",
      "Iteration 8105, Average Reward: -122.14\n",
      "Iteration 8106, Average Reward: -114.79\n",
      "Iteration 8107, Average Reward: -128.8\n",
      "Iteration 8108, Average Reward: -107.28\n",
      "Iteration 8109, Average Reward: -117.8\n",
      "Iteration 8110, Average Reward: -126.1\n",
      "Iteration 8111, Average Reward: -128.54\n",
      "Iteration 8112, Average Reward: -134.49\n",
      "Iteration 8113, Average Reward: -130.83\n",
      "Iteration 8114, Average Reward: -137.26\n",
      "Iteration 8115, Average Reward: -130.2\n",
      "Iteration 8116, Average Reward: -115.81\n",
      "Iteration 8117, Average Reward: -116.47\n",
      "Iteration 8118, Average Reward: -111.48\n",
      "Iteration 8119, Average Reward: -102.86\n",
      "Iteration 8120, Average Reward: -110.22\n",
      "Iteration 8121, Average Reward: -106.85\n",
      "Iteration 8122, Average Reward: -103.08\n",
      "Iteration 8123, Average Reward: -136.23\n",
      "Iteration 8124, Average Reward: -134.1\n",
      "Iteration 8125, Average Reward: -123.67\n",
      "Iteration 8126, Average Reward: -133.94\n",
      "Iteration 8127, Average Reward: -115.29\n",
      "Iteration 8128, Average Reward: -117.23\n",
      "Iteration 8129, Average Reward: -132.28\n",
      "Iteration 8130, Average Reward: -118.93\n",
      "Iteration 8131, Average Reward: -118.72\n",
      "Iteration 8132, Average Reward: -132.35\n",
      "Iteration 8133, Average Reward: -119.84\n",
      "Iteration 8134, Average Reward: -116.09\n",
      "Iteration 8135, Average Reward: -125.89\n",
      "Iteration 8136, Average Reward: -121.66\n",
      "Iteration 8137, Average Reward: -112.35\n",
      "Iteration 8138, Average Reward: -118.73\n",
      "Iteration 8139, Average Reward: -127.45\n",
      "Iteration 8140, Average Reward: -118.39\n",
      "Iteration 8141, Average Reward: -121.17\n",
      "Iteration 8142, Average Reward: -116.08\n",
      "Iteration 8143, Average Reward: -127.23\n",
      "Iteration 8144, Average Reward: -124.3\n",
      "Iteration 8145, Average Reward: -125.36\n",
      "Iteration 8146, Average Reward: -140.3\n",
      "Iteration 8147, Average Reward: -126.0\n",
      "Iteration 8148, Average Reward: -132.72\n",
      "Iteration 8149, Average Reward: -135.27\n",
      "Iteration 8150, Average Reward: -105.08\n",
      "Iteration 8151, Average Reward: -109.12\n",
      "Iteration 8152, Average Reward: -131.58\n",
      "Iteration 8153, Average Reward: -120.66\n",
      "Iteration 8154, Average Reward: -125.96\n",
      "Iteration 8155, Average Reward: -122.84\n",
      "Iteration 8156, Average Reward: -121.66\n",
      "Iteration 8157, Average Reward: -125.24\n",
      "Iteration 8158, Average Reward: -119.63\n",
      "Iteration 8159, Average Reward: -121.8\n",
      "Iteration 8160, Average Reward: -114.55\n",
      "Iteration 8161, Average Reward: -103.12\n",
      "Iteration 8162, Average Reward: -115.4\n",
      "Iteration 8163, Average Reward: -112.23\n",
      "Iteration 8164, Average Reward: -112.57\n",
      "Iteration 8165, Average Reward: -112.64\n",
      "Iteration 8166, Average Reward: -119.16\n",
      "Iteration 8167, Average Reward: -135.84\n",
      "Iteration 8168, Average Reward: -141.21\n",
      "Iteration 8169, Average Reward: -136.68\n",
      "Iteration 8170, Average Reward: -139.63\n",
      "Iteration 8171, Average Reward: -108.43\n",
      "Iteration 8172, Average Reward: -90.08\n",
      "Iteration 8173, Average Reward: -127.71\n",
      "Iteration 8174, Average Reward: -128.99\n",
      "Iteration 8175, Average Reward: -126.49\n",
      "Iteration 8176, Average Reward: -128.05\n",
      "Iteration 8177, Average Reward: -118.61\n",
      "Iteration 8178, Average Reward: -119.7\n",
      "Iteration 8179, Average Reward: -120.52\n",
      "Iteration 8180, Average Reward: -111.67\n",
      "Iteration 8181, Average Reward: -121.54\n",
      "Iteration 8182, Average Reward: -141.7\n",
      "Iteration 8183, Average Reward: -126.44\n",
      "Iteration 8184, Average Reward: -114.03\n",
      "Iteration 8185, Average Reward: -112.52\n",
      "Iteration 8186, Average Reward: -116.55\n",
      "Iteration 8187, Average Reward: -110.91\n",
      "Iteration 8188, Average Reward: -123.75\n",
      "Iteration 8189, Average Reward: -137.42\n",
      "Iteration 8190, Average Reward: -144.33\n",
      "Iteration 8191, Average Reward: -133.79\n",
      "Iteration 8192, Average Reward: -129.25\n",
      "Iteration 8193, Average Reward: -126.68\n",
      "Iteration 8194, Average Reward: -112.09\n",
      "Iteration 8195, Average Reward: -111.34\n",
      "Iteration 8196, Average Reward: -128.73\n",
      "Iteration 8197, Average Reward: -127.41\n",
      "Iteration 8198, Average Reward: -121.19\n",
      "Iteration 8199, Average Reward: -121.66\n",
      "Iteration 8200, Average Reward: -132.21\n",
      "Iteration 8201, Average Reward: -122.75\n",
      "Iteration 8202, Average Reward: -114.05\n",
      "Iteration 8203, Average Reward: -117.83\n",
      "Iteration 8204, Average Reward: -121.3\n",
      "Iteration 8205, Average Reward: -115.88\n",
      "Iteration 8206, Average Reward: -122.0\n",
      "Iteration 8207, Average Reward: -124.4\n",
      "Iteration 8208, Average Reward: -121.37\n",
      "Iteration 8209, Average Reward: -103.8\n",
      "Iteration 8210, Average Reward: -109.33\n",
      "Iteration 8211, Average Reward: -128.43\n",
      "Iteration 8212, Average Reward: -125.13\n",
      "Iteration 8213, Average Reward: -128.3\n",
      "Iteration 8214, Average Reward: -134.65\n",
      "Iteration 8215, Average Reward: -118.4\n",
      "Iteration 8216, Average Reward: -106.86\n",
      "Iteration 8217, Average Reward: -98.58\n",
      "Iteration 8218, Average Reward: -107.49\n",
      "Iteration 8219, Average Reward: -114.52\n",
      "Iteration 8220, Average Reward: -101.8\n",
      "Iteration 8221, Average Reward: -105.48\n",
      "Iteration 8222, Average Reward: -126.18\n",
      "Iteration 8223, Average Reward: -133.81\n",
      "Iteration 8224, Average Reward: -130.55\n",
      "Iteration 8225, Average Reward: -134.11\n",
      "Iteration 8226, Average Reward: -133.71\n",
      "Iteration 8227, Average Reward: -118.62\n",
      "Iteration 8228, Average Reward: -115.07\n",
      "Iteration 8229, Average Reward: -130.45\n",
      "Iteration 8230, Average Reward: -130.81\n",
      "Iteration 8231, Average Reward: -106.18\n",
      "Iteration 8232, Average Reward: -119.73\n",
      "Iteration 8233, Average Reward: -123.72\n",
      "Iteration 8234, Average Reward: -132.97\n",
      "Iteration 8235, Average Reward: -139.35\n",
      "Iteration 8236, Average Reward: -162.03\n",
      "Iteration 8237, Average Reward: -152.42\n",
      "Iteration 8238, Average Reward: -130.69\n",
      "Iteration 8239, Average Reward: -132.8\n",
      "Iteration 8240, Average Reward: -120.94\n",
      "Iteration 8241, Average Reward: -124.33\n",
      "Iteration 8242, Average Reward: -128.28\n",
      "Iteration 8243, Average Reward: -122.43\n",
      "Iteration 8244, Average Reward: -124.51\n",
      "Iteration 8245, Average Reward: -120.13\n",
      "Iteration 8246, Average Reward: -115.36\n",
      "Iteration 8247, Average Reward: -122.66\n",
      "Iteration 8248, Average Reward: -124.33\n",
      "Iteration 8249, Average Reward: -132.06\n",
      "Iteration 8250, Average Reward: -126.36\n",
      "Iteration 8251, Average Reward: -113.17\n",
      "Iteration 8252, Average Reward: -100.06\n",
      "Iteration 8253, Average Reward: -110.2\n",
      "Iteration 8254, Average Reward: -126.88\n",
      "Iteration 8255, Average Reward: -119.83\n",
      "Iteration 8256, Average Reward: -130.71\n",
      "Iteration 8257, Average Reward: -138.81\n",
      "Iteration 8258, Average Reward: -126.92\n",
      "Iteration 8259, Average Reward: -112.38\n",
      "Iteration 8260, Average Reward: -120.4\n",
      "Iteration 8261, Average Reward: -116.82\n",
      "Iteration 8262, Average Reward: -103.33\n",
      "Iteration 8263, Average Reward: -104.17\n",
      "Iteration 8264, Average Reward: -127.84\n",
      "Iteration 8265, Average Reward: -124.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8266, Average Reward: -116.35\n",
      "Iteration 8267, Average Reward: -121.62\n",
      "Iteration 8268, Average Reward: -119.62\n",
      "Iteration 8269, Average Reward: -135.2\n",
      "Iteration 8270, Average Reward: -129.44\n",
      "Iteration 8271, Average Reward: -130.01\n",
      "Iteration 8272, Average Reward: -139.49\n",
      "Iteration 8273, Average Reward: -121.05\n",
      "Iteration 8274, Average Reward: -110.41\n",
      "Iteration 8275, Average Reward: -106.89\n",
      "Iteration 8276, Average Reward: -109.95\n",
      "Iteration 8277, Average Reward: -117.21\n",
      "Iteration 8278, Average Reward: -118.99\n",
      "Iteration 8279, Average Reward: -114.31\n",
      "Iteration 8280, Average Reward: -126.88\n",
      "Iteration 8281, Average Reward: -120.99\n",
      "Iteration 8282, Average Reward: -131.22\n",
      "Iteration 8283, Average Reward: -129.0\n",
      "Iteration 8284, Average Reward: -120.81\n",
      "Iteration 8285, Average Reward: -126.84\n",
      "Iteration 8286, Average Reward: -130.2\n",
      "Iteration 8287, Average Reward: -117.45\n",
      "Iteration 8288, Average Reward: -120.03\n",
      "Iteration 8289, Average Reward: -108.14\n",
      "Iteration 8290, Average Reward: -105.2\n",
      "Iteration 8291, Average Reward: -96.62\n",
      "Iteration 8292, Average Reward: -102.97\n",
      "Iteration 8293, Average Reward: -121.98\n",
      "Iteration 8294, Average Reward: -132.74\n",
      "Iteration 8295, Average Reward: -138.82\n",
      "Iteration 8296, Average Reward: -128.07\n",
      "Iteration 8297, Average Reward: -128.38\n",
      "Iteration 8298, Average Reward: -129.5\n",
      "Iteration 8299, Average Reward: -132.64\n",
      "Iteration 8300, Average Reward: -138.7\n",
      "Iteration 8301, Average Reward: -136.41\n",
      "Iteration 8302, Average Reward: -135.35\n",
      "Iteration 8303, Average Reward: -113.7\n",
      "Iteration 8304, Average Reward: -107.91\n",
      "Iteration 8305, Average Reward: -111.93\n",
      "Iteration 8306, Average Reward: -121.01\n",
      "Iteration 8307, Average Reward: -129.26\n",
      "Iteration 8308, Average Reward: -120.63\n",
      "Iteration 8309, Average Reward: -123.09\n",
      "Iteration 8310, Average Reward: -124.13\n",
      "Iteration 8311, Average Reward: -129.81\n",
      "Iteration 8312, Average Reward: -128.69\n",
      "Iteration 8313, Average Reward: -135.45\n",
      "Iteration 8314, Average Reward: -143.33\n",
      "Iteration 8315, Average Reward: -124.87\n",
      "Iteration 8316, Average Reward: -130.94\n",
      "Iteration 8317, Average Reward: -136.48\n",
      "Iteration 8318, Average Reward: -135.55\n",
      "Iteration 8319, Average Reward: -126.31\n",
      "Iteration 8320, Average Reward: -112.28\n",
      "Iteration 8321, Average Reward: -99.77\n",
      "Iteration 8322, Average Reward: -109.13\n",
      "Iteration 8323, Average Reward: -136.07\n",
      "Iteration 8324, Average Reward: -137.54\n",
      "Iteration 8325, Average Reward: -118.36\n",
      "Iteration 8326, Average Reward: -107.84\n",
      "Iteration 8327, Average Reward: -130.01\n",
      "Iteration 8328, Average Reward: -131.55\n",
      "Iteration 8329, Average Reward: -130.33\n",
      "Iteration 8330, Average Reward: -137.62\n",
      "Iteration 8331, Average Reward: -124.82\n",
      "Iteration 8332, Average Reward: -134.78\n",
      "Iteration 8333, Average Reward: -125.94\n",
      "Iteration 8334, Average Reward: -110.85\n",
      "Iteration 8335, Average Reward: -109.74\n",
      "Iteration 8336, Average Reward: -99.86\n",
      "Iteration 8337, Average Reward: -107.46\n",
      "Iteration 8338, Average Reward: -126.5\n",
      "Iteration 8339, Average Reward: -134.58\n",
      "Iteration 8340, Average Reward: -126.94\n",
      "Iteration 8341, Average Reward: -141.55\n",
      "Iteration 8342, Average Reward: -136.18\n",
      "Iteration 8343, Average Reward: -122.5\n",
      "Iteration 8344, Average Reward: -127.33\n",
      "Iteration 8345, Average Reward: -115.92\n",
      "Iteration 8346, Average Reward: -116.27\n",
      "Iteration 8347, Average Reward: -110.88\n",
      "Iteration 8348, Average Reward: -123.14\n",
      "Iteration 8349, Average Reward: -125.54\n",
      "Iteration 8350, Average Reward: -116.96\n",
      "Iteration 8351, Average Reward: -127.4\n",
      "Iteration 8352, Average Reward: -126.65\n",
      "Iteration 8353, Average Reward: -137.87\n",
      "Iteration 8354, Average Reward: -148.49\n",
      "Iteration 8355, Average Reward: -133.03\n",
      "Iteration 8356, Average Reward: -117.82\n",
      "Iteration 8357, Average Reward: -108.49\n",
      "Iteration 8358, Average Reward: -104.98\n",
      "Iteration 8359, Average Reward: -115.44\n",
      "Iteration 8360, Average Reward: -116.11\n",
      "Iteration 8361, Average Reward: -110.92\n",
      "Iteration 8362, Average Reward: -122.44\n",
      "Iteration 8363, Average Reward: -127.33\n",
      "Iteration 8364, Average Reward: -126.78\n",
      "Iteration 8365, Average Reward: -126.48\n",
      "Iteration 8366, Average Reward: -121.67\n",
      "Iteration 8367, Average Reward: -127.74\n",
      "Iteration 8368, Average Reward: -130.31\n",
      "Iteration 8369, Average Reward: -130.58\n",
      "Iteration 8370, Average Reward: -136.21\n",
      "Iteration 8371, Average Reward: -128.84\n",
      "Iteration 8372, Average Reward: -101.97\n",
      "Iteration 8373, Average Reward: -89.2\n",
      "Iteration 8374, Average Reward: -100.26\n",
      "Iteration 8375, Average Reward: -100.38\n",
      "Iteration 8376, Average Reward: -108.81\n",
      "Iteration 8377, Average Reward: -123.09\n",
      "Iteration 8378, Average Reward: -149.2\n",
      "Iteration 8379, Average Reward: -145.61\n",
      "Iteration 8380, Average Reward: -131.45\n",
      "Iteration 8381, Average Reward: -116.37\n",
      "Iteration 8382, Average Reward: -122.35\n",
      "Iteration 8383, Average Reward: -108.14\n",
      "Iteration 8384, Average Reward: -110.89\n",
      "Iteration 8385, Average Reward: -109.18\n",
      "Iteration 8386, Average Reward: -112.31\n",
      "Iteration 8387, Average Reward: -123.32\n",
      "Iteration 8388, Average Reward: -102.71\n",
      "Iteration 8389, Average Reward: -102.75\n",
      "Iteration 8390, Average Reward: -120.97\n",
      "Iteration 8391, Average Reward: -113.27\n",
      "Iteration 8392, Average Reward: -111.59\n",
      "Iteration 8393, Average Reward: -119.92\n",
      "Iteration 8394, Average Reward: -131.75\n",
      "Iteration 8395, Average Reward: -124.14\n",
      "Iteration 8396, Average Reward: -115.57\n",
      "Iteration 8397, Average Reward: -120.77\n",
      "Iteration 8398, Average Reward: -123.01\n",
      "Iteration 8399, Average Reward: -132.2\n",
      "Iteration 8400, Average Reward: -139.1\n",
      "Iteration 8401, Average Reward: -130.56\n",
      "Iteration 8402, Average Reward: -125.99\n",
      "Iteration 8403, Average Reward: -118.13\n",
      "Iteration 8404, Average Reward: -112.36\n",
      "Iteration 8405, Average Reward: -116.34\n",
      "Iteration 8406, Average Reward: -117.75\n",
      "Iteration 8407, Average Reward: -118.61\n",
      "Iteration 8408, Average Reward: -124.34\n",
      "Iteration 8409, Average Reward: -121.7\n",
      "Iteration 8410, Average Reward: -129.83\n",
      "Iteration 8411, Average Reward: -132.75\n",
      "Iteration 8412, Average Reward: -139.92\n",
      "Iteration 8413, Average Reward: -149.0\n",
      "Iteration 8414, Average Reward: -156.0\n",
      "Iteration 8415, Average Reward: -147.41\n",
      "Iteration 8416, Average Reward: -112.26\n",
      "Iteration 8417, Average Reward: -112.94\n",
      "Iteration 8418, Average Reward: -115.65\n",
      "Iteration 8419, Average Reward: -115.29\n",
      "Iteration 8420, Average Reward: -119.29\n",
      "Iteration 8421, Average Reward: -111.29\n",
      "Iteration 8422, Average Reward: -130.83\n",
      "Iteration 8423, Average Reward: -121.38\n",
      "Iteration 8424, Average Reward: -112.66\n",
      "Iteration 8425, Average Reward: -119.39\n",
      "Iteration 8426, Average Reward: -124.25\n",
      "Iteration 8427, Average Reward: -104.99\n",
      "Iteration 8428, Average Reward: -116.03\n",
      "Iteration 8429, Average Reward: -129.68\n",
      "Iteration 8430, Average Reward: -126.53\n",
      "Iteration 8431, Average Reward: -118.55\n",
      "Iteration 8432, Average Reward: -114.35\n",
      "Iteration 8433, Average Reward: -114.39\n",
      "Iteration 8434, Average Reward: -121.27\n",
      "Iteration 8435, Average Reward: -126.2\n",
      "Iteration 8436, Average Reward: -116.91\n",
      "Iteration 8437, Average Reward: -118.43\n",
      "Iteration 8438, Average Reward: -132.91\n",
      "Iteration 8439, Average Reward: -136.32\n",
      "Iteration 8440, Average Reward: -121.79\n",
      "Iteration 8441, Average Reward: -120.74\n",
      "Iteration 8442, Average Reward: -121.13\n",
      "Iteration 8443, Average Reward: -119.1\n",
      "Iteration 8444, Average Reward: -104.18\n",
      "Iteration 8445, Average Reward: -107.35\n",
      "Iteration 8446, Average Reward: -101.96\n",
      "Iteration 8447, Average Reward: -111.47\n",
      "Iteration 8448, Average Reward: -127.22\n",
      "Iteration 8449, Average Reward: -136.83\n",
      "Iteration 8450, Average Reward: -138.46\n",
      "Iteration 8451, Average Reward: -146.76\n",
      "Iteration 8452, Average Reward: -149.29\n",
      "Iteration 8453, Average Reward: -138.56\n",
      "Iteration 8454, Average Reward: -113.17\n",
      "Iteration 8455, Average Reward: -120.11\n",
      "Iteration 8456, Average Reward: -111.27\n",
      "Iteration 8457, Average Reward: -122.88\n",
      "Iteration 8458, Average Reward: -131.43\n",
      "Iteration 8459, Average Reward: -110.6\n",
      "Iteration 8460, Average Reward: -124.52\n",
      "Iteration 8461, Average Reward: -124.77\n",
      "Iteration 8462, Average Reward: -122.72\n",
      "Iteration 8463, Average Reward: -121.71\n",
      "Iteration 8464, Average Reward: -136.91\n",
      "Iteration 8465, Average Reward: -130.82\n",
      "Iteration 8466, Average Reward: -137.35\n",
      "Iteration 8467, Average Reward: -132.35\n",
      "Iteration 8468, Average Reward: -124.55\n",
      "Iteration 8469, Average Reward: -110.54\n",
      "Iteration 8470, Average Reward: -118.17\n",
      "Iteration 8471, Average Reward: -110.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8472, Average Reward: -95.21\n",
      "Iteration 8473, Average Reward: -115.19\n",
      "Iteration 8474, Average Reward: -124.04\n",
      "Iteration 8475, Average Reward: -123.18\n",
      "Iteration 8476, Average Reward: -112.82\n",
      "Iteration 8477, Average Reward: -136.72\n",
      "Iteration 8478, Average Reward: -154.65\n",
      "Iteration 8479, Average Reward: -127.0\n",
      "Iteration 8480, Average Reward: -118.28\n",
      "Iteration 8481, Average Reward: -123.85\n",
      "Iteration 8482, Average Reward: -113.21\n",
      "Iteration 8483, Average Reward: -110.75\n",
      "Iteration 8484, Average Reward: -111.78\n",
      "Iteration 8485, Average Reward: -124.13\n",
      "Iteration 8486, Average Reward: -123.43\n",
      "Iteration 8487, Average Reward: -122.99\n",
      "Iteration 8488, Average Reward: -127.41\n",
      "Iteration 8489, Average Reward: -130.05\n",
      "Iteration 8490, Average Reward: -117.47\n",
      "Iteration 8491, Average Reward: -101.66\n",
      "Iteration 8492, Average Reward: -117.87\n",
      "Iteration 8493, Average Reward: -133.9\n",
      "Iteration 8494, Average Reward: -131.79\n",
      "Iteration 8495, Average Reward: -128.18\n",
      "Iteration 8496, Average Reward: -131.17\n",
      "Iteration 8497, Average Reward: -135.16\n",
      "Iteration 8498, Average Reward: -145.45\n",
      "Iteration 8499, Average Reward: -149.19\n",
      "Iteration 8500, Average Reward: -139.56\n",
      "Iteration 8501, Average Reward: -125.16\n",
      "Iteration 8502, Average Reward: -94.53\n",
      "Iteration 8503, Average Reward: -99.64\n",
      "Iteration 8504, Average Reward: -112.31\n",
      "Iteration 8505, Average Reward: -132.11\n",
      "Iteration 8506, Average Reward: -143.14\n",
      "Iteration 8507, Average Reward: -140.07\n",
      "Iteration 8508, Average Reward: -134.7\n",
      "Iteration 8509, Average Reward: -128.77\n",
      "Iteration 8510, Average Reward: -108.35\n",
      "Iteration 8511, Average Reward: -114.02\n",
      "Iteration 8512, Average Reward: -131.76\n",
      "Iteration 8513, Average Reward: -131.54\n",
      "Iteration 8514, Average Reward: -127.84\n",
      "Iteration 8515, Average Reward: -147.68\n",
      "Iteration 8516, Average Reward: -123.34\n",
      "Iteration 8517, Average Reward: -89.64\n",
      "Iteration 8518, Average Reward: -103.66\n",
      "Iteration 8519, Average Reward: -139.11\n",
      "Iteration 8520, Average Reward: -153.1\n",
      "Iteration 8521, Average Reward: -137.53\n",
      "Iteration 8522, Average Reward: -116.54\n",
      "Iteration 8523, Average Reward: -129.23\n",
      "Iteration 8524, Average Reward: -131.82\n",
      "Iteration 8525, Average Reward: -106.95\n",
      "Iteration 8526, Average Reward: -108.64\n",
      "Iteration 8527, Average Reward: -127.78\n",
      "Iteration 8528, Average Reward: -113.04\n",
      "Iteration 8529, Average Reward: -113.38\n",
      "Iteration 8530, Average Reward: -134.86\n",
      "Iteration 8531, Average Reward: -149.8\n",
      "Iteration 8532, Average Reward: -141.06\n",
      "Iteration 8533, Average Reward: -129.24\n",
      "Iteration 8534, Average Reward: -124.98\n",
      "Iteration 8535, Average Reward: -96.24\n",
      "Iteration 8536, Average Reward: -111.9\n",
      "Iteration 8537, Average Reward: -105.49\n",
      "Iteration 8538, Average Reward: -103.02\n",
      "Iteration 8539, Average Reward: -113.65\n",
      "Iteration 8540, Average Reward: -131.99\n",
      "Iteration 8541, Average Reward: -124.78\n",
      "Iteration 8542, Average Reward: -145.97\n",
      "Iteration 8543, Average Reward: -160.31\n",
      "Iteration 8544, Average Reward: -156.04\n",
      "Iteration 8545, Average Reward: -143.4\n",
      "Iteration 8546, Average Reward: -125.48\n",
      "Iteration 8547, Average Reward: -124.37\n",
      "Iteration 8548, Average Reward: -124.35\n",
      "Iteration 8549, Average Reward: -117.64\n",
      "Iteration 8550, Average Reward: -98.83\n",
      "Iteration 8551, Average Reward: -126.63\n",
      "Iteration 8552, Average Reward: -125.44\n",
      "Iteration 8553, Average Reward: -131.48\n",
      "Iteration 8554, Average Reward: -140.68\n",
      "Iteration 8555, Average Reward: -140.25\n",
      "Iteration 8556, Average Reward: -129.72\n",
      "Iteration 8557, Average Reward: -116.99\n",
      "Iteration 8558, Average Reward: -110.63\n",
      "Iteration 8559, Average Reward: -123.05\n",
      "Iteration 8560, Average Reward: -116.34\n",
      "Iteration 8561, Average Reward: -109.9\n",
      "Iteration 8562, Average Reward: -127.3\n",
      "Iteration 8563, Average Reward: -117.53\n",
      "Iteration 8564, Average Reward: -120.92\n",
      "Iteration 8565, Average Reward: -128.76\n",
      "Iteration 8566, Average Reward: -141.4\n",
      "Iteration 8567, Average Reward: -132.74\n",
      "Iteration 8568, Average Reward: -124.01\n",
      "Iteration 8569, Average Reward: -114.54\n",
      "Iteration 8570, Average Reward: -118.51\n",
      "Iteration 8571, Average Reward: -130.79\n",
      "Iteration 8572, Average Reward: -152.03\n",
      "Iteration 8573, Average Reward: -144.42\n",
      "Iteration 8574, Average Reward: -126.87\n",
      "Iteration 8575, Average Reward: -132.43\n",
      "Iteration 8576, Average Reward: -115.16\n",
      "Iteration 8577, Average Reward: -98.15\n",
      "Iteration 8578, Average Reward: -101.12\n",
      "Iteration 8579, Average Reward: -107.16\n",
      "Iteration 8580, Average Reward: -113.21\n",
      "Iteration 8581, Average Reward: -130.61\n",
      "Iteration 8582, Average Reward: -143.3\n",
      "Iteration 8583, Average Reward: -145.55\n",
      "Iteration 8584, Average Reward: -136.14\n",
      "Iteration 8585, Average Reward: -119.05\n",
      "Iteration 8586, Average Reward: -121.97\n",
      "Iteration 8587, Average Reward: -118.84\n",
      "Iteration 8588, Average Reward: -114.96\n",
      "Iteration 8589, Average Reward: -132.36\n",
      "Iteration 8590, Average Reward: -130.41\n",
      "Iteration 8591, Average Reward: -126.36\n",
      "Iteration 8592, Average Reward: -120.04\n",
      "Iteration 8593, Average Reward: -129.55\n",
      "Iteration 8594, Average Reward: -149.89\n",
      "Iteration 8595, Average Reward: -138.25\n",
      "Iteration 8596, Average Reward: -128.82\n",
      "Iteration 8597, Average Reward: -127.25\n",
      "Iteration 8598, Average Reward: -128.88\n",
      "Iteration 8599, Average Reward: -134.54\n",
      "Iteration 8600, Average Reward: -126.64\n",
      "Iteration 8601, Average Reward: -127.36\n",
      "Iteration 8602, Average Reward: -118.82\n",
      "Iteration 8603, Average Reward: -126.34\n",
      "Iteration 8604, Average Reward: -129.51\n",
      "Iteration 8605, Average Reward: -134.85\n",
      "Iteration 8606, Average Reward: -122.85\n",
      "Iteration 8607, Average Reward: -126.87\n",
      "Iteration 8608, Average Reward: -131.3\n",
      "Iteration 8609, Average Reward: -137.38\n",
      "Iteration 8610, Average Reward: -125.17\n",
      "Iteration 8611, Average Reward: -128.74\n",
      "Iteration 8612, Average Reward: -124.3\n",
      "Iteration 8613, Average Reward: -128.76\n",
      "Iteration 8614, Average Reward: -125.32\n",
      "Iteration 8615, Average Reward: -120.41\n",
      "Iteration 8616, Average Reward: -122.67\n",
      "Iteration 8617, Average Reward: -118.7\n",
      "Iteration 8618, Average Reward: -125.14\n",
      "Iteration 8619, Average Reward: -127.71\n",
      "Iteration 8620, Average Reward: -111.97\n",
      "Iteration 8621, Average Reward: -120.32\n",
      "Iteration 8622, Average Reward: -122.51\n",
      "Iteration 8623, Average Reward: -120.84\n",
      "Iteration 8624, Average Reward: -113.7\n",
      "Iteration 8625, Average Reward: -123.33\n",
      "Iteration 8626, Average Reward: -118.44\n",
      "Iteration 8627, Average Reward: -136.32\n",
      "Iteration 8628, Average Reward: -146.19\n",
      "Iteration 8629, Average Reward: -133.41\n",
      "Iteration 8630, Average Reward: -119.22\n",
      "Iteration 8631, Average Reward: -113.47\n",
      "Iteration 8632, Average Reward: -121.23\n",
      "Iteration 8633, Average Reward: -126.14\n",
      "Iteration 8634, Average Reward: -120.1\n",
      "Iteration 8635, Average Reward: -117.06\n",
      "Iteration 8636, Average Reward: -123.35\n",
      "Iteration 8637, Average Reward: -111.89\n",
      "Iteration 8638, Average Reward: -122.48\n",
      "Iteration 8639, Average Reward: -124.72\n",
      "Iteration 8640, Average Reward: -119.02\n",
      "Iteration 8641, Average Reward: -121.87\n",
      "Iteration 8642, Average Reward: -125.67\n",
      "Iteration 8643, Average Reward: -119.63\n",
      "Iteration 8644, Average Reward: -114.86\n",
      "Iteration 8645, Average Reward: -117.4\n",
      "Iteration 8646, Average Reward: -129.64\n",
      "Iteration 8647, Average Reward: -112.86\n",
      "Iteration 8648, Average Reward: -115.69\n",
      "Iteration 8649, Average Reward: -106.22\n",
      "Iteration 8650, Average Reward: -107.94\n",
      "Iteration 8651, Average Reward: -123.2\n",
      "Iteration 8652, Average Reward: -127.09\n",
      "Iteration 8653, Average Reward: -131.19\n",
      "Iteration 8654, Average Reward: -115.08\n",
      "Iteration 8655, Average Reward: -118.83\n",
      "Iteration 8656, Average Reward: -119.56\n",
      "Iteration 8657, Average Reward: -129.52\n",
      "Iteration 8658, Average Reward: -123.38\n",
      "Iteration 8659, Average Reward: -133.97\n",
      "Iteration 8660, Average Reward: -119.1\n",
      "Iteration 8661, Average Reward: -118.27\n",
      "Iteration 8662, Average Reward: -115.09\n",
      "Iteration 8663, Average Reward: -117.29\n",
      "Iteration 8664, Average Reward: -108.05\n",
      "Iteration 8665, Average Reward: -113.66\n",
      "Iteration 8666, Average Reward: -104.79\n",
      "Iteration 8667, Average Reward: -107.21\n",
      "Iteration 8668, Average Reward: -132.12\n",
      "Iteration 8669, Average Reward: -138.1\n",
      "Iteration 8670, Average Reward: -131.55\n",
      "Iteration 8671, Average Reward: -131.15\n",
      "Iteration 8672, Average Reward: -142.79\n",
      "Iteration 8673, Average Reward: -130.02\n",
      "Iteration 8674, Average Reward: -122.82\n",
      "Iteration 8675, Average Reward: -110.34\n",
      "Iteration 8676, Average Reward: -115.76\n",
      "Iteration 8677, Average Reward: -123.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8678, Average Reward: -131.86\n",
      "Iteration 8679, Average Reward: -122.7\n",
      "Iteration 8680, Average Reward: -142.07\n",
      "Iteration 8681, Average Reward: -145.91\n",
      "Iteration 8682, Average Reward: -132.24\n",
      "Iteration 8683, Average Reward: -133.85\n",
      "Iteration 8684, Average Reward: -134.53\n",
      "Iteration 8685, Average Reward: -109.77\n",
      "Iteration 8686, Average Reward: -123.65\n",
      "Iteration 8687, Average Reward: -115.43\n",
      "Iteration 8688, Average Reward: -110.16\n",
      "Iteration 8689, Average Reward: -108.29\n",
      "Iteration 8690, Average Reward: -118.72\n",
      "Iteration 8691, Average Reward: -121.81\n",
      "Iteration 8692, Average Reward: -120.44\n",
      "Iteration 8693, Average Reward: -104.82\n",
      "Iteration 8694, Average Reward: -112.38\n",
      "Iteration 8695, Average Reward: -123.65\n",
      "Iteration 8696, Average Reward: -119.11\n",
      "Iteration 8697, Average Reward: -124.96\n",
      "Iteration 8698, Average Reward: -137.91\n",
      "Iteration 8699, Average Reward: -133.44\n",
      "Iteration 8700, Average Reward: -130.22\n",
      "Iteration 8701, Average Reward: -114.08\n",
      "Iteration 8702, Average Reward: -126.18\n",
      "Iteration 8703, Average Reward: -126.48\n",
      "Iteration 8704, Average Reward: -119.48\n",
      "Iteration 8705, Average Reward: -104.59\n",
      "Iteration 8706, Average Reward: -116.59\n",
      "Iteration 8707, Average Reward: -109.12\n",
      "Iteration 8708, Average Reward: -110.92\n",
      "Iteration 8709, Average Reward: -110.17\n",
      "Iteration 8710, Average Reward: -134.02\n",
      "Iteration 8711, Average Reward: -123.29\n",
      "Iteration 8712, Average Reward: -114.28\n",
      "Iteration 8713, Average Reward: -148.36\n",
      "Iteration 8714, Average Reward: -145.43\n",
      "Iteration 8715, Average Reward: -126.6\n",
      "Iteration 8716, Average Reward: -118.02\n",
      "Iteration 8717, Average Reward: -112.16\n",
      "Iteration 8718, Average Reward: -121.56\n",
      "Iteration 8719, Average Reward: -141.9\n",
      "Iteration 8720, Average Reward: -132.06\n",
      "Iteration 8721, Average Reward: -128.32\n",
      "Iteration 8722, Average Reward: -131.84\n",
      "Iteration 8723, Average Reward: -124.08\n",
      "Iteration 8724, Average Reward: -108.57\n",
      "Iteration 8725, Average Reward: -109.68\n",
      "Iteration 8726, Average Reward: -135.39\n",
      "Iteration 8727, Average Reward: -147.49\n",
      "Iteration 8728, Average Reward: -133.28\n",
      "Iteration 8729, Average Reward: -130.43\n",
      "Iteration 8730, Average Reward: -138.03\n",
      "Iteration 8731, Average Reward: -126.74\n",
      "Iteration 8732, Average Reward: -124.84\n",
      "Iteration 8733, Average Reward: -115.01\n",
      "Iteration 8734, Average Reward: -111.28\n",
      "Iteration 8735, Average Reward: -118.36\n",
      "Iteration 8736, Average Reward: -110.09\n",
      "Iteration 8737, Average Reward: -108.68\n",
      "Iteration 8738, Average Reward: -127.26\n",
      "Iteration 8739, Average Reward: -138.8\n",
      "Iteration 8740, Average Reward: -140.02\n",
      "Iteration 8741, Average Reward: -126.68\n",
      "Iteration 8742, Average Reward: -121.95\n",
      "Iteration 8743, Average Reward: -105.34\n",
      "Iteration 8744, Average Reward: -126.73\n",
      "Iteration 8745, Average Reward: -129.13\n",
      "Iteration 8746, Average Reward: -120.09\n",
      "Iteration 8747, Average Reward: -114.6\n",
      "Iteration 8748, Average Reward: -111.87\n",
      "Iteration 8749, Average Reward: -128.29\n",
      "Iteration 8750, Average Reward: -141.55\n",
      "Iteration 8751, Average Reward: -124.66\n",
      "Iteration 8752, Average Reward: -111.34\n",
      "Iteration 8753, Average Reward: -110.08\n",
      "Iteration 8754, Average Reward: -99.36\n",
      "Iteration 8755, Average Reward: -97.13\n",
      "Iteration 8756, Average Reward: -105.23\n",
      "Iteration 8757, Average Reward: -124.02\n",
      "Iteration 8758, Average Reward: -133.16\n",
      "Iteration 8759, Average Reward: -132.04\n",
      "Iteration 8760, Average Reward: -137.77\n",
      "Iteration 8761, Average Reward: -142.74\n",
      "Iteration 8762, Average Reward: -140.52\n",
      "Iteration 8763, Average Reward: -122.95\n",
      "Iteration 8764, Average Reward: -110.64\n",
      "Iteration 8765, Average Reward: -112.96\n",
      "Iteration 8766, Average Reward: -105.49\n",
      "Iteration 8767, Average Reward: -119.61\n",
      "Iteration 8768, Average Reward: -128.43\n",
      "Iteration 8769, Average Reward: -131.46\n",
      "Iteration 8770, Average Reward: -112.62\n",
      "Iteration 8771, Average Reward: -115.28\n",
      "Iteration 8772, Average Reward: -121.66\n",
      "Iteration 8773, Average Reward: -124.36\n",
      "Iteration 8774, Average Reward: -125.84\n",
      "Iteration 8775, Average Reward: -141.7\n",
      "Iteration 8776, Average Reward: -141.51\n",
      "Iteration 8777, Average Reward: -120.36\n",
      "Iteration 8778, Average Reward: -124.96\n",
      "Iteration 8779, Average Reward: -132.69\n",
      "Iteration 8780, Average Reward: -122.64\n",
      "Iteration 8781, Average Reward: -111.74\n",
      "Iteration 8782, Average Reward: -118.26\n",
      "Iteration 8783, Average Reward: -112.08\n",
      "Iteration 8784, Average Reward: -125.23\n",
      "Iteration 8785, Average Reward: -122.01\n",
      "Iteration 8786, Average Reward: -146.55\n",
      "Iteration 8787, Average Reward: -141.17\n",
      "Iteration 8788, Average Reward: -134.67\n",
      "Iteration 8789, Average Reward: -122.63\n",
      "Iteration 8790, Average Reward: -105.32\n",
      "Iteration 8791, Average Reward: -119.61\n",
      "Iteration 8792, Average Reward: -117.64\n",
      "Iteration 8793, Average Reward: -103.58\n",
      "Iteration 8794, Average Reward: -127.42\n",
      "Iteration 8795, Average Reward: -141.79\n",
      "Iteration 8796, Average Reward: -148.57\n",
      "Iteration 8797, Average Reward: -142.78\n",
      "Iteration 8798, Average Reward: -126.3\n",
      "Iteration 8799, Average Reward: -113.16\n",
      "Iteration 8800, Average Reward: -97.6\n",
      "Iteration 8801, Average Reward: -98.35\n",
      "Iteration 8802, Average Reward: -86.62\n",
      "Iteration 8803, Average Reward: -125.09\n",
      "Iteration 8804, Average Reward: -122.78\n",
      "Iteration 8805, Average Reward: -124.74\n",
      "Iteration 8806, Average Reward: -127.99\n",
      "Iteration 8807, Average Reward: -117.32\n",
      "Iteration 8808, Average Reward: -121.88\n",
      "Iteration 8809, Average Reward: -121.26\n",
      "Iteration 8810, Average Reward: -119.5\n",
      "Iteration 8811, Average Reward: -106.95\n",
      "Iteration 8812, Average Reward: -122.34\n",
      "Iteration 8813, Average Reward: -126.01\n",
      "Iteration 8814, Average Reward: -123.81\n",
      "Iteration 8815, Average Reward: -110.53\n",
      "Iteration 8816, Average Reward: -102.58\n",
      "Iteration 8817, Average Reward: -101.5\n",
      "Iteration 8818, Average Reward: -105.63\n",
      "Iteration 8819, Average Reward: -107.99\n",
      "Iteration 8820, Average Reward: -120.58\n",
      "Iteration 8821, Average Reward: -119.34\n",
      "Iteration 8822, Average Reward: -132.47\n",
      "Iteration 8823, Average Reward: -124.46\n",
      "Iteration 8824, Average Reward: -116.39\n",
      "Iteration 8825, Average Reward: -120.77\n",
      "Iteration 8826, Average Reward: -123.84\n",
      "Iteration 8827, Average Reward: -129.64\n",
      "Iteration 8828, Average Reward: -119.63\n",
      "Iteration 8829, Average Reward: -101.71\n",
      "Iteration 8830, Average Reward: -124.92\n",
      "Iteration 8831, Average Reward: -130.95\n",
      "Iteration 8832, Average Reward: -134.45\n",
      "Iteration 8833, Average Reward: -122.67\n",
      "Iteration 8834, Average Reward: -138.98\n",
      "Iteration 8835, Average Reward: -138.37\n",
      "Iteration 8836, Average Reward: -127.79\n",
      "Iteration 8837, Average Reward: -131.85\n",
      "Iteration 8838, Average Reward: -137.4\n",
      "Iteration 8839, Average Reward: -108.67\n",
      "Iteration 8840, Average Reward: -94.94\n",
      "Iteration 8841, Average Reward: -123.79\n",
      "Iteration 8842, Average Reward: -132.94\n",
      "Iteration 8843, Average Reward: -118.89\n",
      "Iteration 8844, Average Reward: -119.93\n",
      "Iteration 8845, Average Reward: -128.76\n",
      "Iteration 8846, Average Reward: -121.87\n",
      "Iteration 8847, Average Reward: -114.97\n",
      "Iteration 8848, Average Reward: -143.29\n",
      "Iteration 8849, Average Reward: -141.89\n",
      "Iteration 8850, Average Reward: -129.34\n",
      "Iteration 8851, Average Reward: -118.19\n",
      "Iteration 8852, Average Reward: -125.76\n",
      "Iteration 8853, Average Reward: -114.08\n",
      "Iteration 8854, Average Reward: -105.64\n",
      "Iteration 8855, Average Reward: -110.96\n",
      "Iteration 8856, Average Reward: -135.53\n",
      "Iteration 8857, Average Reward: -142.48\n",
      "Iteration 8858, Average Reward: -138.82\n",
      "Iteration 8859, Average Reward: -128.81\n",
      "Iteration 8860, Average Reward: -128.3\n",
      "Iteration 8861, Average Reward: -122.29\n",
      "Iteration 8862, Average Reward: -115.97\n",
      "Iteration 8863, Average Reward: -113.31\n",
      "Iteration 8864, Average Reward: -122.04\n",
      "Iteration 8865, Average Reward: -120.53\n",
      "Iteration 8866, Average Reward: -149.15\n",
      "Iteration 8867, Average Reward: -122.55\n",
      "Iteration 8868, Average Reward: -115.14\n",
      "Iteration 8869, Average Reward: -112.25\n",
      "Iteration 8870, Average Reward: -133.42\n",
      "Iteration 8871, Average Reward: -132.03\n",
      "Iteration 8872, Average Reward: -113.97\n",
      "Iteration 8873, Average Reward: -122.47\n",
      "Iteration 8874, Average Reward: -125.66\n",
      "Iteration 8875, Average Reward: -113.31\n",
      "Iteration 8876, Average Reward: -111.23\n",
      "Iteration 8877, Average Reward: -110.04\n",
      "Iteration 8878, Average Reward: -111.91\n",
      "Iteration 8879, Average Reward: -106.36\n",
      "Iteration 8880, Average Reward: -112.56\n",
      "Iteration 8881, Average Reward: -131.51\n",
      "Iteration 8882, Average Reward: -134.88\n",
      "Iteration 8883, Average Reward: -132.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8884, Average Reward: -111.17\n",
      "Iteration 8885, Average Reward: -117.25\n",
      "Iteration 8886, Average Reward: -134.26\n",
      "Iteration 8887, Average Reward: -123.3\n",
      "Iteration 8888, Average Reward: -119.1\n",
      "Iteration 8889, Average Reward: -121.17\n",
      "Iteration 8890, Average Reward: -119.86\n",
      "Iteration 8891, Average Reward: -130.23\n",
      "Iteration 8892, Average Reward: -123.15\n",
      "Iteration 8893, Average Reward: -130.88\n",
      "Iteration 8894, Average Reward: -121.32\n",
      "Iteration 8895, Average Reward: -132.41\n",
      "Iteration 8896, Average Reward: -138.14\n",
      "Iteration 8897, Average Reward: -118.01\n",
      "Iteration 8898, Average Reward: -124.48\n",
      "Iteration 8899, Average Reward: -125.14\n",
      "Iteration 8900, Average Reward: -118.49\n",
      "Iteration 8901, Average Reward: -119.5\n",
      "Iteration 8902, Average Reward: -113.86\n",
      "Iteration 8903, Average Reward: -112.94\n",
      "Iteration 8904, Average Reward: -119.99\n",
      "Iteration 8905, Average Reward: -128.68\n",
      "Iteration 8906, Average Reward: -125.69\n",
      "Iteration 8907, Average Reward: -121.31\n",
      "Iteration 8908, Average Reward: -127.27\n",
      "Iteration 8909, Average Reward: -112.33\n",
      "Iteration 8910, Average Reward: -102.03\n",
      "Iteration 8911, Average Reward: -107.7\n",
      "Iteration 8912, Average Reward: -115.48\n",
      "Iteration 8913, Average Reward: -120.5\n",
      "Iteration 8914, Average Reward: -107.05\n",
      "Iteration 8915, Average Reward: -104.17\n",
      "Iteration 8916, Average Reward: -122.14\n",
      "Iteration 8917, Average Reward: -138.46\n",
      "Iteration 8918, Average Reward: -130.6\n",
      "Iteration 8919, Average Reward: -127.3\n",
      "Iteration 8920, Average Reward: -135.72\n",
      "Iteration 8921, Average Reward: -132.06\n",
      "Iteration 8922, Average Reward: -119.6\n",
      "Iteration 8923, Average Reward: -125.85\n",
      "Iteration 8924, Average Reward: -131.64\n",
      "Iteration 8925, Average Reward: -114.4\n",
      "Iteration 8926, Average Reward: -107.32\n",
      "Iteration 8927, Average Reward: -107.76\n",
      "Iteration 8928, Average Reward: -114.48\n",
      "Iteration 8929, Average Reward: -125.78\n",
      "Iteration 8930, Average Reward: -124.86\n",
      "Iteration 8931, Average Reward: -128.89\n",
      "Iteration 8932, Average Reward: -137.98\n",
      "Iteration 8933, Average Reward: -157.55\n",
      "Iteration 8934, Average Reward: -151.83\n",
      "Iteration 8935, Average Reward: -135.42\n",
      "Iteration 8936, Average Reward: -121.87\n",
      "Iteration 8937, Average Reward: -128.62\n",
      "Iteration 8938, Average Reward: -113.25\n",
      "Iteration 8939, Average Reward: -109.88\n",
      "Iteration 8940, Average Reward: -117.02\n",
      "Iteration 8941, Average Reward: -122.26\n",
      "Iteration 8942, Average Reward: -123.72\n",
      "Iteration 8943, Average Reward: -129.57\n",
      "Iteration 8944, Average Reward: -118.64\n",
      "Iteration 8945, Average Reward: -111.45\n",
      "Iteration 8946, Average Reward: -118.42\n",
      "Iteration 8947, Average Reward: -139.49\n",
      "Iteration 8948, Average Reward: -139.3\n",
      "Iteration 8949, Average Reward: -126.77\n",
      "Iteration 8950, Average Reward: -132.6\n",
      "Iteration 8951, Average Reward: -118.79\n",
      "Iteration 8952, Average Reward: -117.32\n",
      "Iteration 8953, Average Reward: -111.74\n",
      "Iteration 8954, Average Reward: -120.7\n",
      "Iteration 8955, Average Reward: -133.46\n",
      "Iteration 8956, Average Reward: -121.33\n",
      "Iteration 8957, Average Reward: -109.84\n",
      "Iteration 8958, Average Reward: -122.0\n",
      "Iteration 8959, Average Reward: -109.12\n",
      "Iteration 8960, Average Reward: -110.47\n",
      "Iteration 8961, Average Reward: -139.34\n",
      "Iteration 8962, Average Reward: -123.77\n",
      "Iteration 8963, Average Reward: -118.54\n",
      "Iteration 8964, Average Reward: -109.76\n",
      "Iteration 8965, Average Reward: -122.92\n",
      "Iteration 8966, Average Reward: -120.21\n",
      "Iteration 8967, Average Reward: -112.43\n",
      "Iteration 8968, Average Reward: -131.22\n",
      "Iteration 8969, Average Reward: -132.61\n",
      "Iteration 8970, Average Reward: -131.89\n",
      "Iteration 8971, Average Reward: -131.0\n",
      "Iteration 8972, Average Reward: -116.37\n",
      "Iteration 8973, Average Reward: -120.09\n",
      "Iteration 8974, Average Reward: -114.84\n",
      "Iteration 8975, Average Reward: -101.77\n",
      "Iteration 8976, Average Reward: -116.71\n",
      "Iteration 8977, Average Reward: -125.23\n",
      "Iteration 8978, Average Reward: -120.77\n",
      "Iteration 8979, Average Reward: -122.57\n",
      "Iteration 8980, Average Reward: -133.04\n",
      "Iteration 8981, Average Reward: -130.33\n",
      "Iteration 8982, Average Reward: -117.17\n",
      "Iteration 8983, Average Reward: -131.91\n",
      "Iteration 8984, Average Reward: -126.01\n",
      "Iteration 8985, Average Reward: -118.67\n",
      "Iteration 8986, Average Reward: -120.15\n",
      "Iteration 8987, Average Reward: -129.59\n",
      "Iteration 8988, Average Reward: -120.64\n",
      "Iteration 8989, Average Reward: -116.32\n",
      "Iteration 8990, Average Reward: -120.97\n",
      "Iteration 8991, Average Reward: -117.08\n",
      "Iteration 8992, Average Reward: -117.58\n",
      "Iteration 8993, Average Reward: -119.43\n",
      "Iteration 8994, Average Reward: -142.48\n",
      "Iteration 8995, Average Reward: -135.64\n",
      "Iteration 8996, Average Reward: -123.21\n",
      "Iteration 8997, Average Reward: -133.43\n",
      "Iteration 8998, Average Reward: -136.8\n",
      "Iteration 8999, Average Reward: -124.97\n",
      "Iteration 9000, Average Reward: -129.28\n",
      "Iteration 9001, Average Reward: -128.88\n",
      "Iteration 9002, Average Reward: -113.31\n",
      "Iteration 9003, Average Reward: -109.08\n",
      "Iteration 9004, Average Reward: -112.69\n",
      "Iteration 9005, Average Reward: -123.69\n",
      "Iteration 9006, Average Reward: -136.55\n",
      "Iteration 9007, Average Reward: -113.28\n",
      "Iteration 9008, Average Reward: -148.77\n",
      "Iteration 9009, Average Reward: -146.98\n",
      "Iteration 9010, Average Reward: -126.23\n",
      "Iteration 9011, Average Reward: -133.14\n",
      "Iteration 9012, Average Reward: -128.51\n",
      "Iteration 9013, Average Reward: -104.9\n",
      "Iteration 9014, Average Reward: -92.05\n",
      "Iteration 9015, Average Reward: -102.24\n",
      "Iteration 9016, Average Reward: -119.99\n",
      "Iteration 9017, Average Reward: -125.55\n",
      "Iteration 9018, Average Reward: -123.59\n",
      "Iteration 9019, Average Reward: -138.7\n",
      "Iteration 9020, Average Reward: -144.68\n",
      "Iteration 9021, Average Reward: -129.0\n",
      "Iteration 9022, Average Reward: -142.46\n",
      "Iteration 9023, Average Reward: -139.04\n",
      "Iteration 9024, Average Reward: -116.28\n",
      "Iteration 9025, Average Reward: -108.67\n",
      "Iteration 9026, Average Reward: -116.91\n",
      "Iteration 9027, Average Reward: -131.99\n",
      "Iteration 9028, Average Reward: -136.93\n",
      "Iteration 9029, Average Reward: -121.21\n",
      "Iteration 9030, Average Reward: -130.51\n",
      "Iteration 9031, Average Reward: -124.59\n",
      "Iteration 9032, Average Reward: -116.14\n",
      "Iteration 9033, Average Reward: -124.25\n",
      "Iteration 9034, Average Reward: -113.81\n",
      "Iteration 9035, Average Reward: -117.06\n",
      "Iteration 9036, Average Reward: -120.0\n",
      "Iteration 9037, Average Reward: -118.19\n",
      "Iteration 9038, Average Reward: -114.28\n",
      "Iteration 9039, Average Reward: -113.1\n",
      "Iteration 9040, Average Reward: -120.82\n",
      "Iteration 9041, Average Reward: -140.36\n",
      "Iteration 9042, Average Reward: -119.52\n",
      "Iteration 9043, Average Reward: -121.04\n",
      "Iteration 9044, Average Reward: -105.95\n",
      "Iteration 9045, Average Reward: -117.83\n",
      "Iteration 9046, Average Reward: -114.92\n",
      "Iteration 9047, Average Reward: -107.71\n",
      "Iteration 9048, Average Reward: -117.75\n",
      "Iteration 9049, Average Reward: -137.12\n",
      "Iteration 9050, Average Reward: -138.14\n",
      "Iteration 9051, Average Reward: -133.9\n",
      "Iteration 9052, Average Reward: -137.1\n",
      "Iteration 9053, Average Reward: -130.17\n",
      "Iteration 9054, Average Reward: -126.44\n",
      "Iteration 9055, Average Reward: -125.58\n",
      "Iteration 9056, Average Reward: -111.8\n",
      "Iteration 9057, Average Reward: -102.94\n",
      "Iteration 9058, Average Reward: -109.02\n",
      "Iteration 9059, Average Reward: -123.6\n",
      "Iteration 9060, Average Reward: -133.34\n",
      "Iteration 9061, Average Reward: -137.0\n",
      "Iteration 9062, Average Reward: -130.99\n",
      "Iteration 9063, Average Reward: -136.33\n",
      "Iteration 9064, Average Reward: -126.63\n",
      "Iteration 9065, Average Reward: -120.87\n",
      "Iteration 9066, Average Reward: -124.15\n",
      "Iteration 9067, Average Reward: -119.18\n",
      "Iteration 9068, Average Reward: -110.48\n",
      "Iteration 9069, Average Reward: -128.57\n",
      "Iteration 9070, Average Reward: -119.68\n",
      "Iteration 9071, Average Reward: -127.89\n",
      "Iteration 9072, Average Reward: -127.89\n",
      "Iteration 9073, Average Reward: -138.09\n",
      "Iteration 9074, Average Reward: -121.68\n",
      "Iteration 9075, Average Reward: -118.31\n",
      "Iteration 9076, Average Reward: -129.93\n",
      "Iteration 9077, Average Reward: -123.63\n",
      "Iteration 9078, Average Reward: -133.09\n",
      "Iteration 9079, Average Reward: -140.41\n",
      "Iteration 9080, Average Reward: -138.01\n",
      "Iteration 9081, Average Reward: -115.13\n",
      "Iteration 9082, Average Reward: -128.18\n",
      "Iteration 9083, Average Reward: -107.43\n",
      "Iteration 9084, Average Reward: -103.3\n",
      "Iteration 9085, Average Reward: -96.77\n",
      "Iteration 9086, Average Reward: -112.02\n",
      "Iteration 9087, Average Reward: -132.72\n",
      "Iteration 9088, Average Reward: -127.55\n",
      "Iteration 9089, Average Reward: -112.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9090, Average Reward: -120.85\n",
      "Iteration 9091, Average Reward: -115.37\n",
      "Iteration 9092, Average Reward: -116.12\n",
      "Iteration 9093, Average Reward: -118.9\n",
      "Iteration 9094, Average Reward: -110.89\n",
      "Iteration 9095, Average Reward: -124.65\n",
      "Iteration 9096, Average Reward: -121.34\n",
      "Iteration 9097, Average Reward: -127.16\n",
      "Iteration 9098, Average Reward: -131.75\n",
      "Iteration 9099, Average Reward: -118.43\n",
      "Iteration 9100, Average Reward: -94.49\n",
      "Iteration 9101, Average Reward: -92.9\n",
      "Iteration 9102, Average Reward: -102.38\n",
      "Iteration 9103, Average Reward: -100.82\n",
      "Iteration 9104, Average Reward: -134.03\n",
      "Iteration 9105, Average Reward: -127.59\n",
      "Iteration 9106, Average Reward: -126.74\n",
      "Iteration 9107, Average Reward: -133.7\n",
      "Iteration 9108, Average Reward: -142.68\n",
      "Iteration 9109, Average Reward: -117.3\n",
      "Iteration 9110, Average Reward: -119.95\n",
      "Iteration 9111, Average Reward: -111.26\n",
      "Iteration 9112, Average Reward: -115.17\n",
      "Iteration 9113, Average Reward: -127.18\n",
      "Iteration 9114, Average Reward: -125.27\n",
      "Iteration 9115, Average Reward: -137.84\n",
      "Iteration 9116, Average Reward: -128.5\n",
      "Iteration 9117, Average Reward: -137.6\n",
      "Iteration 9118, Average Reward: -116.52\n",
      "Iteration 9119, Average Reward: -144.53\n",
      "Iteration 9120, Average Reward: -137.49\n",
      "Iteration 9121, Average Reward: -121.29\n",
      "Iteration 9122, Average Reward: -117.77\n",
      "Iteration 9123, Average Reward: -125.51\n",
      "Iteration 9124, Average Reward: -99.12\n",
      "Iteration 9125, Average Reward: -88.45\n",
      "Iteration 9126, Average Reward: -120.44\n",
      "Iteration 9127, Average Reward: -115.89\n",
      "Iteration 9128, Average Reward: -120.59\n",
      "Iteration 9129, Average Reward: -137.53\n",
      "Iteration 9130, Average Reward: -125.68\n",
      "Iteration 9131, Average Reward: -120.85\n",
      "Iteration 9132, Average Reward: -118.36\n",
      "Iteration 9133, Average Reward: -98.59\n",
      "Iteration 9134, Average Reward: -111.56\n",
      "Iteration 9135, Average Reward: -106.4\n",
      "Iteration 9136, Average Reward: -105.74\n",
      "Iteration 9137, Average Reward: -106.49\n",
      "Iteration 9138, Average Reward: -126.96\n",
      "Iteration 9139, Average Reward: -135.84\n",
      "Iteration 9140, Average Reward: -132.39\n",
      "Iteration 9141, Average Reward: -121.38\n",
      "Iteration 9142, Average Reward: -119.16\n",
      "Iteration 9143, Average Reward: -129.81\n",
      "Iteration 9144, Average Reward: -143.23\n",
      "Iteration 9145, Average Reward: -134.94\n",
      "Iteration 9146, Average Reward: -134.72\n",
      "Iteration 9147, Average Reward: -127.22\n",
      "Iteration 9148, Average Reward: -130.07\n",
      "Iteration 9149, Average Reward: -118.85\n",
      "Iteration 9150, Average Reward: -107.6\n",
      "Iteration 9151, Average Reward: -115.59\n",
      "Iteration 9152, Average Reward: -131.84\n",
      "Iteration 9153, Average Reward: -123.46\n",
      "Iteration 9154, Average Reward: -120.12\n",
      "Iteration 9155, Average Reward: -123.31\n",
      "Iteration 9156, Average Reward: -116.01\n",
      "Iteration 9157, Average Reward: -125.93\n",
      "Iteration 9158, Average Reward: -132.31\n",
      "Iteration 9159, Average Reward: -147.49\n",
      "Iteration 9160, Average Reward: -151.0\n",
      "Iteration 9161, Average Reward: -136.85\n",
      "Iteration 9162, Average Reward: -136.33\n",
      "Iteration 9163, Average Reward: -134.76\n",
      "Iteration 9164, Average Reward: -115.63\n",
      "Iteration 9165, Average Reward: -112.16\n",
      "Iteration 9166, Average Reward: -110.32\n",
      "Iteration 9167, Average Reward: -110.55\n",
      "Iteration 9168, Average Reward: -97.11\n",
      "Iteration 9169, Average Reward: -111.03\n",
      "Iteration 9170, Average Reward: -128.21\n",
      "Iteration 9171, Average Reward: -129.87\n",
      "Iteration 9172, Average Reward: -125.58\n",
      "Iteration 9173, Average Reward: -128.21\n",
      "Iteration 9174, Average Reward: -124.79\n",
      "Iteration 9175, Average Reward: -113.08\n",
      "Iteration 9176, Average Reward: -109.42\n",
      "Iteration 9177, Average Reward: -112.52\n",
      "Iteration 9178, Average Reward: -133.97\n",
      "Iteration 9179, Average Reward: -121.95\n",
      "Iteration 9180, Average Reward: -128.4\n",
      "Iteration 9181, Average Reward: -131.27\n",
      "Iteration 9182, Average Reward: -138.84\n",
      "Iteration 9183, Average Reward: -141.07\n",
      "Iteration 9184, Average Reward: -122.21\n",
      "Iteration 9185, Average Reward: -104.52\n",
      "Iteration 9186, Average Reward: -120.37\n",
      "Iteration 9187, Average Reward: -118.7\n",
      "Iteration 9188, Average Reward: -112.49\n",
      "Iteration 9189, Average Reward: -118.75\n",
      "Iteration 9190, Average Reward: -131.68\n",
      "Iteration 9191, Average Reward: -120.41\n",
      "Iteration 9192, Average Reward: -131.23\n",
      "Iteration 9193, Average Reward: -139.42\n",
      "Iteration 9194, Average Reward: -130.11\n",
      "Iteration 9195, Average Reward: -124.26\n",
      "Iteration 9196, Average Reward: -104.17\n",
      "Iteration 9197, Average Reward: -115.88\n",
      "Iteration 9198, Average Reward: -121.67\n",
      "Iteration 9199, Average Reward: -129.45\n",
      "Iteration 9200, Average Reward: -133.47\n",
      "Iteration 9201, Average Reward: -132.85\n",
      "Iteration 9202, Average Reward: -138.21\n",
      "Iteration 9203, Average Reward: -144.92\n",
      "Iteration 9204, Average Reward: -147.0\n",
      "Iteration 9205, Average Reward: -124.47\n",
      "Iteration 9206, Average Reward: -111.02\n",
      "Iteration 9207, Average Reward: -111.05\n",
      "Iteration 9208, Average Reward: -113.53\n",
      "Iteration 9209, Average Reward: -122.57\n",
      "Iteration 9210, Average Reward: -103.43\n",
      "Iteration 9211, Average Reward: -103.98\n",
      "Iteration 9212, Average Reward: -117.79\n",
      "Iteration 9213, Average Reward: -103.84\n",
      "Iteration 9214, Average Reward: -108.0\n",
      "Iteration 9215, Average Reward: -121.26\n",
      "Iteration 9216, Average Reward: -126.64\n",
      "Iteration 9217, Average Reward: -124.59\n",
      "Iteration 9218, Average Reward: -111.46\n",
      "Iteration 9219, Average Reward: -109.46\n",
      "Iteration 9220, Average Reward: -114.04\n",
      "Iteration 9221, Average Reward: -111.08\n",
      "Iteration 9222, Average Reward: -109.72\n",
      "Iteration 9223, Average Reward: -127.84\n",
      "Iteration 9224, Average Reward: -135.43\n",
      "Iteration 9225, Average Reward: -114.96\n",
      "Iteration 9226, Average Reward: -126.05\n",
      "Iteration 9227, Average Reward: -110.25\n",
      "Iteration 9228, Average Reward: -114.92\n",
      "Iteration 9229, Average Reward: -133.44\n",
      "Iteration 9230, Average Reward: -113.24\n",
      "Iteration 9231, Average Reward: -120.19\n",
      "Iteration 9232, Average Reward: -123.55\n",
      "Iteration 9233, Average Reward: -115.25\n",
      "Iteration 9234, Average Reward: -119.35\n",
      "Iteration 9235, Average Reward: -137.86\n",
      "Iteration 9236, Average Reward: -133.44\n",
      "Iteration 9237, Average Reward: -127.57\n",
      "Iteration 9238, Average Reward: -137.04\n",
      "Iteration 9239, Average Reward: -146.69\n",
      "Iteration 9240, Average Reward: -132.52\n",
      "Iteration 9241, Average Reward: -123.38\n",
      "Iteration 9242, Average Reward: -112.83\n",
      "Iteration 9243, Average Reward: -118.46\n",
      "Iteration 9244, Average Reward: -113.96\n",
      "Iteration 9245, Average Reward: -124.96\n",
      "Iteration 9246, Average Reward: -154.2\n",
      "Iteration 9247, Average Reward: -151.68\n",
      "Iteration 9248, Average Reward: -146.0\n",
      "Iteration 9249, Average Reward: -128.95\n",
      "Iteration 9250, Average Reward: -139.23\n",
      "Iteration 9251, Average Reward: -127.67\n",
      "Iteration 9252, Average Reward: -116.7\n",
      "Iteration 9253, Average Reward: -116.81\n",
      "Iteration 9254, Average Reward: -109.54\n",
      "Iteration 9255, Average Reward: -90.6\n",
      "Iteration 9256, Average Reward: -110.24\n",
      "Iteration 9257, Average Reward: -132.4\n",
      "Iteration 9258, Average Reward: -117.23\n",
      "Iteration 9259, Average Reward: -109.22\n",
      "Iteration 9260, Average Reward: -120.28\n",
      "Iteration 9261, Average Reward: -147.63\n",
      "Iteration 9262, Average Reward: -150.81\n",
      "Iteration 9263, Average Reward: -136.26\n",
      "Iteration 9264, Average Reward: -145.73\n",
      "Iteration 9265, Average Reward: -139.45\n",
      "Iteration 9266, Average Reward: -110.97\n",
      "Iteration 9267, Average Reward: -100.47\n",
      "Iteration 9268, Average Reward: -109.77\n",
      "Iteration 9269, Average Reward: -115.0\n",
      "Iteration 9270, Average Reward: -128.58\n",
      "Iteration 9271, Average Reward: -134.8\n",
      "Iteration 9272, Average Reward: -142.83\n",
      "Iteration 9273, Average Reward: -140.65\n",
      "Iteration 9274, Average Reward: -115.09\n",
      "Iteration 9275, Average Reward: -113.62\n",
      "Iteration 9276, Average Reward: -108.73\n",
      "Iteration 9277, Average Reward: -112.74\n",
      "Iteration 9278, Average Reward: -121.81\n",
      "Iteration 9279, Average Reward: -136.54\n",
      "Iteration 9280, Average Reward: -117.7\n",
      "Iteration 9281, Average Reward: -128.53\n",
      "Iteration 9282, Average Reward: -130.22\n",
      "Iteration 9283, Average Reward: -134.27\n",
      "Iteration 9284, Average Reward: -115.19\n",
      "Iteration 9285, Average Reward: -117.94\n",
      "Iteration 9286, Average Reward: -114.6\n",
      "Iteration 9287, Average Reward: -118.21\n",
      "Iteration 9288, Average Reward: -120.03\n",
      "Iteration 9289, Average Reward: -105.83\n",
      "Iteration 9290, Average Reward: -114.91\n",
      "Iteration 9291, Average Reward: -121.72\n",
      "Iteration 9292, Average Reward: -132.37\n",
      "Iteration 9293, Average Reward: -124.95\n",
      "Iteration 9294, Average Reward: -127.23\n",
      "Iteration 9295, Average Reward: -128.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9296, Average Reward: -117.9\n",
      "Iteration 9297, Average Reward: -97.3\n",
      "Iteration 9298, Average Reward: -101.79\n",
      "Iteration 9299, Average Reward: -139.07\n",
      "Iteration 9300, Average Reward: -133.33\n",
      "Iteration 9301, Average Reward: -129.38\n",
      "Iteration 9302, Average Reward: -135.92\n",
      "Iteration 9303, Average Reward: -135.75\n",
      "Iteration 9304, Average Reward: -105.25\n",
      "Iteration 9305, Average Reward: -111.12\n",
      "Iteration 9306, Average Reward: -127.32\n",
      "Iteration 9307, Average Reward: -123.98\n",
      "Iteration 9308, Average Reward: -125.8\n",
      "Iteration 9309, Average Reward: -126.87\n",
      "Iteration 9310, Average Reward: -117.36\n",
      "Iteration 9311, Average Reward: -138.44\n",
      "Iteration 9312, Average Reward: -140.78\n",
      "Iteration 9313, Average Reward: -141.26\n",
      "Iteration 9314, Average Reward: -121.8\n",
      "Iteration 9315, Average Reward: -107.54\n",
      "Iteration 9316, Average Reward: -113.49\n",
      "Iteration 9317, Average Reward: -113.55\n",
      "Iteration 9318, Average Reward: -107.95\n",
      "Iteration 9319, Average Reward: -100.03\n",
      "Iteration 9320, Average Reward: -125.1\n",
      "Iteration 9321, Average Reward: -139.0\n",
      "Iteration 9322, Average Reward: -154.98\n",
      "Iteration 9323, Average Reward: -144.17\n",
      "Iteration 9324, Average Reward: -137.82\n",
      "Iteration 9325, Average Reward: -133.45\n",
      "Iteration 9326, Average Reward: -111.5\n",
      "Iteration 9327, Average Reward: -104.29\n",
      "Iteration 9328, Average Reward: -108.82\n",
      "Iteration 9329, Average Reward: -120.9\n",
      "Iteration 9330, Average Reward: -119.57\n",
      "Iteration 9331, Average Reward: -128.81\n",
      "Iteration 9332, Average Reward: -136.62\n",
      "Iteration 9333, Average Reward: -122.0\n",
      "Iteration 9334, Average Reward: -118.98\n",
      "Iteration 9335, Average Reward: -125.65\n",
      "Iteration 9336, Average Reward: -115.73\n",
      "Iteration 9337, Average Reward: -111.17\n",
      "Iteration 9338, Average Reward: -118.82\n",
      "Iteration 9339, Average Reward: -119.78\n",
      "Iteration 9340, Average Reward: -132.33\n",
      "Iteration 9341, Average Reward: -149.52\n",
      "Iteration 9342, Average Reward: -155.55\n",
      "Iteration 9343, Average Reward: -136.52\n",
      "Iteration 9344, Average Reward: -112.06\n",
      "Iteration 9345, Average Reward: -97.41\n",
      "Iteration 9346, Average Reward: -107.47\n",
      "Iteration 9347, Average Reward: -107.0\n",
      "Iteration 9348, Average Reward: -108.89\n",
      "Iteration 9349, Average Reward: -113.58\n",
      "Iteration 9350, Average Reward: -112.33\n",
      "Iteration 9351, Average Reward: -111.15\n",
      "Iteration 9352, Average Reward: -111.12\n",
      "Iteration 9353, Average Reward: -125.71\n",
      "Iteration 9354, Average Reward: -124.31\n",
      "Iteration 9355, Average Reward: -102.14\n",
      "Iteration 9356, Average Reward: -116.04\n",
      "Iteration 9357, Average Reward: -128.11\n",
      "Iteration 9358, Average Reward: -126.4\n",
      "Iteration 9359, Average Reward: -115.79\n",
      "Iteration 9360, Average Reward: -122.34\n",
      "Iteration 9361, Average Reward: -101.7\n",
      "Iteration 9362, Average Reward: -112.1\n",
      "Iteration 9363, Average Reward: -116.67\n",
      "Iteration 9364, Average Reward: -123.43\n",
      "Iteration 9365, Average Reward: -129.2\n",
      "Iteration 9366, Average Reward: -132.13\n",
      "Iteration 9367, Average Reward: -128.84\n",
      "Iteration 9368, Average Reward: -131.24\n",
      "Iteration 9369, Average Reward: -118.72\n",
      "Iteration 9370, Average Reward: -121.18\n",
      "Iteration 9371, Average Reward: -133.58\n",
      "Iteration 9372, Average Reward: -131.44\n",
      "Iteration 9373, Average Reward: -115.32\n",
      "Iteration 9374, Average Reward: -128.39\n",
      "Iteration 9375, Average Reward: -134.81\n",
      "Iteration 9376, Average Reward: -121.52\n",
      "Iteration 9377, Average Reward: -113.63\n",
      "Iteration 9378, Average Reward: -125.37\n",
      "Iteration 9379, Average Reward: -133.14\n",
      "Iteration 9380, Average Reward: -136.38\n",
      "Iteration 9381, Average Reward: -129.16\n",
      "Iteration 9382, Average Reward: -120.34\n",
      "Iteration 9383, Average Reward: -139.26\n",
      "Iteration 9384, Average Reward: -125.4\n",
      "Iteration 9385, Average Reward: -96.77\n",
      "Iteration 9386, Average Reward: -104.19\n",
      "Iteration 9387, Average Reward: -107.07\n",
      "Iteration 9388, Average Reward: -99.98\n",
      "Iteration 9389, Average Reward: -110.05\n",
      "Iteration 9390, Average Reward: -123.35\n",
      "Iteration 9391, Average Reward: -124.93\n",
      "Iteration 9392, Average Reward: -120.52\n",
      "Iteration 9393, Average Reward: -130.11\n",
      "Iteration 9394, Average Reward: -120.21\n",
      "Iteration 9395, Average Reward: -100.78\n",
      "Iteration 9396, Average Reward: -101.37\n",
      "Iteration 9397, Average Reward: -119.06\n",
      "Iteration 9398, Average Reward: -121.41\n",
      "Iteration 9399, Average Reward: -123.72\n",
      "Iteration 9400, Average Reward: -109.35\n",
      "Iteration 9401, Average Reward: -122.3\n",
      "Iteration 9402, Average Reward: -128.75\n",
      "Iteration 9403, Average Reward: -115.18\n",
      "Iteration 9404, Average Reward: -135.44\n",
      "Iteration 9405, Average Reward: -137.06\n",
      "Iteration 9406, Average Reward: -122.54\n",
      "Iteration 9407, Average Reward: -114.84\n",
      "Iteration 9408, Average Reward: -116.57\n",
      "Iteration 9409, Average Reward: -117.67\n",
      "Iteration 9410, Average Reward: -120.97\n",
      "Iteration 9411, Average Reward: -103.72\n",
      "Iteration 9412, Average Reward: -121.34\n",
      "Iteration 9413, Average Reward: -135.66\n",
      "Iteration 9414, Average Reward: -143.34\n",
      "Iteration 9415, Average Reward: -146.02\n",
      "Iteration 9416, Average Reward: -144.01\n",
      "Iteration 9417, Average Reward: -147.09\n",
      "Iteration 9418, Average Reward: -127.08\n",
      "Iteration 9419, Average Reward: -134.47\n",
      "Iteration 9420, Average Reward: -134.47\n",
      "Iteration 9421, Average Reward: -120.08\n",
      "Iteration 9422, Average Reward: -112.03\n",
      "Iteration 9423, Average Reward: -121.32\n",
      "Iteration 9424, Average Reward: -123.15\n",
      "Iteration 9425, Average Reward: -125.95\n",
      "Iteration 9426, Average Reward: -120.58\n",
      "Iteration 9427, Average Reward: -131.41\n",
      "Iteration 9428, Average Reward: -140.14\n",
      "Iteration 9429, Average Reward: -140.0\n",
      "Iteration 9430, Average Reward: -114.14\n",
      "Iteration 9431, Average Reward: -128.15\n",
      "Iteration 9432, Average Reward: -131.24\n",
      "Iteration 9433, Average Reward: -125.7\n",
      "Iteration 9434, Average Reward: -132.08\n",
      "Iteration 9435, Average Reward: -129.08\n",
      "Iteration 9436, Average Reward: -134.25\n",
      "Iteration 9437, Average Reward: -128.12\n",
      "Iteration 9438, Average Reward: -116.4\n",
      "Iteration 9439, Average Reward: -126.26\n",
      "Iteration 9440, Average Reward: -130.42\n",
      "Iteration 9441, Average Reward: -131.14\n",
      "Iteration 9442, Average Reward: -124.99\n",
      "Iteration 9443, Average Reward: -122.33\n",
      "Iteration 9444, Average Reward: -111.69\n",
      "Iteration 9445, Average Reward: -120.45\n",
      "Iteration 9446, Average Reward: -129.44\n",
      "Iteration 9447, Average Reward: -137.82\n",
      "Iteration 9448, Average Reward: -144.61\n",
      "Iteration 9449, Average Reward: -138.95\n",
      "Iteration 9450, Average Reward: -128.74\n",
      "Iteration 9451, Average Reward: -115.24\n",
      "Iteration 9452, Average Reward: -133.11\n",
      "Iteration 9453, Average Reward: -123.28\n",
      "Iteration 9454, Average Reward: -119.83\n",
      "Iteration 9455, Average Reward: -111.5\n",
      "Iteration 9456, Average Reward: -114.43\n",
      "Iteration 9457, Average Reward: -117.86\n",
      "Iteration 9458, Average Reward: -144.83\n",
      "Iteration 9459, Average Reward: -152.01\n",
      "Iteration 9460, Average Reward: -155.29\n",
      "Iteration 9461, Average Reward: -141.04\n",
      "Iteration 9462, Average Reward: -129.42\n",
      "Iteration 9463, Average Reward: -114.88\n",
      "Iteration 9464, Average Reward: -105.53\n",
      "Iteration 9465, Average Reward: -110.67\n",
      "Iteration 9466, Average Reward: -117.24\n",
      "Iteration 9467, Average Reward: -118.5\n",
      "Iteration 9468, Average Reward: -118.68\n",
      "Iteration 9469, Average Reward: -119.18\n",
      "Iteration 9470, Average Reward: -122.74\n",
      "Iteration 9471, Average Reward: -127.55\n",
      "Iteration 9472, Average Reward: -134.03\n",
      "Iteration 9473, Average Reward: -124.09\n",
      "Iteration 9474, Average Reward: -100.11\n",
      "Iteration 9475, Average Reward: -113.36\n",
      "Iteration 9476, Average Reward: -130.68\n",
      "Iteration 9477, Average Reward: -132.45\n",
      "Iteration 9478, Average Reward: -111.43\n",
      "Iteration 9479, Average Reward: -128.77\n",
      "Iteration 9480, Average Reward: -139.93\n",
      "Iteration 9481, Average Reward: -133.04\n",
      "Iteration 9482, Average Reward: -125.76\n",
      "Iteration 9483, Average Reward: -128.2\n",
      "Iteration 9484, Average Reward: -119.63\n",
      "Iteration 9485, Average Reward: -131.05\n",
      "Iteration 9486, Average Reward: -118.76\n",
      "Iteration 9487, Average Reward: -129.89\n",
      "Iteration 9488, Average Reward: -136.41\n",
      "Iteration 9489, Average Reward: -131.5\n",
      "Iteration 9490, Average Reward: -110.84\n",
      "Iteration 9491, Average Reward: -130.22\n",
      "Iteration 9492, Average Reward: -120.73\n",
      "Iteration 9493, Average Reward: -105.38\n",
      "Iteration 9494, Average Reward: -104.78\n",
      "Iteration 9495, Average Reward: -123.7\n",
      "Iteration 9496, Average Reward: -120.7\n",
      "Iteration 9497, Average Reward: -123.7\n",
      "Iteration 9498, Average Reward: -122.3\n",
      "Iteration 9499, Average Reward: -132.1\n",
      "Iteration 9500, Average Reward: -136.72\n",
      "Iteration 9501, Average Reward: -124.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9502, Average Reward: -126.05\n",
      "Iteration 9503, Average Reward: -134.42\n",
      "Iteration 9504, Average Reward: -134.22\n",
      "Iteration 9505, Average Reward: -117.19\n",
      "Iteration 9506, Average Reward: -107.57\n",
      "Iteration 9507, Average Reward: -105.82\n",
      "Iteration 9508, Average Reward: -111.08\n",
      "Iteration 9509, Average Reward: -117.75\n",
      "Iteration 9510, Average Reward: -122.28\n",
      "Iteration 9511, Average Reward: -128.18\n",
      "Iteration 9512, Average Reward: -131.04\n",
      "Iteration 9513, Average Reward: -131.79\n",
      "Iteration 9514, Average Reward: -117.99\n",
      "Iteration 9515, Average Reward: -125.71\n",
      "Iteration 9516, Average Reward: -124.32\n",
      "Iteration 9517, Average Reward: -117.08\n",
      "Iteration 9518, Average Reward: -118.29\n",
      "Iteration 9519, Average Reward: -127.71\n",
      "Iteration 9520, Average Reward: -123.15\n",
      "Iteration 9521, Average Reward: -120.53\n",
      "Iteration 9522, Average Reward: -120.8\n",
      "Iteration 9523, Average Reward: -118.22\n",
      "Iteration 9524, Average Reward: -112.77\n",
      "Iteration 9525, Average Reward: -99.66\n",
      "Iteration 9526, Average Reward: -108.2\n",
      "Iteration 9527, Average Reward: -110.27\n",
      "Iteration 9528, Average Reward: -115.03\n",
      "Iteration 9529, Average Reward: -111.65\n",
      "Iteration 9530, Average Reward: -126.08\n",
      "Iteration 9531, Average Reward: -131.31\n",
      "Iteration 9532, Average Reward: -141.04\n",
      "Iteration 9533, Average Reward: -138.25\n",
      "Iteration 9534, Average Reward: -143.67\n",
      "Iteration 9535, Average Reward: -136.53\n",
      "Iteration 9536, Average Reward: -128.67\n",
      "Iteration 9537, Average Reward: -104.93\n",
      "Iteration 9538, Average Reward: -116.83\n",
      "Iteration 9539, Average Reward: -117.99\n",
      "Iteration 9540, Average Reward: -114.54\n",
      "Iteration 9541, Average Reward: -113.35\n",
      "Iteration 9542, Average Reward: -127.86\n",
      "Iteration 9543, Average Reward: -112.24\n",
      "Iteration 9544, Average Reward: -118.66\n",
      "Iteration 9545, Average Reward: -123.49\n",
      "Iteration 9546, Average Reward: -137.57\n",
      "Iteration 9547, Average Reward: -122.59\n",
      "Iteration 9548, Average Reward: -122.72\n",
      "Iteration 9549, Average Reward: -122.83\n",
      "Iteration 9550, Average Reward: -117.76\n",
      "Iteration 9551, Average Reward: -118.21\n",
      "Iteration 9552, Average Reward: -114.11\n",
      "Iteration 9553, Average Reward: -116.71\n",
      "Iteration 9554, Average Reward: -115.62\n",
      "Iteration 9555, Average Reward: -127.57\n",
      "Iteration 9556, Average Reward: -130.29\n",
      "Iteration 9557, Average Reward: -122.54\n",
      "Iteration 9558, Average Reward: -111.48\n",
      "Iteration 9559, Average Reward: -120.29\n",
      "Iteration 9560, Average Reward: -120.87\n",
      "Iteration 9561, Average Reward: -135.48\n",
      "Iteration 9562, Average Reward: -135.73\n",
      "Iteration 9563, Average Reward: -139.64\n",
      "Iteration 9564, Average Reward: -124.08\n",
      "Iteration 9565, Average Reward: -106.76\n",
      "Iteration 9566, Average Reward: -108.76\n",
      "Iteration 9567, Average Reward: -109.02\n",
      "Iteration 9568, Average Reward: -99.85\n",
      "Iteration 9569, Average Reward: -112.95\n",
      "Iteration 9570, Average Reward: -130.55\n",
      "Iteration 9571, Average Reward: -124.49\n",
      "Iteration 9572, Average Reward: -128.58\n",
      "Iteration 9573, Average Reward: -122.71\n",
      "Iteration 9574, Average Reward: -123.39\n",
      "Iteration 9575, Average Reward: -116.73\n",
      "Iteration 9576, Average Reward: -113.99\n",
      "Iteration 9577, Average Reward: -132.42\n",
      "Iteration 9578, Average Reward: -136.55\n",
      "Iteration 9579, Average Reward: -126.0\n",
      "Iteration 9580, Average Reward: -122.61\n",
      "Iteration 9581, Average Reward: -125.29\n",
      "Iteration 9582, Average Reward: -133.26\n",
      "Iteration 9583, Average Reward: -137.72\n",
      "Iteration 9584, Average Reward: -133.77\n",
      "Iteration 9585, Average Reward: -134.52\n",
      "Iteration 9586, Average Reward: -116.55\n",
      "Iteration 9587, Average Reward: -112.32\n",
      "Iteration 9588, Average Reward: -107.24\n",
      "Iteration 9589, Average Reward: -114.85\n",
      "Iteration 9590, Average Reward: -125.26\n",
      "Iteration 9591, Average Reward: -137.59\n",
      "Iteration 9592, Average Reward: -134.61\n",
      "Iteration 9593, Average Reward: -141.08\n",
      "Iteration 9594, Average Reward: -138.33\n",
      "Iteration 9595, Average Reward: -115.97\n",
      "Iteration 9596, Average Reward: -124.59\n",
      "Iteration 9597, Average Reward: -129.04\n",
      "Iteration 9598, Average Reward: -123.56\n",
      "Iteration 9599, Average Reward: -112.27\n",
      "Iteration 9600, Average Reward: -109.3\n",
      "Iteration 9601, Average Reward: -119.21\n",
      "Iteration 9602, Average Reward: -124.76\n",
      "Iteration 9603, Average Reward: -139.19\n",
      "Iteration 9604, Average Reward: -135.68\n",
      "Iteration 9605, Average Reward: -129.84\n",
      "Iteration 9606, Average Reward: -112.66\n",
      "Iteration 9607, Average Reward: -102.99\n",
      "Iteration 9608, Average Reward: -104.32\n",
      "Iteration 9609, Average Reward: -113.06\n",
      "Iteration 9610, Average Reward: -130.78\n",
      "Iteration 9611, Average Reward: -144.37\n",
      "Iteration 9612, Average Reward: -141.22\n",
      "Iteration 9613, Average Reward: -130.02\n",
      "Iteration 9614, Average Reward: -132.01\n",
      "Iteration 9615, Average Reward: -126.55\n",
      "Iteration 9616, Average Reward: -108.42\n",
      "Iteration 9617, Average Reward: -114.59\n",
      "Iteration 9618, Average Reward: -121.69\n",
      "Iteration 9619, Average Reward: -117.58\n",
      "Iteration 9620, Average Reward: -119.86\n",
      "Iteration 9621, Average Reward: -123.27\n",
      "Iteration 9622, Average Reward: -119.89\n",
      "Iteration 9623, Average Reward: -130.01\n",
      "Iteration 9624, Average Reward: -142.43\n",
      "Iteration 9625, Average Reward: -141.92\n",
      "Iteration 9626, Average Reward: -137.46\n",
      "Iteration 9627, Average Reward: -111.64\n",
      "Iteration 9628, Average Reward: -108.82\n",
      "Iteration 9629, Average Reward: -114.99\n",
      "Iteration 9630, Average Reward: -120.51\n",
      "Iteration 9631, Average Reward: -105.25\n",
      "Iteration 9632, Average Reward: -105.7\n",
      "Iteration 9633, Average Reward: -122.86\n",
      "Iteration 9634, Average Reward: -131.68\n",
      "Iteration 9635, Average Reward: -116.21\n",
      "Iteration 9636, Average Reward: -124.88\n",
      "Iteration 9637, Average Reward: -133.1\n",
      "Iteration 9638, Average Reward: -138.19\n",
      "Iteration 9639, Average Reward: -126.47\n",
      "Iteration 9640, Average Reward: -132.4\n",
      "Iteration 9641, Average Reward: -146.2\n",
      "Iteration 9642, Average Reward: -143.37\n",
      "Iteration 9643, Average Reward: -126.23\n",
      "Iteration 9644, Average Reward: -113.85\n",
      "Iteration 9645, Average Reward: -119.2\n",
      "Iteration 9646, Average Reward: -117.19\n",
      "Iteration 9647, Average Reward: -113.49\n",
      "Iteration 9648, Average Reward: -111.96\n",
      "Iteration 9649, Average Reward: -100.99\n",
      "Iteration 9650, Average Reward: -117.14\n",
      "Iteration 9651, Average Reward: -122.17\n",
      "Iteration 9652, Average Reward: -131.31\n",
      "Iteration 9653, Average Reward: -136.05\n",
      "Iteration 9654, Average Reward: -142.25\n",
      "Iteration 9655, Average Reward: -120.06\n",
      "Iteration 9656, Average Reward: -117.75\n",
      "Iteration 9657, Average Reward: -115.41\n",
      "Iteration 9658, Average Reward: -121.02\n",
      "Iteration 9659, Average Reward: -128.3\n",
      "Iteration 9660, Average Reward: -121.88\n",
      "Iteration 9661, Average Reward: -132.25\n",
      "Iteration 9662, Average Reward: -116.54\n",
      "Iteration 9663, Average Reward: -113.43\n",
      "Iteration 9664, Average Reward: -114.28\n",
      "Iteration 9665, Average Reward: -136.85\n",
      "Iteration 9666, Average Reward: -135.69\n",
      "Iteration 9667, Average Reward: -142.02\n",
      "Iteration 9668, Average Reward: -150.6\n",
      "Iteration 9669, Average Reward: -149.57\n",
      "Iteration 9670, Average Reward: -130.82\n",
      "Iteration 9671, Average Reward: -119.75\n",
      "Iteration 9672, Average Reward: -107.47\n",
      "Iteration 9673, Average Reward: -126.1\n",
      "Iteration 9674, Average Reward: -125.08\n",
      "Iteration 9675, Average Reward: -118.88\n",
      "Iteration 9676, Average Reward: -125.96\n",
      "Iteration 9677, Average Reward: -126.15\n",
      "Iteration 9678, Average Reward: -135.72\n",
      "Iteration 9679, Average Reward: -136.31\n",
      "Iteration 9680, Average Reward: -128.75\n",
      "Iteration 9681, Average Reward: -113.41\n",
      "Iteration 9682, Average Reward: -108.91\n",
      "Iteration 9683, Average Reward: -114.82\n",
      "Iteration 9684, Average Reward: -120.11\n",
      "Iteration 9685, Average Reward: -117.15\n",
      "Iteration 9686, Average Reward: -129.53\n",
      "Iteration 9687, Average Reward: -120.27\n",
      "Iteration 9688, Average Reward: -128.22\n",
      "Iteration 9689, Average Reward: -126.9\n",
      "Iteration 9690, Average Reward: -116.7\n",
      "Iteration 9691, Average Reward: -121.17\n",
      "Iteration 9692, Average Reward: -109.12\n",
      "Iteration 9693, Average Reward: -107.12\n",
      "Iteration 9694, Average Reward: -117.34\n",
      "Iteration 9695, Average Reward: -131.81\n",
      "Iteration 9696, Average Reward: -121.67\n",
      "Iteration 9697, Average Reward: -100.48\n",
      "Iteration 9698, Average Reward: -120.05\n",
      "Iteration 9699, Average Reward: -115.83\n",
      "Iteration 9700, Average Reward: -105.29\n",
      "Iteration 9701, Average Reward: -129.08\n",
      "Iteration 9702, Average Reward: -131.57\n",
      "Iteration 9703, Average Reward: -112.87\n",
      "Iteration 9704, Average Reward: -108.61\n",
      "Iteration 9705, Average Reward: -120.01\n",
      "Iteration 9706, Average Reward: -109.58\n",
      "Iteration 9707, Average Reward: -95.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9708, Average Reward: -111.29\n",
      "Iteration 9709, Average Reward: -115.6\n",
      "Iteration 9710, Average Reward: -118.74\n",
      "Iteration 9711, Average Reward: -127.05\n",
      "Iteration 9712, Average Reward: -119.62\n",
      "Iteration 9713, Average Reward: -120.09\n",
      "Iteration 9714, Average Reward: -116.09\n",
      "Iteration 9715, Average Reward: -112.6\n",
      "Iteration 9716, Average Reward: -123.53\n",
      "Iteration 9717, Average Reward: -129.19\n",
      "Iteration 9718, Average Reward: -125.92\n",
      "Iteration 9719, Average Reward: -129.3\n",
      "Iteration 9720, Average Reward: -131.28\n",
      "Iteration 9721, Average Reward: -135.81\n",
      "Iteration 9722, Average Reward: -128.86\n",
      "Iteration 9723, Average Reward: -127.73\n",
      "Iteration 9724, Average Reward: -119.17\n",
      "Iteration 9725, Average Reward: -115.43\n",
      "Iteration 9726, Average Reward: -121.04\n",
      "Iteration 9727, Average Reward: -111.58\n",
      "Iteration 9728, Average Reward: -122.49\n",
      "Iteration 9729, Average Reward: -129.41\n",
      "Iteration 9730, Average Reward: -128.85\n",
      "Iteration 9731, Average Reward: -117.81\n",
      "Iteration 9732, Average Reward: -132.6\n",
      "Iteration 9733, Average Reward: -141.48\n",
      "Iteration 9734, Average Reward: -138.98\n",
      "Iteration 9735, Average Reward: -130.83\n",
      "Iteration 9736, Average Reward: -110.51\n",
      "Iteration 9737, Average Reward: -96.98\n",
      "Iteration 9738, Average Reward: -124.65\n",
      "Iteration 9739, Average Reward: -122.25\n",
      "Iteration 9740, Average Reward: -119.02\n",
      "Iteration 9741, Average Reward: -118.57\n",
      "Iteration 9742, Average Reward: -128.39\n",
      "Iteration 9743, Average Reward: -118.97\n",
      "Iteration 9744, Average Reward: -112.8\n",
      "Iteration 9745, Average Reward: -139.64\n",
      "Iteration 9746, Average Reward: -135.81\n",
      "Iteration 9747, Average Reward: -135.53\n",
      "Iteration 9748, Average Reward: -127.14\n",
      "Iteration 9749, Average Reward: -118.49\n",
      "Iteration 9750, Average Reward: -99.09\n",
      "Iteration 9751, Average Reward: -102.66\n",
      "Iteration 9752, Average Reward: -103.31\n",
      "Iteration 9753, Average Reward: -131.78\n",
      "Iteration 9754, Average Reward: -124.35\n",
      "Iteration 9755, Average Reward: -123.64\n",
      "Iteration 9756, Average Reward: -119.24\n",
      "Iteration 9757, Average Reward: -133.95\n",
      "Iteration 9758, Average Reward: -116.59\n",
      "Iteration 9759, Average Reward: -120.24\n",
      "Iteration 9760, Average Reward: -132.03\n",
      "Iteration 9761, Average Reward: -129.59\n",
      "Iteration 9762, Average Reward: -119.91\n",
      "Iteration 9763, Average Reward: -128.89\n",
      "Iteration 9764, Average Reward: -130.33\n",
      "Iteration 9765, Average Reward: -123.19\n",
      "Iteration 9766, Average Reward: -120.03\n",
      "Iteration 9767, Average Reward: -143.83\n",
      "Iteration 9768, Average Reward: -129.82\n",
      "Iteration 9769, Average Reward: -132.05\n",
      "Iteration 9770, Average Reward: -128.35\n",
      "Iteration 9771, Average Reward: -128.03\n",
      "Iteration 9772, Average Reward: -119.48\n",
      "Iteration 9773, Average Reward: -118.95\n",
      "Iteration 9774, Average Reward: -133.73\n",
      "Iteration 9775, Average Reward: -130.13\n",
      "Iteration 9776, Average Reward: -118.32\n",
      "Iteration 9777, Average Reward: -109.7\n",
      "Iteration 9778, Average Reward: -112.26\n",
      "Iteration 9779, Average Reward: -105.19\n",
      "Iteration 9780, Average Reward: -105.75\n",
      "Iteration 9781, Average Reward: -124.57\n",
      "Iteration 9782, Average Reward: -113.26\n",
      "Iteration 9783, Average Reward: -105.35\n",
      "Iteration 9784, Average Reward: -117.61\n",
      "Iteration 9785, Average Reward: -129.88\n",
      "Iteration 9786, Average Reward: -135.77\n",
      "Iteration 9787, Average Reward: -143.26\n",
      "Iteration 9788, Average Reward: -121.09\n",
      "Iteration 9789, Average Reward: -118.97\n",
      "Iteration 9790, Average Reward: -143.06\n",
      "Iteration 9791, Average Reward: -126.97\n",
      "Iteration 9792, Average Reward: -120.74\n",
      "Iteration 9793, Average Reward: -123.68\n",
      "Iteration 9794, Average Reward: -126.8\n",
      "Iteration 9795, Average Reward: -117.04\n",
      "Iteration 9796, Average Reward: -111.58\n",
      "Iteration 9797, Average Reward: -98.43\n",
      "Iteration 9798, Average Reward: -106.32\n",
      "Iteration 9799, Average Reward: -131.19\n",
      "Iteration 9800, Average Reward: -123.5\n",
      "Iteration 9801, Average Reward: -131.57\n",
      "Iteration 9802, Average Reward: -144.63\n",
      "Iteration 9803, Average Reward: -121.87\n",
      "Iteration 9804, Average Reward: -117.39\n",
      "Iteration 9805, Average Reward: -112.8\n",
      "Iteration 9806, Average Reward: -124.53\n",
      "Iteration 9807, Average Reward: -123.18\n",
      "Iteration 9808, Average Reward: -124.12\n",
      "Iteration 9809, Average Reward: -122.7\n",
      "Iteration 9810, Average Reward: -123.32\n",
      "Iteration 9811, Average Reward: -134.09\n",
      "Iteration 9812, Average Reward: -131.37\n",
      "Iteration 9813, Average Reward: -130.19\n",
      "Iteration 9814, Average Reward: -135.54\n",
      "Iteration 9815, Average Reward: -112.73\n",
      "Iteration 9816, Average Reward: -102.29\n",
      "Iteration 9817, Average Reward: -114.27\n",
      "Iteration 9818, Average Reward: -106.22\n",
      "Iteration 9819, Average Reward: -109.36\n",
      "Iteration 9820, Average Reward: -120.8\n",
      "Iteration 9821, Average Reward: -122.68\n",
      "Iteration 9822, Average Reward: -119.36\n",
      "Iteration 9823, Average Reward: -148.49\n",
      "Iteration 9824, Average Reward: -129.49\n",
      "Iteration 9825, Average Reward: -108.65\n",
      "Iteration 9826, Average Reward: -117.31\n",
      "Iteration 9827, Average Reward: -105.62\n",
      "Iteration 9828, Average Reward: -112.97\n",
      "Iteration 9829, Average Reward: -123.44\n",
      "Iteration 9830, Average Reward: -133.54\n",
      "Iteration 9831, Average Reward: -127.55\n",
      "Iteration 9832, Average Reward: -125.33\n",
      "Iteration 9833, Average Reward: -123.16\n",
      "Iteration 9834, Average Reward: -117.87\n",
      "Iteration 9835, Average Reward: -99.03\n",
      "Iteration 9836, Average Reward: -101.22\n",
      "Iteration 9837, Average Reward: -105.57\n",
      "Iteration 9838, Average Reward: -129.86\n",
      "Iteration 9839, Average Reward: -141.89\n",
      "Iteration 9840, Average Reward: -123.93\n",
      "Iteration 9841, Average Reward: -116.85\n",
      "Iteration 9842, Average Reward: -127.56\n",
      "Iteration 9843, Average Reward: -113.15\n",
      "Iteration 9844, Average Reward: -116.38\n",
      "Iteration 9845, Average Reward: -137.91\n",
      "Iteration 9846, Average Reward: -141.44\n",
      "Iteration 9847, Average Reward: -133.32\n",
      "Iteration 9848, Average Reward: -116.89\n",
      "Iteration 9849, Average Reward: -110.64\n",
      "Iteration 9850, Average Reward: -128.09\n",
      "Iteration 9851, Average Reward: -136.97\n",
      "Iteration 9852, Average Reward: -148.32\n",
      "Iteration 9853, Average Reward: -123.68\n",
      "Iteration 9854, Average Reward: -124.94\n",
      "Iteration 9855, Average Reward: -124.31\n",
      "Iteration 9856, Average Reward: -117.31\n",
      "Iteration 9857, Average Reward: -122.08\n",
      "Iteration 9858, Average Reward: -116.01\n",
      "Iteration 9859, Average Reward: -124.11\n",
      "Iteration 9860, Average Reward: -122.91\n",
      "Iteration 9861, Average Reward: -119.4\n",
      "Iteration 9862, Average Reward: -123.04\n",
      "Iteration 9863, Average Reward: -119.47\n",
      "Iteration 9864, Average Reward: -132.86\n",
      "Iteration 9865, Average Reward: -137.17\n",
      "Iteration 9866, Average Reward: -130.79\n",
      "Iteration 9867, Average Reward: -127.47\n",
      "Iteration 9868, Average Reward: -134.44\n",
      "Iteration 9869, Average Reward: -141.34\n",
      "Iteration 9870, Average Reward: -131.05\n",
      "Iteration 9871, Average Reward: -129.8\n",
      "Iteration 9872, Average Reward: -140.37\n",
      "Iteration 9873, Average Reward: -130.96\n",
      "Iteration 9874, Average Reward: -113.48\n",
      "Iteration 9875, Average Reward: -114.41\n",
      "Iteration 9876, Average Reward: -118.07\n",
      "Iteration 9877, Average Reward: -126.81\n",
      "Iteration 9878, Average Reward: -129.32\n",
      "Iteration 9879, Average Reward: -140.31\n",
      "Iteration 9880, Average Reward: -121.37\n",
      "Iteration 9881, Average Reward: -121.42\n",
      "Iteration 9882, Average Reward: -132.34\n",
      "Iteration 9883, Average Reward: -119.75\n",
      "Iteration 9884, Average Reward: -129.62\n",
      "Iteration 9885, Average Reward: -135.99\n",
      "Iteration 9886, Average Reward: -120.88\n",
      "Iteration 9887, Average Reward: -122.93\n",
      "Iteration 9888, Average Reward: -138.32\n",
      "Iteration 9889, Average Reward: -127.63\n",
      "Iteration 9890, Average Reward: -125.81\n",
      "Iteration 9891, Average Reward: -135.46\n",
      "Iteration 9892, Average Reward: -139.72\n",
      "Iteration 9893, Average Reward: -123.13\n",
      "Iteration 9894, Average Reward: -119.75\n",
      "Iteration 9895, Average Reward: -100.6\n",
      "Iteration 9896, Average Reward: -108.06\n",
      "Iteration 9897, Average Reward: -116.62\n",
      "Iteration 9898, Average Reward: -136.06\n",
      "Iteration 9899, Average Reward: -128.77\n",
      "Iteration 9900, Average Reward: -140.18\n",
      "Iteration 9901, Average Reward: -150.77\n",
      "Iteration 9902, Average Reward: -151.94\n",
      "Iteration 9903, Average Reward: -135.11\n",
      "Iteration 9904, Average Reward: -119.12\n",
      "Iteration 9905, Average Reward: -104.99\n",
      "Iteration 9906, Average Reward: -123.16\n",
      "Iteration 9907, Average Reward: -123.56\n",
      "Iteration 9908, Average Reward: -127.54\n",
      "Iteration 9909, Average Reward: -143.64\n",
      "Iteration 9910, Average Reward: -150.54\n",
      "Iteration 9911, Average Reward: -120.43\n",
      "Iteration 9912, Average Reward: -111.6\n",
      "Iteration 9913, Average Reward: -107.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9914, Average Reward: -114.44\n",
      "Iteration 9915, Average Reward: -114.64\n",
      "Iteration 9916, Average Reward: -128.12\n",
      "Iteration 9917, Average Reward: -140.04\n",
      "Iteration 9918, Average Reward: -137.19\n",
      "Iteration 9919, Average Reward: -130.81\n",
      "Iteration 9920, Average Reward: -141.26\n",
      "Iteration 9921, Average Reward: -130.29\n",
      "Iteration 9922, Average Reward: -113.57\n",
      "Iteration 9923, Average Reward: -118.18\n",
      "Iteration 9924, Average Reward: -120.25\n",
      "Iteration 9925, Average Reward: -140.2\n",
      "Iteration 9926, Average Reward: -126.4\n",
      "Iteration 9927, Average Reward: -114.37\n",
      "Iteration 9928, Average Reward: -114.94\n",
      "Iteration 9929, Average Reward: -126.66\n",
      "Iteration 9930, Average Reward: -131.11\n",
      "Iteration 9931, Average Reward: -125.05\n",
      "Iteration 9932, Average Reward: -111.22\n",
      "Iteration 9933, Average Reward: -121.93\n",
      "Iteration 9934, Average Reward: -120.35\n",
      "Iteration 9935, Average Reward: -116.14\n",
      "Iteration 9936, Average Reward: -129.45\n",
      "Iteration 9937, Average Reward: -132.24\n",
      "Iteration 9938, Average Reward: -124.65\n",
      "Iteration 9939, Average Reward: -116.32\n",
      "Iteration 9940, Average Reward: -131.35\n",
      "Iteration 9941, Average Reward: -135.11\n",
      "Iteration 9942, Average Reward: -133.82\n",
      "Iteration 9943, Average Reward: -124.98\n",
      "Iteration 9944, Average Reward: -121.93\n",
      "Iteration 9945, Average Reward: -143.32\n",
      "Iteration 9946, Average Reward: -138.64\n",
      "Iteration 9947, Average Reward: -127.79\n",
      "Iteration 9948, Average Reward: -120.33\n",
      "Iteration 9949, Average Reward: -117.1\n",
      "Iteration 9950, Average Reward: -105.15\n",
      "Iteration 9951, Average Reward: -116.29\n",
      "Iteration 9952, Average Reward: -129.37\n",
      "Iteration 9953, Average Reward: -125.54\n",
      "Iteration 9954, Average Reward: -138.63\n",
      "Iteration 9955, Average Reward: -127.68\n",
      "Iteration 9956, Average Reward: -109.09\n",
      "Iteration 9957, Average Reward: -119.35\n",
      "Iteration 9958, Average Reward: -105.08\n",
      "Iteration 9959, Average Reward: -104.14\n",
      "Iteration 9960, Average Reward: -106.28\n",
      "Iteration 9961, Average Reward: -134.4\n",
      "Iteration 9962, Average Reward: -117.83\n",
      "Iteration 9963, Average Reward: -123.67\n",
      "Iteration 9964, Average Reward: -115.34\n",
      "Iteration 9965, Average Reward: -113.59\n",
      "Iteration 9966, Average Reward: -121.92\n",
      "Iteration 9967, Average Reward: -140.62\n",
      "Iteration 9968, Average Reward: -137.2\n",
      "Iteration 9969, Average Reward: -128.66\n",
      "Iteration 9970, Average Reward: -125.69\n",
      "Iteration 9971, Average Reward: -132.67\n",
      "Iteration 9972, Average Reward: -125.49\n",
      "Iteration 9973, Average Reward: -121.68\n",
      "Iteration 9974, Average Reward: -116.29\n",
      "Iteration 9975, Average Reward: -114.98\n",
      "Iteration 9976, Average Reward: -115.11\n",
      "Iteration 9977, Average Reward: -117.67\n",
      "Iteration 9978, Average Reward: -135.2\n",
      "Iteration 9979, Average Reward: -135.53\n",
      "Iteration 9980, Average Reward: -130.85\n",
      "Iteration 9981, Average Reward: -142.17\n",
      "Iteration 9982, Average Reward: -121.54\n",
      "Iteration 9983, Average Reward: -119.23\n",
      "Iteration 9984, Average Reward: -122.23\n",
      "Iteration 9985, Average Reward: -126.87\n",
      "Iteration 9986, Average Reward: -135.82\n",
      "Iteration 9987, Average Reward: -126.55\n",
      "Iteration 9988, Average Reward: -125.92\n",
      "Iteration 9989, Average Reward: -126.0\n",
      "Iteration 9990, Average Reward: -120.12\n",
      "Iteration 9991, Average Reward: -119.71\n",
      "Iteration 9992, Average Reward: -111.93\n",
      "Iteration 9993, Average Reward: -133.93\n",
      "Iteration 9994, Average Reward: -144.18\n",
      "Iteration 9995, Average Reward: -128.25\n",
      "Iteration 9996, Average Reward: -115.8\n",
      "Iteration 9997, Average Reward: -128.3\n",
      "Iteration 9998, Average Reward: -124.38\n",
      "Iteration 9999, Average Reward: -108.44\n",
      "Iteration 10000, Average Reward: -113.87\n",
      "Final Best Position: [[-1.72697007e+01  3.22327758e+00  2.18591051e+00 ...  1.15647919e-01\n",
      "   6.04960941e-01  2.40016629e-01]\n",
      " [-6.03161534e+01  3.22327758e+00  4.37836841e+00 ...  5.42200057e-02\n",
      "   6.04960941e-01  9.68045859e-03]\n",
      " [-6.03161534e+01  3.22327758e+00  2.17067345e+01 ...  1.68440489e-01\n",
      "   6.04960941e-01  6.00181970e-01]\n",
      " ...\n",
      " [ 1.38380745e+01  3.22327758e+00 -4.40315904e+01 ...  5.83579114e-01\n",
      "   6.04960941e-01  7.80685781e-01]\n",
      " [-6.03161534e+01  3.22327758e+00 -2.29452815e+01 ...  1.60349104e-01\n",
      "   6.04960941e-01  7.42006495e-01]\n",
      " [-6.03161534e+01  3.22327758e+00 -2.11104715e+01 ...  2.01899402e-01\n",
      "   6.04960941e-01  9.37452032e-01]]\n",
      "Final Best Cost: -275.0\n",
      "Total optimization time: 58158.374499082565 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyswarms as ps\n",
    "import time\n",
    "# from your_environment import PowerSystemEnv  # Replace with your actual environment import\n",
    "\n",
    "# Initialize the environment\n",
    "env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "# Constants\n",
    "T = 48  # Time horizon\n",
    "NDim = T * env.action_space.shape[0]\n",
    "NParticle = 100  # Number of particles\n",
    "MaxIters = 10000  # Number of iterations\n",
    "\n",
    "# Define bounds\n",
    "bounds = (np.tile(env.action_space.low, T), np.tile(env.action_space.high, T))\n",
    "\n",
    "# Fitness function\n",
    "def f(x):\n",
    "    n_particles = x.shape[0]\n",
    "    j = np.zeros(n_particles)\n",
    "    for i in range(n_particles):\n",
    "        cumulative_reward = 0\n",
    "        env.reset()\n",
    "        actions = np.clip(x[i].reshape(T, env.action_space.shape[0]), env.action_space.low, env.action_space.high)\n",
    "        for t in range(T):\n",
    "            obs, reward, done, truncated, info = env.step(actions[t])\n",
    "            cumulative_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        j[i] = cumulative_reward\n",
    "    return j\n",
    "\n",
    "# Initialize swarm with adjusted hyperparameters for more exploration\n",
    "options = {'c1': 1.0, 'c2': 0.5, 'w': 0.9}\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=NParticle, dimensions=NDim, options=options, bounds=bounds)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Manually iterate through the optimization process\n",
    "for i in range(MaxIters):\n",
    "    optimizer.optimize(f, iters=1, verbose=False)  # Run one iteration at a time\n",
    "\n",
    "    # Calculate the average reward for this iteration\n",
    "    current_rewards = f(optimizer.swarm.position)\n",
    "    average_reward = np.mean(current_rewards)\n",
    "    \n",
    "    print(f\"Iteration {i+1}, Average Reward: {average_reward}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Final results\n",
    "best_cost = optimizer.cost_history[-1]  # Corrected attribute\n",
    "best_position = optimizer.pos_history[-1]  # Corrected attribute\n",
    "\n",
    "print(f\"Final Best Position: {best_position}\")\n",
    "print(f\"Final Best Cost: {best_cost}\")\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002536d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-11-03 17:49:32</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:10.84        </td></tr>\n",
       "<tr><td>Memory:      </td><td>48.3/127.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 11.0/24 CPUs, 0/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">    minibatch_buffer_siz\n",
       "e</th><th style=\"text-align: right;\">  num_sgd_iter</th><th style=\"text-align: right;\">  train_batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>IMPALA_PowerSystemEnv_b5163_00000</td><td>RUNNING </td><td>127.0.0.1:58652</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">              2500</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         50.2876</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-139.297</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                -275</td><td style=\"text-align: right;\">                 1</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=58652)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=58652)\u001b[0m 2023-11-03 17:48:26,427\tWARNING algorithm_config.py:656 -- Cannot create ImpalaConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=58160)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32988)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=58160)\u001b[0m OpenDSS Started successfully! \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58160)\u001b[0m OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=58160)\u001b[0m Reset options: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,341\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,412\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,415\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,421\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,421\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,427\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,427\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,427\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,427\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,431\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDiagGaussian` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchDiagGaussian` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19320)\u001b[0m 2023-11-03 17:48:32,431\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=58652)\u001b[0m 2023-11-03 17:48:32,649\tWARNING impala.py:521 -- In multi-GPU mode you should have at least as many multi-GPU tower stacks (to load data into on one device) as you have stack-index slots in the buffer! You have configured 1 stacks and a buffer of size 128. Setting `minibatch_buffer_size=1`.\n",
      "\u001b[2m\u001b[36m(Impala pid=58652)\u001b[0m 2023-11-03 17:48:32,649\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.multi_gpu_learner_thread.MultiGPULearnerThread` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=58652)\u001b[0m 2023-11-03 17:48:32,649\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.minibatch_buffer.MinibatchBuffer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=58652)\u001b[0m 2023-11-03 17:48:32,649\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.learner_thread.LearnerThread` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(Impala pid=58652)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(Impala pid=58652)\u001b[0m 2023-11-03 17:48:34,210\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.replay_ops.SimpleReplayBuffer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ImpalaConfig()\n",
    "# Print out some default values.\n",
    "print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.0005]), train_batch_size=tune.grid_search([ 2500]), minibatch_buffer_size = tune.grid_search([128]),\n",
    "        num_sgd_iter = tune.grid_search([ 1]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=10) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"IMPALA\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"PowerSystemEnv\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = PPOConfig()\n",
    "# Print out some default values.\n",
    "print(config.kl_coeff)   \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([0.001, 0.0001, 0.00001, 0.000001]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=23) \n",
    "config = config.environment(env=\"PowerSystemEnv\")\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner( \n",
    "    \"PPO\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54004f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = PPOConfig()\n",
    "# Print out some default values.\n",
    "print(config.kl_coeff)   \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.00001]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\")\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner( \n",
    "    \"PPO\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = PPOConfig()\n",
    "# Print out some default values.\n",
    "print(config.kl_coeff)   \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.00001]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=11) \n",
    "config = config.environment(env=\"PowerSystemEnv\")\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner( \n",
    "    \"PPO\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a697173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = PPOConfig()\n",
    "# Print out some default values.\n",
    "\n",
    "\n",
    "print(config.kl_coeff)   \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.00001]), train_batch_size=tune.grid_search([2500]), num_sgd_iter = tune.grid_search([1]))\n",
    "\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=7) \n",
    "\n",
    "config = config.environment(env=\"PowerSystemEnv\")\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner( \n",
    "    \"PPO\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe40e66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-11-04 13:47:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:17:51.78        </td></tr>\n",
       "<tr><td>Memory:      </td><td>114.6/127.8 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 24.0/24 CPUs, 0/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_PowerSystemEnv_bcc63_00000</td><td>TERMINATED</td><td>127.0.0.1:61752</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1054.76</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-0.36125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -48</td><td style=\"text-align: right;\">                 1</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 13:29:41,565\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=61752)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m 2023-11-04 13:29:45,331\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m 2023-11-04 13:29:45,333\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=59484)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=55976)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m 2023-11-04 13:29:52,099\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m 2023-11-04 13:29:52,131\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m OpenDSS Started successfully! \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m OpenDSS Version 9.5.1.1 (64-bit build); License Status: Open \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=52832)\u001b[0m Reset options: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m 2023-11-04 13:29:52,218\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m 2023-11-04 13:29:52,218\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m 2023-11-04 13:29:52,218\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=55976)\u001b[0m 2023-11-04 13:29:52,218\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m Install gputil for GPU system monitoring.\n",
      "2023-11-04 13:34:44,013\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/PPO', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\PPO'\n",
      "2023-11-04 13:39:46,923\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/PPO', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\PPO'\n",
      "2023-11-04 13:44:49,431\tWARNING syncer.py:586 -- Last sync command failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/PPO', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\PPO'\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m Caught sync error: Sync process failed: [WinError 32] Failed copying 'C:/Users/Alaa/ray_results/PPO/PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41/checkpoint_000050/.is_checkpoint' to 'c:///Users/Alaa/ray_results/PPO/PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41/checkpoint_000050/.is_checkpoint'. Detail: [Windows error 32] The process cannot access the file because it is being used by another process.\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m . Retrying after sleeping for 1.0 seconds...\n",
      "\u001b[2m\u001b[36m(pid=59636)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m 2023-11-04 13:29:52,664\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m 2023-11-04 13:29:52,712\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m 2023-11-04 13:29:52,712\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m 2023-11-04 13:29:52,712\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m 2023-11-04 13:29:52,712\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m Caught sync error: Sync process failed: [WinError 32] Failed copying 'C:/Users/Alaa/ray_results/PPO/PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41/checkpoint_000050/.is_checkpoint' to '/Users/Alaa/ray_results/PPO/PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41/checkpoint_000050/.is_checkpoint'. Detail: [Windows error 32] The process cannot access the file because it is being used by another process.\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m . Retrying after sleeping for 1.0 seconds...\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m Caught sync error: Sync process failed: [WinError 32] Failed copying 'C:/Users/Alaa/ray_results/PPO/PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41/checkpoint_000050/.is_checkpoint' to '/Users/Alaa/ray_results/PPO/PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41/checkpoint_000050/.is_checkpoint'. Detail: [Windows error 32] The process cannot access the file because it is being used by another process.\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m . Retrying after sleeping for 1.0 seconds...\n",
      "2023-11-04 13:47:33,345\tWARNING tune.py:1122 -- Trial Runner checkpointing failed: Sync process failed: GetFileInfo() yielded path 'C:/Users/Alaa/ray_results/PPO', which is outside base dir 'C:\\Users\\Alaa\\ray_results\\PPO'\n",
      "\u001b[2m\u001b[36m(PPO pid=61752)\u001b[0m Could not upload checkpoint to c://\\Users\\Alaa\\ray_results\\PPO\\PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41\\checkpoint_000050 even after 3 retries.Please check if the credentials expired and that the remote filesystem is supported. For large checkpoints or artifacts, consider increasing `SyncConfig(sync_timeout)` (current value: 1800 seconds).\n",
      "2023-11-04 13:47:33,751\tINFO tune.py:1148 -- Total run time: 1072.19 seconds (1071.75 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 0.4479674385237033}, 'default_policy': {'total_loss': 0.4479674385237033, 'policy_loss': -0.014584969107145027, 'vf_loss': 0.4084113155489664, 'vf_loss_unclipped': 5.908612712010392, 'vf_explained_var': 3.1882765959066623e-06, 'entropy': 134.4855212636594, 'mean_kl_loss': 0.007041669684725487, 'curr_lr': 0.0005, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 7.688672065734863}}, 'num_env_steps_sampled': 200000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 200000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 0.0, 'episode_reward_min': -48.0, 'episode_reward_mean': -0.36125, 'episode_len_mean': 1.0, 'episode_media': {}, 'episodes_this_iter': 4000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, -9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, -4.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38.0, -2.0, 0.0, -4.0, 0.0, -13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -23.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, -9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -20.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -5.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, -5.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -14.0, 0.0, -1.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, -7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, -5.0, -2.0, 0.0, 0.0, -35.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, -2.0, 0.0, 0.0, -7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -17.0, -2.0, -5.0, -2.0, -5.0, 0.0, -2.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -31.0, -2.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, -7.0, -5.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, -5.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -30.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -2.0, -5.0, 0.0, 0.0, 0.0, -18.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, -5.0, 0.0, -2.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -48.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -32.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10.0, -2.0, -7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -37.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -7.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, -4.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -7.0, -28.0, 0.0, 0.0, 0.0, -10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 35.37866048663432, 'mean_inference_ms': 3.207474317942833, 'mean_action_processing_ms': 0.4209051625180545, 'mean_env_wait_ms': 8.340075833642555, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.024807411432266235, 'StateBufferConnector_ms': 0.18964234590530396, 'ViewRequirementAgentConnector_ms': 0.6404817819595336}}, 'episode_reward_max': 0.0, 'episode_reward_min': -48.0, 'episode_reward_mean': -0.36125, 'episode_len_mean': 1.0, 'episodes_this_iter': 4000, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, -9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, -4.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38.0, -2.0, 0.0, -4.0, 0.0, -13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -23.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, -9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -20.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -5.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, -5.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -14.0, 0.0, -1.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, -7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, -5.0, -2.0, 0.0, 0.0, -35.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, -2.0, 0.0, 0.0, -7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -17.0, -2.0, -5.0, -2.0, -5.0, 0.0, -2.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -31.0, -2.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, -7.0, -5.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, -5.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -30.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -2.0, -5.0, 0.0, 0.0, 0.0, -18.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, -5.0, 0.0, -2.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, -1.0, -48.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, -3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -32.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10.0, -2.0, -7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -37.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -38.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -7.0, -6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -3.0, -4.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0.0, 0.0, -7.0, -28.0, 0.0, 0.0, 0.0, -10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 35.37866048663432, 'mean_inference_ms': 3.207474317942833, 'mean_action_processing_ms': 0.4209051625180545, 'mean_env_wait_ms': 8.340075833642555, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.024807411432266235, 'StateBufferConnector_ms': 0.18964234590530396, 'ViewRequirementAgentConnector_ms': 0.6404817819595336}, 'num_healthy_workers': 23, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 200000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 200000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 187.6504381643395, 'num_env_steps_trained_throughput_per_sec': 0.0, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 200000, 'timers': {'training_iteration_time_ms': 21160.37, 'sample_time_ms': 8781.271, 'synch_weights_time_ms': 38.0}, 'counters': {'num_env_steps_sampled': 200000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 200000, 'num_agent_steps_trained': 0}, 'done': True, 'trial_id': 'bcc63_00000', 'perf': {'cpu_util_percent': 71.53571428571429, 'ram_util_percent': 89.66428571428568}, 'experiment_tag': '0_lr=0.0005'},\n",
       "    path='c://\\\\Users\\\\Alaa\\\\ray_results\\\\PPO\\\\PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41',\n",
       "    checkpoint=Checkpoint(uri=c://\\Users\\Alaa\\ray_results\\PPO\\PPO_PowerSystemEnv_bcc63_00000_0_lr=0.0005_2023-11-04_13-29-41\\checkpoint_000050)\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = PPOConfig()\n",
    "# Print out some default values.\n",
    "print(config.kl_coeff)   \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.0005]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=1)  \n",
    "config = config.rollouts(num_rollout_workers=23) \n",
    "config = config.environment(env=\"PowerSystemEnv\")\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner( \n",
    "    \"PPO\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 200000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray import serve\n",
    "\n",
    "def train_ppo_model():\n",
    "    trainer = ppo.PPOTrainer(\n",
    "        config={\"framework\": \"torch\", \"num_workers\": 0},\n",
    "        env=\"CartPole-v0\",\n",
    "    )\n",
    "    # Train for one iteration\n",
    "    trainer.train()\n",
    "    trainer.save(\"/tmp/rllib_checkpoint\")\n",
    "    return \"/tmp/rllib_checkpoint/checkpoint_000001/checkpoint-1\"\n",
    "\n",
    "\n",
    "checkpoint_path = train_ppo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "config = ImpalaConfig()\n",
    "config = config.training(lr=0.0003, train_batch_size=512)  \n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=20)  \n",
    "print(config.to_dict())  \n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"PowerSystemEnv\")  \n",
    "algo.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7170b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.mbmpo import MBMPOConfig\n",
    "config = MBMPOConfig()\n",
    "config = config.training(lr=0.0003, train_batch_size=512)  \n",
    "config = config.resources(num_gpus=0) \n",
    "config = config.rollouts(num_rollout_workers=23)  \n",
    "print(config.to_dict())  \n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"PowerSystemEnv\")  \n",
    "algo.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.mbmpo import MBMPOConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = MBMPOConfig()\n",
    "# Print out some default values.\n",
    "#print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config = config.training(   \n",
    "    lr=tune.grid_search([0.00001])\n",
    ")\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=23) \n",
    "config = config.environment(env=\"PowerSystemEnv\") \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"AlphaStar\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\":0}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7096ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ars import ARSConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ARSConfig()\n",
    "# Print out some default values.\n",
    "print(config.action_noise_std)  \n",
    "# Update the config object.\n",
    "config = config.training(  \n",
    "    rollouts_used=tune.grid_search([32, 64]), eval_prob=0.5)\n",
    "# Set the config object's env.\n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"ARS\",\n",
    "    run_config=air.RunConfig(stop={\"sampler_results/episode_reward_mean\": 0}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.es import ESConfig\n",
    "from ray import tune\n",
    "config = ESConfig()\n",
    "# Print out some default values.\n",
    "print(config.action_noise_std)  \n",
    "# Update the config object.\n",
    "config = config.training(eval_prob=0.5)\n",
    "# Set the config object's env.\n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"ES\",\n",
    "    run_config=air.RunConfig(stop={\"sampler_results/episode_reward_mean\": 0}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876549b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! PyTorch is using GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # Get the current cuda device\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(\"Current device:\", current_device)\n",
    "\n",
    "    # Get the number of GPU devices available\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"Number of devices available:\", num_devices)\n",
    "\n",
    "    # Get the name of the current device\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(\"Device name:\", device_name)\n",
    "\n",
    "    # Additional information, like memory allocated, can be retrieved like this\n",
    "    memory_allocated = torch.cuda.memory_allocated(current_device)\n",
    "    memory_cached = torch.cuda.memory_reserved(current_device)\n",
    "    print(\"Memory Allocated (bytes):\", memory_allocated)\n",
    "    print(\"Memory Cached (bytes):\", memory_cached)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ImpalaConfig()\n",
    "# Print out some default values.\n",
    "print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config = config.training(   \n",
    "    lr=tune.grid_search([0.0001])\n",
    ")\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"IMPALA\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 0.0001}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ImpalaConfig()\n",
    "# Print out some default values.\n",
    "print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config = config.training(   \n",
    "    lr=tune.grid_search([0.00001])\n",
    ")\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=1)  \n",
    "config = config.rollouts(num_rollout_workers=1) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"IMPALA\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": -0.1}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ImpalaConfig()\n",
    "# Print out some default values.\n",
    "print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.0001,0.0005,0.0009]), train_batch_size=tune.grid_search([ 2500]), minibatch_buffer_size = tune.grid_search([1, 128]),\n",
    "        num_sgd_iter = tune.grid_search([ 1,30]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"IMPALA\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"PowerSystemEnv\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ImpalaConfig()\n",
    "# Print out some default values.\n",
    "print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.0001,0.0002,0.0003,0.0004, 0.0005]), train_batch_size=tune.grid_search([ 2500]), minibatch_buffer_size = tune.grid_search([1, 128]),\n",
    "        num_sgd_iter = tune.grid_search([ 1,30]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"IMPALA\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"PowerSystemEnv\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf08f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ImpalaConfig()\n",
    "# Print out some default values.\n",
    "print(config.vtrace)  \n",
    "# Update the config object.\n",
    "config.training( \n",
    "    lr=tune.grid_search([ 0.0005]), train_batch_size=tune.grid_search([ 2500]), minibatch_buffer_size = tune.grid_search([128]),\n",
    "        num_sgd_iter = tune.grid_search([ 1]))\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=8) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"IMPALA\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 100000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=\"PowerSystemEnv\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.a3c import A3CConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = A3CConfig()\n",
    "# Print out some default values.\n",
    "#print(config.sample_async)  \n",
    "# Update the config object.\n",
    "config = config.training( \n",
    "    lr=tune.grid_search([ 0.0001]))\n",
    "# Set the config object's env.\n",
    "config = config.environment(env=\"PowerSystemEnv\") \n",
    "config = config.rollouts(num_rollout_workers=23) \n",
    "config = config.resources(num_gpus=1)  \n",
    "\n",
    "\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"A3C\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 50000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c3298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 100000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"num_gpus\": 1,\n",
    "        \"num_workers\": 23,\n",
    "        \"lr\": 3e-3\n",
    "       # \"alpha\": 0.1,          # This decreases the temperature\n",
    "        # Optionally adjust the target entropy as well\n",
    "        # \"target_entropy\": some_value\n",
    "    },\n",
    "   local_dir=\"SAC-Test\",\n",
    "    checkpoint_freq=10,  # Save a checkpoint every 10 training iterations\n",
    "    checkpoint_at_end=True  # Also save a checkpoint at the end of training\n",
    ")\n",
    "\n",
    "# Access the last checkpoint path\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7334bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 100000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 23,\n",
    "        \"lr\": 3e-3\n",
    "       # \"alpha\": 0.1,          # This decreases the temperature\n",
    "        # Optionally adjust the target entropy as well\n",
    "        # \"target_entropy\": some_value\n",
    "    },\n",
    "   local_dir=\"SAC-Test\",\n",
    "    checkpoint_freq=10,  # Save a checkpoint every 10 training iterations\n",
    "    checkpoint_at_end=True  # Also save a checkpoint at the end of training\n",
    ")\n",
    "\n",
    "# Access the last checkpoint path\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c236d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 100000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 11,\n",
    "        \"lr\": 3e-3\n",
    "       # \"alpha\": 0.1,          # This decreases the temperature\n",
    "        # Optionally adjust the target entropy as well\n",
    "        # \"target_entropy\": some_value\n",
    "    },\n",
    "   local_dir=\"SAC-Test\",\n",
    "    checkpoint_freq=10,  # Save a checkpoint every 10 training iterations\n",
    "    checkpoint_at_end=True  # Also save a checkpoint at the end of training\n",
    ")\n",
    "\n",
    "# Access the last checkpoint path\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 100000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 5,\n",
    "        \"lr\": 3e-3\n",
    "       # \"alpha\": 0.1,          # This decreases the temperature\n",
    "        # Optionally adjust the target entropy as well\n",
    "        # \"target_entropy\": some_value\n",
    "    },\n",
    "   local_dir=\"SAC-Test\",\n",
    "    checkpoint_freq=10,  # Save a checkpoint every 10 training iterations\n",
    "    checkpoint_at_end=True  # Also save a checkpoint at the end of training\n",
    ")\n",
    "\n",
    "# Access the last checkpoint path\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"DDPG\",\n",
    "    stop={\"episodes_total\": 50000},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 5,\n",
    "        \"lr\": 3e-3\n",
    "       # \"alpha\": 0.1,          # This decreases the temperature\n",
    "        # Optionally adjust the target entropy as well\n",
    "        # \"target_entropy\": some_value\n",
    "    },\n",
    "   local_dir=\"SAC-Test\",\n",
    "    checkpoint_freq=10,  # Save a checkpoint every 10 training iterations\n",
    "    checkpoint_at_end=True  # Also save a checkpoint at the end of training\n",
    ")\n",
    "\n",
    "# Access the last checkpoint path\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import air\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.dreamer import DreamerConfig\n",
    "config = DreamerConfig()\n",
    "# Print out some default values.\n",
    "#print(config.clip_param)  \n",
    "# Update the config object.\n",
    "config = config.training(  \n",
    "    lr=tune.grid_search([0.0001]))\n",
    "# Set the config object's env.\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\") # Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"Dreamer\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 50000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ars import ARSConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = ARSConfig()\n",
    "# Print out some default values.\n",
    "print(config.action_noise_std)  \n",
    "# Update the config object.\n",
    "config = config.training(  \n",
    "    rollouts_used=tune.grid_search([32, 64]), eval_prob=0.5)\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\") \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"ARS\",\n",
    "    run_config=air.RunConfig(stop={\"episodes_total\": 50000}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.rllib.algorithms.a2c import A2CConfig\n",
    "config = A2CConfig()\n",
    "# Print out some default values.\n",
    "print(config.sample_async)   \n",
    "# Update the config object.\n",
    "config = config.training(lr=tune.grid_search(  \n",
    "    [0.001, 0.0001]), use_critic=False)\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"A3C\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 0}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1650e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.maml import MAMLConfig\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = MAMLConfig()\n",
    "# Print out some default values.\n",
    "print(config.lr)  \n",
    "# Update the config object.\n",
    "config = config.training(  \n",
    "    grad_clip=tune.grid_search([10.0, 40.0]))\n",
    "# Set the config object's env.\n",
    "# Set the config object's env.\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=5) \n",
    "config = config.environment(env=\"PowerSystemEnv\") # Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"MAML\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 0}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.agents.sac import SACTrainer\n",
    "\n",
    "# ... (your other imports and code)\n",
    "\n",
    "def train_and_evaluate():\n",
    "    # Create a trainer instance\n",
    "    trainer = SACTrainer(\n",
    "        config={\n",
    "            \"env\": \"PowerSystemEnv\",\n",
    "            \"num_gpus\": 0,\n",
    "            \"num_workers\": 19,\n",
    "            \"lr\": 3e-3\n",
    "            # ... (any other config options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Train the policy for a specified number of iterations\n",
    "    for i in range(10):  # Adjust the range as needed\n",
    "        result = trainer.train()\n",
    "        print(f\"Iteration {i}: {result}\")\n",
    "\n",
    "    # Save a checkpoint\n",
    "    checkpoint_path = trainer.save()\n",
    "    print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "    # Perform evaluation\n",
    "    evaluation_results = trainer.evaluate()\n",
    "    print(evaluation_results)\n",
    "\n",
    "# Run the training and evaluation\n",
    "train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d05eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.td3 import TD3Config\n",
    "from ray import air\n",
    "from ray import tune\n",
    "config = TD3Config()\n",
    "# Print out some default values.\n",
    "print(config.lr)   \n",
    "# Update the config object.\n",
    "config = config.training(lr=tune.grid_search(  \n",
    "    [0.001, 0.0001]))\n",
    "config = config.rollouts(num_rollout_workers=17)\n",
    "# Set the config object's env.\n",
    "config.environment(env=\"PowerSystemEnv\")  \n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(  \n",
    "    \"TD3\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium  --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded45ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.8.0 ray[rllib]==2.2.0 protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install py-dss-interface==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium>=0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Testing with partial observability of voltage violatiosn for each agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Tuple, Dict\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.sac import SAC\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "from py_dss_interface import DSSDLL\n",
    "import stable_baselines3\n",
    "#from stable_baselines3 import SAC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from stable_baselines3 import A2C, DQN, PPO, TD3, SAC\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenDSS\n",
    "dss = DSSDLL(r\"C:\\Program Files\\OpenDSS\")\n",
    "dss_file = r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\"\n",
    "dss.text(f\"compile [{dss_file}]\")\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "TRANSFORMER_TAPS_ACTION_LOW = 0.9\n",
    "TRANSFORMER_TAPS_ACTION_HIGH = 1.1\n",
    "\n",
    "CAPACITOR_ACTION_LOW = 0\n",
    "CAPACITOR_ACTION_HIGH = 1\n",
    "\n",
    "\n",
    "class PowerSystemEnv(MultiAgentEnv):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(PowerSystemEnv, self).__init__()\n",
    "\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses  = ['50', '49', '51', '47', '48', '44', '57', '54', '64', '63', '55', '62', '56', '65', '43', '60', '42', '52', '53', '61', '66', '36', '40', '67', '72', '86', '87', '76', '89', '97']\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "         # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=100 Pmpp=80\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=100 kVAR=0\")\n",
    "        \n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "    \n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)  # Skip the header\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "            \n",
    "            \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array(\n",
    "                [PV_KVAR_ACTION_LOW]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_LOW]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_LOW]*BESS_KW_ACTION_LEN \n",
    "            ),\n",
    "            high=np.array(\n",
    "                [PV_KVAR_ACTION_HIGH]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_HIGH]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_HIGH]*BESS_KW_ACTION_LEN \n",
    "               \n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "            \n",
    "\n",
    "     \n",
    "        # Assuming observation space is the voltage at each bus\n",
    "        self.observation_space = spaces.Box(low=0, high=10, shape=(278,), dtype=np.float32)  # Modified shape\n",
    "            \n",
    "         \n",
    "        # Update the action and observation spaces for each agent\n",
    "        self.action_space_dict = {\n",
    "            f\"agent_{i}\": spaces.Box(\n",
    "                low=np.concatenate(([PV_KW_ACTION_LOW] * 6, [PV_KVAR_ACTION_LOW] * 6, [BESS_KW_ACTION_LOW] * 6)),\n",
    "                high=np.concatenate(([PV_KW_ACTION_HIGH] * 6, [PV_KVAR_ACTION_HIGH] * 6, [BESS_KW_ACTION_HIGH] * 6)),\n",
    "                shape=(18,), dtype=np.float32) for i in range(5)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        self.observation_space_dict = {}\n",
    "\n",
    "        for i in range(4):  # For the first four agents\n",
    "            self.observation_space_dict[f'agent_{i}'] = spaces.Box(low=0, high=2, shape=(50,), dtype=np.float32)\n",
    "\n",
    "        # For the last agent\n",
    "        self.observation_space_dict['agent_4'] = spaces.Box(low=0, high=2, shape=(78,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        self.current_step = 0\n",
    "\n",
    "        \n",
    "         # Initialize control step counter\n",
    "        self.control_steps = int(0)\n",
    "\n",
    "        # Maximum control steps allowed in one episode\n",
    "        self.max_control_steps =int(3)  # for example\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "          # Define the agent IDs\n",
    "        self._agent_ids = [f'agent_{i}' for i in range(5)]\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "        \n",
    "    def _take_action(self, action_dict):\n",
    "        # Iterate through the action_dict\n",
    "        for agent_id, action in action_dict.items():\n",
    "            idx_offset = int(agent_id.split('_')[1]) * 6\n",
    "\n",
    "            # Actions for PVs\n",
    "            for idx in range(6):  \n",
    "                kw_action_value = action[idx]\n",
    "                kvar_action_value = action[idx + 6]\n",
    "                \n",
    "                # handle kW actions\n",
    "                irradiance = self.irradiance_profile[(self.current_step) % 8760]\n",
    "                scaled_pv_kw = kw_action_value * irradiance\n",
    "                if scaled_pv_kw > irradiance* self.KWrated:  # clip to 100\n",
    "                    scaled_pv_kw = irradiance* self.KWrated\n",
    "                \n",
    "                # handle kVAR actions\n",
    "                pv_kvar = kvar_action_value\n",
    "                    \n",
    "                S_max = 100  # Maximum apparent power (example value)\n",
    "\n",
    "                q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "    \n",
    "                pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "                    \n",
    "                \n",
    "                self.controller.text(f\"edit PVSystem.PV{idx + idx_offset + 1} phases=3 kV=4.16 kW={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "\n",
    "                \n",
    "\n",
    "            for idx, action_value in enumerate(action[12:18]):  \n",
    "                    self.controller.text(f\"edit Storage.Battery{idx + idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "\n",
    "   \n",
    "    # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "\n",
    "    def step(self, action_dict):\n",
    "    # Execute the action\n",
    "        self._take_action(action_dict)\n",
    "\n",
    "    # Calculate the rewards\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages if v <= 0.95 or v >= 1.05)\n",
    "\n",
    "    # Calculate the sum of absolute voltage deviations from 1\n",
    "        voltage_deviations = sum(abs(v - 1) for v in all_bus_voltages)\n",
    "\n",
    "    # Combine losses, violations, and deviations in the reward\n",
    "        reward = - voltage_violations\n",
    "\n",
    "    # Gather observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # Set the reward for all agents\n",
    "        rewards = {agent_id: reward for agent_id in self._agent_ids}\n",
    "\n",
    "    # Set termination status for all agents\n",
    "        terminations = {agent_id: (self.control_steps >= self.max_control_steps) or (voltage_deviations < 0.5) or (voltage_violations == 0) for agent_id in self._agent_ids}\n",
    "        terminations['__all__'] = any(terminations.values())\n",
    "\n",
    "    # Assuming no truncation; modify as needed\n",
    "        truncateds = {agent_id: False for agent_id in self._agent_ids}\n",
    "        truncateds['__all__'] = any(truncateds.values())\n",
    "\n",
    "    # Gather additional information if required\n",
    "        infos = {agent_id: {} for agent_id in self._agent_ids}\n",
    "\n",
    "    # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        return observations, rewards, terminations, truncateds, infos\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "    # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    # You can use options to customize the reset function if needed\n",
    "    # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        \n",
    "        #self.current_step = int(np.clip(np.random.normal(loc=0, scale=8760), 0, 8760))\n",
    "        self.current_step = np.random.randint(0, 8761)\n",
    "       \n",
    "\n",
    " # assuming the profile has 8760 hours  #use gaussian distribution\n",
    "        self.control_steps = 0\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None, squeeze=True).tolist()\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 8760], scale=0.5, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 8760]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 8760]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "        \n",
    "        # Get the observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # You can include any additional information here. If there's nothing, just return an empty dictionary.\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self, agent_id):\n",
    "    # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "    # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "\n",
    "    # Extract agent index from the agent_id\n",
    "        agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "    # Determine the start and end index for the observation chunk for this agent\n",
    "        if agent_idx < 4:\n",
    "            start_idx = agent_idx * 50\n",
    "            end_idx = start_idx + 50\n",
    "        else:  # This is agent_4\n",
    "            start_idx = 200\n",
    "            end_idx = 278  # or you could use end_idx = len(all_bus_voltages) if the size is not guaranteed to be 278\n",
    "\n",
    "    # Extract the observation chunk for this agent\n",
    "        agent_observation = all_bus_voltages[start_idx:end_idx]\n",
    "\n",
    "        return agent_observation\n",
    "\n",
    "\n",
    "    def get_agent_observation(self):\n",
    "        agent_obs = {}\n",
    "        for agent_id in self.agents:\n",
    "            agent_obs[agent_id] = self.get_observation()\n",
    "        return agent_obs\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "    \n",
    "\n",
    "irradiance_csv_file = r\"D:\\Alaa_Selim\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "load_profile_file = r\"D:\\Alaa_Selim\\LoadShape1.csv\"\n",
    "\n",
    "    \n",
    "    \n",
    "original_env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "env = MultiAgentEnvCompatibility(original_env)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PowerSystemEnv(\n",
    "                            dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "                            dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "                            irradiance_csv_file=irradiance_csv_file,\n",
    "                            load_profile_file=load_profile_file\n",
    "                            )\n",
    "\n",
    "register_env(\"PowerSystemEnv\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "# Suppose you have N agents\n",
    "N = 5  # adjust this as per your actual number of agents\n",
    "\n",
    "policies = {\n",
    "    f\"policy_{i}\": (None, \n",
    "                    original_env.observation_space_dict[f\"agent_{i}\"], \n",
    "                    original_env.action_space_dict[f\"agent_{i}\"], \n",
    "                    {}) \n",
    "    for i in range(N)\n",
    "}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    if \"agent_\" in agent_id:\n",
    "        # Extract agent number and return the corresponding policy\n",
    "        agent_num = int(agent_id.split(\"_\")[1])\n",
    "        return f\"policy_{agent_num}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "\n",
    "def evaluate(agent, env_creator):\n",
    "    env = env_creator({})\n",
    "    obs_dict = env.reset()  \n",
    "    done = {\"__all__\": False}\n",
    "    total_rewards = {agent_id: 0 for agent_id in obs_dict.keys()}\n",
    "    \n",
    "    all_obs = []  # List to store observations for all agents at each time step\n",
    "    all_actions = []  # List to store actions for all agents at each time step\n",
    "    \n",
    "    while not done[\"__all__\"]:\n",
    "        actions_dict = {}\n",
    "        current_obs = []\n",
    "        current_actions = []\n",
    "        for agent_id, obs in obs_dict.items():\n",
    "            policy_id = policy_mapping_fn(agent_id, None)\n",
    "            action = agent.compute_action(obs, policy_id=policy_id)\n",
    "            actions_dict[agent_id] = action\n",
    "            \n",
    "            current_obs.append(obs)\n",
    "            current_actions.append(action)\n",
    "        \n",
    "        all_obs.append(current_obs)\n",
    "        all_actions.append(current_actions)\n",
    "        \n",
    "        next_obs_dict, reward_dict, done, _ = env.step(actions_dict)\n",
    "        \n",
    "        for agent_id in obs_dict.keys():\n",
    "            total_rewards[agent_id] += reward_dict.get(agent_id, 0)\n",
    "            obs_dict[agent_id] = next_obs_dict[agent_id]\n",
    "        \n",
    "        done[\"__all__\"] = all(done.values())\n",
    "\n",
    "    avg_reward = np.mean(list(total_rewards.values()))\n",
    "    \n",
    "    return avg_reward, all_obs, all_actions\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "local_dir = r'C:\\Users\\Alaa\\ray_results\\SAC'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "from ray.tune import Callback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from ray.tune import Callback\n",
    "\n",
    "class CustomEvalCallback(Callback):\n",
    "    def __init__(self, eval_num_episodes=10):\n",
    "        self.eval_num_episodes = eval_num_episodes\n",
    "        self.eval_rewards = []\n",
    "        self.eval_reward_maxes = []\n",
    "        self.eval_reward_mins = []\n",
    "        self.all_eval_obs = []\n",
    "        self.all_eval_actions = []\n",
    "\n",
    "    def on_training_iteration(self, iteration, trials, trial, result, **info):\n",
    "        agent = trial.get_trainable()\n",
    "        env_name = trial.config[\"env\"]\n",
    "\n",
    "        # Lists to capture episode-wise observations and actions\n",
    "        episode_obs = []\n",
    "        episode_actions = []\n",
    "\n",
    "        # Evaluate the agent\n",
    "        rewards = []\n",
    "        for _ in range(self.eval_num_episodes):\n",
    "            obs_dict, actions_dict = self._single_evaluation_episode(agent, env_name)\n",
    "            rewards.append(np.mean(list(obs_dict.values())))\n",
    "            episode_obs.append(obs_dict)\n",
    "            episode_actions.append(actions_dict)\n",
    "\n",
    "        mean_reward = np.mean(rewards)\n",
    "        max_reward = np.max(rewards)\n",
    "        min_reward = np.min(rewards)\n",
    "\n",
    "        # Store results\n",
    "        self.eval_rewards.append(mean_reward)\n",
    "        self.eval_reward_maxes.append(max_reward)\n",
    "        self.eval_reward_mins.append(min_reward)\n",
    "        self.all_eval_obs.append(episode_obs)\n",
    "        self.all_eval_actions.append(episode_actions)\n",
    "        print(\"Mean Reward for current iteration:\", mean_reward)\n",
    "        print(\"Max Reward for current iteration:\", max_reward)\n",
    "        print(\"Min Reward for current iteration:\", min_reward)\n",
    "\n",
    "\n",
    "    def _single_evaluation_episode(self, agent, env_name):\n",
    "        \"\"\"Runs a single evaluation episode and returns observations and actions.\"\"\"\n",
    "        env = env_creator({})\n",
    "        obs_dict = env.reset()\n",
    "        done = {\"__all__\": False}\n",
    "        episode_rewards = {agent_id: 0 for agent_id in obs_dict.keys()}\n",
    "        episode_obs = {}\n",
    "        episode_actions = {}\n",
    "\n",
    "        while not done[\"__all__\"]:\n",
    "            actions_dict = {}\n",
    "            for agent_id, obs in obs_dict.items():\n",
    "                policy_id = policy_mapping_fn(agent_id, None)\n",
    "                actions_dict[agent_id] = agent.compute_action(obs, policy_id=policy_id)\n",
    "\n",
    "            next_obs_dict, reward_dict, done, _ = env.step(actions_dict)\n",
    "            for agent_id in obs_dict.keys():\n",
    "                episode_rewards[agent_id] += reward_dict.get(agent_id, 0)\n",
    "                obs_dict[agent_id] = next_obs_dict[agent_id]\n",
    "                \n",
    "                episode_obs.setdefault(agent_id, []).append(obs)\n",
    "                episode_actions.setdefault(agent_id, []).append(actions_dict[agent_id])\n",
    "\n",
    "            done[\"__all__\"] = all(done.values())\n",
    "\n",
    "        print(\"Single Evaluation Episode Completed!\")  # Debugging print\n",
    "        print(\"Observations:\", obs_dict)\n",
    "        print(\"Actions:\", actions_dict)\n",
    "        print(\"Episode Rewards:\", episode_rewards)\n",
    "\n",
    "        return episode_rewards, episode_obs, episode_actions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eval_callback = CustomEvalCallback()\n",
    "\n",
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episode_reward_mean\": 0},\n",
    "    config={\n",
    "        \"env\": \"PowerSystemEnv\",\n",
    "        \"environment\": {\n",
    "            \"disable_env_checking\": True\n",
    "        },\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn\n",
    "        },\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 17,\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    local_dir=local_dir,\n",
    "    callbacks=[eval_callback]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Evaluation Rewards:\", eval_callback.eval_rewards)\n",
    "print(\"Max Evaluation Rewards:\", eval_callback.eval_reward_maxes)\n",
    "print(\"Min Evaluation Rewards:\", eval_callback.eval_reward_mins)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.fill_between(\n",
    "    range(len(eval_callback.eval_rewards)), \n",
    "    eval_callback.eval_reward_mins, \n",
    "    eval_callback.eval_reward_maxes, \n",
    "    color='gray', alpha=0.4\n",
    ")\n",
    "plt.plot(eval_callback.eval_rewards, label='Mean Reward')\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Evaluation Reward')\n",
    "plt.title('Evaluation Rewards over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if eval_callback.all_eval_obs:\n",
    "    for i in range(N):  # Assuming N agents\n",
    "        agent_obs = eval_callback.all_eval_obs[0].get(f\"agent_{i}\", [])\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(agent_obs, label=f'Agent_{i} State')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('State Value')\n",
    "        plt.title(f'Agent_{i} States over Time')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if eval_callback.all_eval_actions:\n",
    "    for i in range(N):  # Assuming N agents\n",
    "        agent_actions = eval_callback.all_eval_actions[0].get(f\"agent_{i}\", [])\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(agent_actions, label=f'Agent_{i} Action')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Action Value')\n",
    "        plt.title(f'Agent_{i} Actions over Time')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34401088",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow  protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51322d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf207d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b027957",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6eedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Testing with partial observability of voltage violatiosn for each agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Tuple, Dict\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.sac import SAC\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "from py_dss_interface import DSSDLL\n",
    "#import stable_baselines3\n",
    "#from stable_baselines3 import SAC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from stable_baselines3 import A2C, DQN, PPO, TD3, SAC\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenDSS\n",
    "dss = DSSDLL(r\"C:\\Program Files\\OpenDSS\")\n",
    "dss_file = r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\"\n",
    "dss.text(f\"compile [{dss_file}]\")\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "TRANSFORMER_TAPS_ACTION_LOW = 0.9\n",
    "TRANSFORMER_TAPS_ACTION_HIGH = 1.1\n",
    "\n",
    "CAPACITOR_ACTION_LOW = 0\n",
    "CAPACITOR_ACTION_HIGH = 1\n",
    "\n",
    "\n",
    "class PowerSystemEnv(MultiAgentEnv):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(PowerSystemEnv, self).__init__()\n",
    "\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses  = ['50', '49', '51', '47', '48', '44', '57', '54', '64', '63', '55', '62', '56', '65', '43', '60', '42', '52', '53', '61', '66', '36', '40', '67', '72', '86', '87', '76', '89', '97']\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "        self.previous_reward = 0.0\n",
    "        self.alpha = 0.1  #\n",
    "         # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=100 Pmpp=80\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=100 kVAR=0\")\n",
    "        \n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "    \n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)  # Skip the header\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "            \n",
    "            \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array(\n",
    "                [PV_KVAR_ACTION_LOW]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_LOW]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_LOW]*BESS_KW_ACTION_LEN \n",
    "            ),\n",
    "            high=np.array(\n",
    "                [PV_KVAR_ACTION_HIGH]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_HIGH]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_HIGH]*BESS_KW_ACTION_LEN \n",
    "               \n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "            \n",
    "\n",
    "     \n",
    "        # Assuming observation space is the voltage at each bus\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(278,), dtype=np.float32)  # Modified shape\n",
    "            \n",
    "         \n",
    "        # Update the action and observation spaces for each agent\n",
    "        self.action_space_dict = {\n",
    "            f\"agent_{i}\": spaces.Box(\n",
    "                low=np.concatenate(([PV_KW_ACTION_LOW] * 6, [PV_KVAR_ACTION_LOW] * 6, [BESS_KW_ACTION_LOW] * 6)),\n",
    "                high=np.concatenate(([PV_KW_ACTION_HIGH] * 6, [PV_KVAR_ACTION_HIGH] * 6, [BESS_KW_ACTION_HIGH] * 6)),\n",
    "                shape=(18,), dtype=np.float32) for i in range(5)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        self.observation_space_dict = {}\n",
    "        \n",
    "        #self.observation_space_dict = {\n",
    "              # agent_id: spaces.Box(low=0, high=2, shape=(278,), dtype=np.float32) for agent_id in self._agent_ids\n",
    "              #  }\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        for i in range(5):  # For the first four agents\n",
    "            self.observation_space_dict[f'agent_{i}'] = spaces.Box(low=0, high=2, shape=(278,), dtype=np.float32)\n",
    "\n",
    "        # For the last agent\n",
    "        #self.observation_space_dict['agent_4'] = spaces.Box(low=0, high=2, shape=(78,), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        self.current_step = 0\n",
    "\n",
    "        \n",
    "         # Initialize control step counter\n",
    "        self.control_steps = 0\n",
    "\n",
    "        # Maximum control steps allowed in one episode\n",
    "        self.max_control_steps =int(2)  # for example\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "          # Define the agent IDs\n",
    "        self._agent_ids = [f'agent_{i}' for i in range(5)]\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "        \n",
    "    def _take_action(self, action_dict):\n",
    "    # Define total actions per agent\n",
    "        TOTAL_ACTIONS_PER_AGENT = 6 + 6 + 6\n",
    "\n",
    "    # Iterate through the action_dict\n",
    "        for agent_id, action in action_dict.items():\n",
    "            agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "        # Determine action segments for this agent\n",
    "            pv_kw_actions = action[:6]\n",
    "            pv_kvar_actions = action[6:12]\n",
    "            bess_kw_actions = action[12:18]\n",
    "\n",
    "        # Index offset based on agent_idx for PV and battery control\n",
    "            pv_idx_offset = agent_idx * 6\n",
    "            battery_idx_offset = agent_idx * 6\n",
    "\n",
    "        # Actions for PVs\n",
    "            for idx, (kw_action_value, kvar_action_value) in enumerate(zip(pv_kw_actions, pv_kvar_actions)):\n",
    "            # handle kW actions\n",
    "                irradiance = self.irradiance_profile[(self.current_step) % 8760]\n",
    "                scaled_pv_kw = kw_action_value * irradiance\n",
    "                if scaled_pv_kw > irradiance * self.KWrated:  # clip to max\n",
    "                    scaled_pv_kw = irradiance * self.KWrated\n",
    "\n",
    "            # handle kVAR actions\n",
    "                pv_kvar = kvar_action_value\n",
    "                S_max = 100  # Maximum apparent power (example value)\n",
    "                q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "                pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "                self.controller.text(f\"edit PVSystem.PV{idx + pv_idx_offset + 1} phases=3 kV=4.16 Pmpp={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "\n",
    "            for idx, action_value in enumerate(bess_kw_actions):\n",
    "                self.controller.text(f\"edit Storage.Battery{idx + battery_idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action_dict):\n",
    "    # Execute the action\n",
    "        self._take_action(action_dict)\n",
    "\n",
    "    # Calculate the rewards\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages if v <= 0.95 or v >= 1.05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate the sum of squared voltage deviations from 1. This penalizes larger deviations more heavily.\n",
    "        voltage_deviations = sum((v - 1)**2 for v in all_bus_voltages)\n",
    "\n",
    "\n",
    "\n",
    "        # Define penalty weights for different components\n",
    "        w_deviation = 1.0\n",
    "        w_violation = 5.0\n",
    "        w_loss = 0.001\n",
    "\n",
    "        # Combine losses, violations, and deviations in the reward\n",
    "        #reward = - w_deviation * voltage_deviations - w_violation * voltage_violations\n",
    "        reward= - voltage_violations \n",
    "        \n",
    "        #-  self.control_steps\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Gather observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # Set the reward for all agents\n",
    "        rewards = {agent_id: reward for agent_id in self._agent_ids}\n",
    "\n",
    "    # Set termination status for all agents\n",
    "        terminations = {agent_id: (self.control_steps >= self.max_control_steps) or (voltage_violations == 0) for agent_id in self._agent_ids}\n",
    "        terminations['__all__'] = any(terminations.values())\n",
    "\n",
    "    # Assuming no truncation; modify as needed\n",
    "        truncateds = {agent_id: False for agent_id in self._agent_ids}\n",
    "        truncateds['__all__'] = any(truncateds.values())\n",
    "\n",
    "    # Gather additional information if required\n",
    "        infos = {agent_id: {} for agent_id in self._agent_ids}\n",
    "\n",
    "    # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        return observations, rewards, terminations, truncateds, infos\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "    # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    # You can use options to customize the reset function if needed\n",
    "    # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        \n",
    "        #self.current_step = int(np.clip(np.random.normal(loc=0, scale=8760), 0, 8760))\n",
    "        self.current_step = np.random.randint(0, 8761)\n",
    "       \n",
    "\n",
    " # assuming the profile has 8760 hours  #use gaussian distribution\n",
    "        self.control_steps = 0\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 8760], scale=0.5, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 8760]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 8760]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "        \n",
    "        # Get the observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "    # You can include any additional information here. If there's nothing, just return an empty dictionary.\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self, agent_id):\n",
    "    # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "    \n",
    "    # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "    \n",
    "        return all_bus_voltages\n",
    "\n",
    "    def get_agent_observation(self):\n",
    "        agent_obs = {}\n",
    "        for agent_id in self.agents:\n",
    "            agent_obs[agent_id] = self.get_observation(agent_id)  # Note: We're still passing agent_id even if it's unused. This keeps the interface consistent.\n",
    "        return agent_obs\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "\n",
    "    \n",
    "\n",
    "irradiance_csv_file = r\"D:\\Alaa_Selim\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "load_profile_file = r\"D:\\Alaa_Selim\\LoadShape1.csv\"\n",
    "\n",
    "    \n",
    "    \n",
    "original_env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "env = MultiAgentEnvCompatibility(original_env)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PowerSystemEnv(\n",
    "                            dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "                            dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "                            irradiance_csv_file=irradiance_csv_file,\n",
    "                            load_profile_file=load_profile_file\n",
    "                            )\n",
    "\n",
    "register_env(\"PowerSystemEnv\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "# Suppose you have N agents\n",
    "N = 5  # adjust this as per your actual number of agents\n",
    "\n",
    "policies = {\n",
    "    f\"policy_{i}\": (None, \n",
    "                    original_env.observation_space_dict[f\"agent_{i}\"], \n",
    "                    original_env.action_space_dict[f\"agent_{i}\"], \n",
    "                    {\"agent_id\": i})  # Set 'agent_id' key here\n",
    "    for i in range(N)\n",
    "}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    if \"agent_\" in agent_id:\n",
    "        # Extract agent number and return the corresponding policy\n",
    "        agent_num = int(agent_id.split(\"_\")[1])\n",
    "        return f\"policy_{agent_num}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import csv\n",
    "from py_dss_interface import DSSDLL\n",
    "import pandas as pd\n",
    "\n",
    "# defining values and lengths as before\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "class CentralizedPowerSystemEnv(gym.Env):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(CentralizedPowerSystemEnv, self).__init__()\n",
    "\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses  = ['50', '49', '51', '47', '48', '44', '57', '54', '64', '63', '55', '62', '56', '65', '43', '60', '42', '52', '53', '61', '66', '36', '40', '67', '72', '86', '87', '76', '89', '97']\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "        self.previous_reward = 0.0\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=100 Pmpp=80\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=100 kVAR=0\")\n",
    "\n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "\n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "\n",
    "        total_action_shape = PV_KVAR_ACTION_LEN + PV_KW_ACTION_LEN + BESS_KW_ACTION_LEN\n",
    "        self.action_space = spaces.Box(low=-100, high=100, shape=(total_action_shape,), dtype=np.float32)\n",
    "\n",
    "        total_observation_shape = 278  # Assuming observation space is the voltage at each bus\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(total_observation_shape,), dtype=np.float32)\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.control_steps = 0\n",
    "        self.max_control_steps = 2\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        TOTAL_ACTIONS_PER_AGENT = 6 + 6 + 6\n",
    "        agent_actions = np.split(action, 5)\n",
    "        \n",
    "        for agent_idx, agent_action in enumerate(agent_actions):\n",
    "            pv_kw_actions = agent_action[:6]\n",
    "            pv_kvar_actions = agent_action[6:12]\n",
    "            bess_kw_actions = agent_action[12:18]\n",
    "\n",
    "            pv_idx_offset = agent_idx * 6\n",
    "            battery_idx_offset = agent_idx * 6\n",
    "\n",
    "            for idx, (kw_action_value, kvar_action_value) in enumerate(zip(pv_kw_actions, pv_kvar_actions)):\n",
    "                irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "                scaled_pv_kw = kw_action_value * irradiance\n",
    "                if scaled_pv_kw > irradiance * self.KWrated:\n",
    "                    scaled_pv_kw = irradiance * self.KWrated\n",
    "\n",
    "                pv_kvar = kvar_action_value\n",
    "                S_max = 100\n",
    "                q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "                pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "                self.controller.text(f\"edit PVSystem.PV{idx + pv_idx_offset + 1} phases=3 kV=4.16 Pmpp={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "\n",
    "            for idx, action_value in enumerate(bess_kw_actions):\n",
    "                self.controller.text(f\"edit Storage.Battery{idx + battery_idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute the action\n",
    "        self._take_action(action)\n",
    "\n",
    "        # Calculate the rewards\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages if v <= 0.95 or v >= 1.05)\n",
    "        voltage_deviations = sum((v - 1)**2 for v in all_bus_voltages)\n",
    "\n",
    "        # Define penalty weights for different components\n",
    "        w_deviation = 1.0\n",
    "        w_violation = 5.0\n",
    "        w_loss = 0.001\n",
    "\n",
    "        reward = - w_deviation * voltage_deviations - w_violation * voltage_violations - w_loss * losses\n",
    "        done = self.control_steps >= self.max_control_steps or voltage_violations == 0\n",
    "            # Set termination status for all agents\n",
    "        terminations = done\n",
    "    # Assuming no truncation; modify as needed\n",
    "        truncateds = False\n",
    "\n",
    "    # Gather additional information if required\n",
    "\n",
    "    # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "\n",
    "   \n",
    "        observations = self.get_central_observation()\n",
    "\n",
    "        # Assuming no additional info for now.\n",
    "        info = {}\n",
    "\n",
    "        return observations, reward, terminations, truncateds, info\n",
    "        \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # You can use options to customize the reset function if needed\n",
    "        # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.current_step = np.random.randint(0, 8761)\n",
    "        self.control_steps = 0\n",
    "\n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 8760], scale=0.5, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 8760]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 8760]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "        observation = self.get_central_observation()\n",
    "        return observation, {}  # Adding the additional reset info (empty dictionary)\n",
    "\n",
    "    def get_central_observation(self):\n",
    "        # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "\n",
    "        # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "\n",
    "        return all_bus_voltages\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fca2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Assuming your environment class is loaded and properly set up.\n",
    "env = DummyVecEnv([lambda: CentralizedPowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")])\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=50000)\n",
    "model.learn(total_timesteps=10000)\n",
    "model.save(\"sac_centralized_power_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import csv\n",
    "from py_dss_interface import DSSDLL\n",
    "import pandas as pd\n",
    "\n",
    "# defining values and lengths as before\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "class CentralizedPowerSystemEnv(gym.Env):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(CentralizedPowerSystemEnv, self).__init__()\n",
    "\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses  = ['50', '49', '51', '47', '48', '44', '57', '54', '64', '63', '55', '62', '56', '65', '43', '60', '42', '52', '53', '61', '66', '36', '40', '67', '72', '86', '87', '76', '89', '97']\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "        self.previous_reward = 0.0\n",
    "        self.alpha = 0.1\n",
    "        self.current_step = 0\n",
    "        self.control_steps = 0\n",
    "        self.max_control_steps = 2\n",
    "        self.num_agents = 5\n",
    "        self.agent_observation_lens = [50, 50, 50, 50, 50]\n",
    "\n",
    "        # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=1000 Pmpp=800\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=1000 kVAR=0\")\n",
    "\n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "\n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "\n",
    "        total_action_shape = PV_KVAR_ACTION_LEN + PV_KW_ACTION_LEN + BESS_KW_ACTION_LEN\n",
    "        #self.action_space = spaces.Box(low=-100, high=100, shape=(total_action_shape,), dtype=np.float32)\n",
    "\n",
    "        total_observation_shape = 283  # Assuming observation space is the voltage at each bus\n",
    "       # Modify the action and observation spaces to have consistent dimensions for each agent\n",
    "        # Combine all agent action spaces into one flat space\n",
    "        total_action_shape = sum([18 for _ in range(self.num_agents)])\n",
    "        self.action_space = spaces.Box(low=-100, high=100, shape=(total_action_shape,), dtype=np.float32)\n",
    "        \n",
    "        # Combine all agent observation spaces into one flat space\n",
    "        total_observation_shape = sum(self.agent_observation_lens)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(total_observation_shape,), dtype=np.float32)\n",
    "\n",
    "    def local_reward(self, agent_idx, voltages):\n",
    "        # Define the start and end index for each agent's subset of voltages\n",
    "        start_idx = sum(self.agent_observation_lens[:agent_idx])\n",
    "        end_idx = start_idx + self.agent_observation_lens[agent_idx]\n",
    "\n",
    "        # Extract the subset of voltages for the agent\n",
    "        agent_voltages = voltages[start_idx:end_idx]\n",
    "\n",
    "        # Calculate the number of voltage violations for the agent's subset of voltages\n",
    "        voltage_violations = sum(1 for v in agent_voltages if v <= 0.95 or v >= 1.05)\n",
    "\n",
    "        # Calculate the agent's reward based on voltage violations\n",
    "        # We use a negative reward for violations, so agents are encouraged to reduce them.\n",
    "        w_violation = -5.0\n",
    "        agent_reward = w_violation * voltage_violations\n",
    "\n",
    "        return agent_reward\n",
    "    \n",
    "    \n",
    "    def get_agent_observation(self, agent_idx, voltages):\n",
    "        start_idx = sum(self.agent_observation_lens[:agent_idx])\n",
    "        end_idx = start_idx + self.agent_observation_lens[agent_idx]\n",
    "        local_obs = voltages[start_idx:end_idx]\n",
    "        return local_obs\n",
    "\n",
    "\n",
    "\n",
    "    def _take_action_for_agent(self, agent_idx, action):\n",
    "        # Define total actions per agent\n",
    "        TOTAL_ACTIONS_PER_AGENT = 6 + 6 + 6\n",
    "\n",
    "        # Index offset based on agent_idx for PV and battery control\n",
    "        pv_idx_offset = agent_idx * 6\n",
    "        battery_idx_offset = agent_idx * 6\n",
    "\n",
    "        # Decompose the action for this agent\n",
    "        pv_kw_actions = action[:6]\n",
    "        pv_kvar_actions = action[6:12]\n",
    "        bess_kw_actions = action[12:18]\n",
    "\n",
    "        # Apply PV actions\n",
    "        for idx, (kw_action_value, kvar_action_value) in enumerate(zip(pv_kw_actions, pv_kvar_actions)):\n",
    "            # handle kW actions\n",
    "            irradiance = self.irradiance_profile[self.current_step % 10]\n",
    "            scaled_pv_kw = kw_action_value * irradiance\n",
    "            if scaled_pv_kw > irradiance * self.KWrated:  # clip to max\n",
    "                scaled_pv_kw = irradiance * self.KWrated\n",
    "\n",
    "            # handle kVAR actions\n",
    "            pv_kvar = kvar_action_value\n",
    "            S_max = 100  # Maximum apparent power (example value)\n",
    "            q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "            pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "            self.controller.text(f\"edit PVSystem.PV{idx + pv_idx_offset + 1} phases=3 kV=4.16 kW={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "\n",
    "        # Apply battery actions\n",
    "        for idx, action_value in enumerate(bess_kw_actions):\n",
    "            self.controller.text(f\"edit Storage.Battery{idx + battery_idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Each agent takes its action\n",
    "        for agent_idx, action in enumerate(actions):\n",
    "            self._take_action_for_agent(agent_idx, action)\n",
    "\n",
    "        # Calculate the global rewards and other metrics\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "\n",
    "        # Calculate local rewards\n",
    "        rewards = [self.local_reward(i, all_bus_voltages) for i in range(self.num_agents)]\n",
    "\n",
    "        # Check if episode should terminate\n",
    "        done = self.control_steps >= self.max_control_steps\n",
    "        terminations = [done] * self.num_agents\n",
    "        truncateds = False\n",
    "\n",
    "        # Update control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        # Get observations for each agent\n",
    "        observations = [self.get_agent_observation(i, all_bus_voltages) for i in range(self.num_agents)]\n",
    "        print(\"Observations after actions:\", observations)\n",
    "        print(\"Observations Shape:\", np.array(observations).shape)\n",
    "\n",
    "        # Prepare empty info dicts for each agent\n",
    "        infos = [{} for _ in range(self.num_agents)]\n",
    "\n",
    "        return observations, rewards, terminations, truncateds, infos\n",
    "\n",
    "        \n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # You can use options to customize the reset function if needed\n",
    "        # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.current_step = np.random.randint(0, 11)\n",
    "        self.control_steps = 0\n",
    "\n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 10], scale=0.5, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 10]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 10]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 10]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "        # Aggregate the observations\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "\n",
    "        # Get individual observations for each agent\n",
    "        all_observations = [self.get_agent_observation(i, all_bus_voltages) for i in range(self.num_agents)]\n",
    "\n",
    "        # Convert the list of observations into a 2D numpy array\n",
    "        obs_array = np.stack(all_observations)\n",
    "\n",
    "        return obs_array, {} \n",
    "\n",
    "    def get_central_observation(self):\n",
    "        # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "\n",
    "        # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "\n",
    "        return all_bus_voltages\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8403b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Convert your environment to vectorized format\n",
    "vec_env = DummyVecEnv([lambda: CentralizedPowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")])\n",
    "# Initialize SAC agents\n",
    "agents = [SAC(\"MlpPolicy\", vec_env, verbose=1) for _ in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import load_results\n",
    "from stable_baselines3.common.results_plotter import ts2xy\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "def make_env(dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "    def _init():\n",
    "        return CentralizedPowerSystemEnv(\n",
    "            dss_path=dss_path,\n",
    "            dss_file=dss_file,\n",
    "            irradiance_csv_file=irradiance_csv_file,\n",
    "            load_profile_file=load_profile_file\n",
    "        )\n",
    "    return _init\n",
    "\n",
    "# Paths for your setup\n",
    "dss_path = r\"C:\\Program Files\\OpenDSS\"\n",
    "dss_file = r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\"\n",
    "irradiance_csv_file = r\"D:\\Alaa_Selim\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "load_profile_file = r\"D:\\Alaa_Selim\\LoadShape1.csv\"\n",
    "\n",
    "# Create a single environment (since we are training sequentially)\n",
    "env = make_env(dss_path, dss_file, irradiance_csv_file, load_profile_file)()\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "vec_env = VecNormalize(vec_env)\n",
    "\n",
    "# Initialize SAC agents for the environment\n",
    "num_agents = 5\n",
    "agents = [SAC(\"MlpPolicy\", vec_env, verbose=1) for _ in range(num_agents)]\n",
    "\n",
    "# Train each agent sequentially\n",
    "total_timesteps = 10000  # Example value, adjust as needed\n",
    "for agent in agents:\n",
    "    agent.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "# Save the agents\n",
    "model_save_path = \"models/\"\n",
    "for idx, agent in enumerate(agents):\n",
    "    agent.save(f\"{model_save_path}agent_{idx}\")\n",
    "\n",
    "# Plot learning curves\n",
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "def plot_results():\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    \n",
    "    for idx in range(num_agents):\n",
    "        results = load_results(f\"{model_save_path}agent_{idx}\")\n",
    "        x, y = ts2xy(results, 'timesteps')\n",
    "        y = moving_average(y, window=50)  # Window of 50 for smoothing\n",
    "        ax.plot(x[:-49], y, label=f\"Agent_{idx}\")\n",
    "    \n",
    "    ax.set_xlabel(\"Timesteps\")\n",
    "    ax.set_ylabel(\"Rewards\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "    plt.savefig('SAC_Learning.pdf', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "plot_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd78498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int):\n",
    "        super(RewardLoggerCallback, self).__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.rewards = []\n",
    "        self.mean_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Get the episode rewards from the environment\n",
    "            episode_rewards = self.locals.get('episode_rewards', [])\n",
    "            self.rewards.extend(episode_rewards)\n",
    "            \n",
    "            if len(self.rewards) > 0:\n",
    "                mean_reward = np.mean(self.rewards[-100:])  # Mean of last 100 rewards\n",
    "                self.mean_rewards.append(mean_reward)\n",
    "                print(f\"Mean reward over last 100 episodes: {mean_reward}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        plt.plot(self.mean_rewards)\n",
    "        plt.ylabel('Mean reward (last 100 episodes)')\n",
    "        plt.xlabel('Number of timesteps')\n",
    "        plt.title('Training progress')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pandas as pd\n",
    "\n",
    "log_dir = './logs/'\n",
    "eval_env = DummyVecEnv([lambda: CentralizedPowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")])\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=log_dir, log_path=log_dir, eval_freq=1000)\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=50000)\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"sac_centralized_power_system\")\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./logs/'):\n",
    "    os.makedirs('./logs/')\n",
    "\n",
    "# Load the results using pandas\n",
    "results = pd.read_csv(log_dir + 'evaluations.csv')\n",
    "mean_rewards = results['mean_reward']\n",
    "\n",
    "# Plot the mean rewards\n",
    "plt.plot(mean_rewards)\n",
    "plt.ylabel('Mean reward')\n",
    "plt.xlabel('Number of timesteps')\n",
    "plt.title('Training progress')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e58302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC, TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# First, wrap the custom environment with Monitor\n",
    "monitored_env = Monitor(CentralizedPowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    "), './models/')\n",
    "\n",
    "# Then, wrap the monitored environment with DummyVecEnv\n",
    "env = DummyVecEnv([lambda: monitored_env])\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./models/')\n",
    "\n",
    "# Training configurations\n",
    "num_timesteps = 100000\n",
    "agents = {\n",
    "    \"SAC\": SAC(\"MlpPolicy\", env, verbose=1),\n",
    "}\n",
    "\n",
    "# Train each agent and store results\n",
    "results = {}\n",
    "for name, agent in agents.items():\n",
    "    print(f\"\\nTraining {name}...\\n\")\n",
    "    agent.learn(total_timesteps=num_timesteps, callback=checkpoint_callback)\n",
    "    results[name] = load_results('./models/')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total optimization time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Plot learning curves\n",
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "def plot_results():\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        x, y = ts2xy(result, 'timesteps')\n",
    "        y = moving_average(y, window=50)  # Window of 50 for smoothing\n",
    "        ax.plot(x[:-49], y, label=name)\n",
    "    \n",
    "    ax.set_xlabel(\"Timesteps\")\n",
    "    ax.set_ylabel(\"Rewards\")\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "    plt.savefig('SAC_Learning.pdf', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "plot_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "\n",
    "# Assuming your environment class CentralizedPowerSystemEnv is defined and properly set up as given in the snippets\n",
    "\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int):\n",
    "        super(RewardLoggerCallback, self).__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.rewards = []\n",
    "        self.mean_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            print(\"RewardLoggerCallback triggered!\")  # Debug print\n",
    "            episode_rewards = self.locals.get('episode_rewards', [])\n",
    "            self.rewards.extend(episode_rewards)\n",
    "            \n",
    "            if len(self.rewards) > 0:\n",
    "                mean_reward = np.mean(self.rewards[-100:])  \n",
    "                self.mean_rewards.append(mean_reward)\n",
    "                print(f\"Mean reward over last 100 episodes: {mean_reward}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "# Set up the environment\n",
    "env = DummyVecEnv([lambda: CentralizedPowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")])\n",
    "\n",
    "reward_logger = RewardLoggerCallback(check_freq=1000)\n",
    "\n",
    "# Ensure the logging directory exists\n",
    "log_dir = './logs/'\n",
    "if not os.path.exists(log_dir):\n",
    "    print(\"Creating logging directory...\")  # Debug print\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Set up the EvalCallback\n",
    "eval_callback = EvalCallback(env, log_path=log_dir, eval_freq=5000)\n",
    "\n",
    "# Train the model\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=50000)\n",
    "print(\"Starting model training...\")  # Debug print\n",
    "model.learn(total_timesteps=10000, callback=[reward_logger, eval_callback])\n",
    "print(\"Training completed!\")  # Debug print\n",
    "\n",
    "# Save the model\n",
    "model.save(\"sac_centralized_power_system\")\n",
    "\n",
    "# Plot rewards using RewardLoggerCallback data\n",
    "print(\"Plotting rewards using RewardLoggerCallback data...\")  # Debug print\n",
    "plt.plot(reward_logger.mean_rewards)\n",
    "plt.ylabel('Mean reward (last 100 episodes)')\n",
    "plt.xlabel('Number of timesteps')\n",
    "plt.title('Training progress using RewardLogger')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot rewards using evaluations.csv (from EvalCallback)\n",
    "print(\"Attempting to plot rewards using evaluations.csv...\")  # Debug print\n",
    "try:\n",
    "    results = pd.read_csv(log_dir + 'evaluations.csv')\n",
    "    mean_rewards = results['mean_reward']\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.ylabel('Mean reward')\n",
    "    plt.xlabel('Number of timesteps')\n",
    "    plt.title('Training progress using EvalCallback')\n",
    "    plt.show()\n",
    "except FileNotFoundError:\n",
    "    print(\"evaluations.csv not found. Perhaps the EvalCallback didn't execute or finish logging.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('./logs/evaluations.csv'):\n",
    "    print(\"The file exists!\")\n",
    "else:\n",
    "    print(\"The file does not exist!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def decentralized_execution(env, model):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    all_agent_actions = [[] for _ in range(env.num_agents)]  # Store actions for plotting later\n",
    "    \n",
    "    while not done:\n",
    "        central_action = model.predict(obs, deterministic=True)[0]\n",
    "        \n",
    "        # Split central action into individual agent actions\n",
    "        agent_actions = np.split(central_action, env.num_agents)\n",
    "        \n",
    "        # Store each agent's actions for plotting\n",
    "        for idx, agent_action in enumerate(agent_actions):\n",
    "            all_agent_actions[idx].append(agent_action)\n",
    "            \n",
    "        obs, reward, done, _ = env.step(central_action)\n",
    "        env.render()  # If you have a render method\n",
    "\n",
    "    # Plot the actions for each agent\n",
    "    for idx, agent_actions in enumerate(all_agent_actions):\n",
    "        agent_actions = np.array(agent_actions)  # Convert to numpy array for easier plotting\n",
    "        plt.figure()\n",
    "        \n",
    "        # Assuming the sum of actions is a meaningful metric for your case\n",
    "        # Adjust this as needed\n",
    "        summed_actions = agent_actions.sum(axis=1)\n",
    "        \n",
    "        plt.plot(summed_actions)\n",
    "        plt.title(f\"Agent {idx+1} Actions over Time\")\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(f\"Summed Actions of Agent {idx+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# Load trained model as before\n",
    "loaded_model = SAC.load(\"sac_centralized_power_system\")\n",
    "\n",
    "# Perform decentralized execution with the above plots\n",
    "decentralized_execution(env, loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf587bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install learn2learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Habitat-Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from py_dss_interface import DSSDLL\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "\n",
    "class CentralizedPowerSystemEnv(ParallelEnv):\n",
    "    metadata = {'render.modes': ['human'], 'is_parallelizable': True,}\n",
    "\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(CentralizedPowerSystemEnv, self).__init__()\n",
    "\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses  = [\n",
    "                                '1', '7', '8', '13', '21', '23', '29', '250', '35', '40', '42', '55',\n",
    "                                '56', '65', '76', '78', '66', '79', '81', '83', '91', '95', '100',\n",
    "                                '197', '300', '110', '135', '160', '152', '610'\n",
    "                            ]\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "        self.previous_reward = 0.0\n",
    "        self.alpha = 0.1\n",
    "        self.current_step = 0\n",
    "        self.control_steps = 0\n",
    "        self.max_control_steps = 10\n",
    "        self.n_agents = 5\n",
    "        self.agent_observation_lens = [56, 56, 56, 55,55]\n",
    "\n",
    "        # Distribute the extra 28 observations\n",
    "        baseline_extra = 28 // 5  # = 5\n",
    "        remainder = 28 % 5  # = 3\n",
    "\n",
    "        # Add baseline extra observations to all agents\n",
    "        self.agent_observation_lens = [x + baseline_extra for x in self.agent_observation_lens]\n",
    "\n",
    "        # Distribute the remainder to the first few agents\n",
    "        for i in range(remainder):\n",
    "            self.agent_observation_lens[i] += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=1000 Pmpp=800\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=1000 kVAR=0\")\n",
    "\n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "\n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "        \n",
    "        self.agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "        self.possible_agents = self.agents.copy()\n",
    "        self.agent_name_to_agent_num = {name: num for num, name in enumerate(self.agents)}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0.0 for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Setting up for PettingZoo's agent iteration\n",
    "        #self._agent_selector = agent_selector(self.agents)\n",
    "\n",
    "        # Agent spaces definition\n",
    "        # Observations and Action spaces\n",
    "        low_bound = np.zeros(18)\n",
    "        high_bound = np.ones(18) * 1000\n",
    "\n",
    "# Modify the bounds for indices from 12 to 18\n",
    "        low_bound[6:12] = -1000\n",
    "        high_bound[6:12] = 1000\n",
    "\n",
    "        self.action_spaces = {agent: spaces.Box(low=low_bound, high=high_bound, dtype=np.float32) for agent in self.agents}\n",
    "        self.observation_spaces = {agent: spaces.Box(low=-np.inf, high=np.inf, shape=(self.agent_observation_lens[idx],), dtype=np.float32) for idx, agent in enumerate(self.agents)}\n",
    "    def local_reward(self, agent_idx, voltages):\n",
    "        # Define the start and end index for each agent's subset of voltages\n",
    "        start_idx = sum(self.agent_observation_lens[:agent_idx])\n",
    "        end_idx = start_idx + self.agent_observation_lens[agent_idx]\n",
    "\n",
    "        # Extract the subset of voltages for the agent\n",
    "        agent_voltages = voltages[start_idx:end_idx]\n",
    "\n",
    "        # Calculate the number of voltage violations for the agent's subset of voltages\n",
    "        voltage_violations = sum(1 for v in agent_voltages if v <= 0.95 or v >= 1.05)\n",
    "\n",
    "        # Calculate the agent's reward based on voltage violations\n",
    "        # We use a negative reward for violations, so agents are encouraged to reduce them.\n",
    "        w_violation = -5.0\n",
    "        agent_reward = w_violation * voltage_violations\n",
    "\n",
    "        return agent_reward\n",
    "    \n",
    "    \n",
    "    def get_agent_observation(self, agent_idx, voltages):\n",
    "        start_idx = sum(self.agent_observation_lens[:agent_idx])\n",
    "        end_idx = start_idx + self.agent_observation_lens[agent_idx]\n",
    "        local_obs = voltages[start_idx:end_idx]\n",
    "        return local_obs\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # You can use options to customize the reset function if needed\n",
    "        # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.current_step = np.random.randint(0, 11)\n",
    "        self.control_steps = 0\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step %10], scale=0.5, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 10]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 10]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 10]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "        # Aggregate the observations\n",
    "        all_bus_voltages = np.clip(self.controller.circuit_all_bus_vmag_pu(), 1e-10, 10.0)\n",
    "\n",
    "        # Get individual observations for each agent\n",
    "        all_observations = [self.get_agent_observation(i, all_bus_voltages) for i in range(self.n_agents)]\n",
    "\n",
    "        # Convert the list of observations into a 2D numpy array\n",
    "        #obs_array = np.stack(all_observations)\n",
    "\n",
    "        self.agents = self.possible_agents.copy()\n",
    "        #self._agent_selector.reinit(self.agents)\n",
    "        #self.agent_selection = self._agent_selector.next()\n",
    "        self._cumulative_rewards = {agent: 0.0 for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        observations = self.get_obs()\n",
    "        \n",
    "       # if return_info:\n",
    "           # return self.get_obs(), self.get_infos()\n",
    "       # else:\n",
    "           # return self.get_obs()\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_obs(), self.get_infos()\n",
    "    \n",
    "    \n",
    "    def _take_action_for_agent(self, agent_idx, action):\n",
    "        # Define total actions per agent\n",
    "        TOTAL_ACTIONS_PER_AGENT = 6 + 6 + 6\n",
    "\n",
    "        # Index offset based on agent_idx for PV and battery control\n",
    "        pv_idx_offset = agent_idx * 6\n",
    "        battery_idx_offset = agent_idx * 6\n",
    "\n",
    "        # Decompose the action for this agent\n",
    "        pv_kw_actions = action[:6]\n",
    "        pv_kvar_actions = action[6:12]\n",
    "        bess_kw_actions = action[12:18]\n",
    "\n",
    "        # Apply PV actions\n",
    "        for idx, (kw_action_value, kvar_action_value) in enumerate(zip(pv_kw_actions, pv_kvar_actions)):\n",
    "            # handle kW actions\n",
    "            irradiance = self.irradiance_profile[self.current_step % 10]\n",
    "            scaled_pv_kw = kw_action_value * irradiance\n",
    "            if scaled_pv_kw > irradiance * self.KWrated:  # clip to max\n",
    "                scaled_pv_kw = irradiance * self.KWrated\n",
    "\n",
    "            # handle kVAR actions\n",
    "            pv_kvar = kvar_action_value\n",
    "            S_max = 100  # Maximum apparent power (example value)\n",
    "            q_max1 = np.sqrt(S_max**2 - np.power(scaled_pv_kw, 2))\n",
    "                        # Set the reactive power based on the maximum possible Q and the desired Q\n",
    "            if scaled_pv_kw == 0:  # If active power is zero\n",
    "                pv_kvar = 0\n",
    "            else:\n",
    "                pv_kvar = np.clip(kvar_action_value, -q_max1, q_max1)  # Ensure desired Q is within feasible limits\n",
    "            self.controller.text(f\"edit PVSystem.PV{idx + pv_idx_offset + 1} phases=3 kV=4.16 kW={scaled_pv_kw} kVAR={pv_kvar}\")\n",
    "            action_str = f\"Agent agent_{agent_idx} sets PVSystem.PV{idx + pv_idx_offset + 1} to kW={scaled_pv_kw} kVAR={pv_kvar}\"\n",
    "            print(action_str)\n",
    "        # Apply battery actions\n",
    "        for idx, action_value in enumerate(bess_kw_actions):\n",
    "            self.controller.text(f\"edit Storage.Battery{idx + battery_idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\")\n",
    "            action_str = f\"Agent agent_{agent_idx} sets Storage.Battery{idx + battery_idx_offset + 1} to kW={action_value}\"\n",
    "            print(action_str)\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "        print(f\"Agent {agent_idx} Action: {action}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Apply actions for all agents\n",
    "        for agent_name, action in actions.items():\n",
    "            agent_idx = self.agent_name_to_agent_num[agent_name]\n",
    "            self._take_action_for_agent(agent_idx, action)\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "        all_bus_voltages = np.clip(self.controller.circuit_all_bus_vmag_pu(), 1e-10, 10.0)\n",
    "        rewards = {agent: self.local_reward(self.agent_name_to_agent_num[agent], all_bus_voltages) for agent in self.agents}\n",
    "        if rewards ==0:\n",
    "            rewards+=1000\n",
    "        self.control_steps += 1\n",
    "        done = self.control_steps >= 10\n",
    "        #terminations = {agent: done for agent in self.agents}\n",
    "        #truncateds = {agent: False for agent in self.agents}\n",
    "        terminations = done \n",
    "        truncateds = False \n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        # Observations for all agents\n",
    "        obs = {agent: self.observe(agent) for agent in self.agents}\n",
    "        for agent in self.agents:\n",
    "            observation = obs[agent]\n",
    "            print(f\"Agent {agent} Observation:\", observation)\n",
    "            print(f\"Length of {agent}'s Observation:\", len(observation))\n",
    "\n",
    "\n",
    "\n",
    "        return obs, rewards, terminations, truncateds, infos\n",
    "\n",
    "    def get_obs(self):\n",
    "        all_bus_voltages = np.clip(self.controller.circuit_all_bus_vmag_pu(), 1e-10, 10.0)\n",
    "        obs = {agent: np.array(self.get_agent_observation(self.agent_name_to_agent_num[agent], all_bus_voltages)) for agent in self.agents}\n",
    "        #print(f\"Agent {agent} Observation: {obs[agent]}\")\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def get_rewards(self):\n",
    "        all_bus_voltages = np.clip(self.controller.circuit_all_bus_vmag_pu(), 1e-10, 10.0)\n",
    "        rewards = {agent: self.local_reward(self.agent_name_to_agent_num[agent], all_bus_voltages) for agent in self.agents}\n",
    "        return rewards\n",
    "\n",
    "    def get_dones(self):\n",
    "        done = self.control_steps >= 10\n",
    "        return {agent: done for agent in self.agents}\n",
    "\n",
    "    def get_infos(self):\n",
    "        return {agent: {} for agent in self.agents}\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Stub for now. Later, you can extend it to visualize the power system, if necessary.\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        # You might want to close the controller or perform cleanup here in the future.\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        # For reproducibility. You might want to seed other random processes, if any.\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    def observe(self, agent):\n",
    "        all_bus_voltages = np.clip(self.controller.circuit_all_bus_vmag_pu(), 1e-10, 10.0)\n",
    "        return np.array(self.get_agent_observation(self.agent_name_to_agent_num[agent], all_bus_voltages))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "# Reset the environment\n",
    "observations = env.reset()\n",
    "\n",
    "# Take random actions for each agent for 5 steps\n",
    "for _ in range(5):\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "    obs, rewards, terminations, truncateds, infos = env.step(actions)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bus_voltages = dss.circuit_all_bus_vmag_pu()\n",
    "print(all_bus_voltages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'dss.circuit_all_node_names(): {dss.circuit_all_node_names()}')\n",
    "z= dss.circuit_all_node_names()\n",
    "print(len(dss.circuit_all_node_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "#env = MultiAgentEnvCompatibility(CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file))\n",
    "\n",
    "\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "\n",
    "env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "converted_env = PettingZooEnv(env)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return converted_env(\n",
    "                            dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "                            dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "                            irradiance_csv_file=irradiance_csv_file,\n",
    "                            load_profile_file=load_profile_file\n",
    "                            )\n",
    "\n",
    "register_env(\"CentralizedPowerSystemEnv\", env_creator)\n",
    "# Suppose you have N agents\n",
    "N = 5  # adjust this as per your actual number of agents\n",
    "\n",
    "policies = {\n",
    "    f\"policy_{i}\": (None, \n",
    "                    original_env.observation_space_dict[f\"agent_{i}\"], \n",
    "                    original_env.action_space_dict[f\"agent_{i}\"], \n",
    "                    {\"agent_id\": i})  # Set 'agent_id' key here\n",
    "    for i in range(N)\n",
    "}\n",
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    if \"agent_\" in agent_id:\n",
    "        # Extract agent number and return the corresponding policy\n",
    "        agent_num = int(agent_id.split(\"_\")[1])\n",
    "        return f\"policy_{agent_num}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c87bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from pettingzoo.utils.wrappers import pad_observations_v0, pad_action_space_v0\n",
    "\n",
    "def env_creator(_):\n",
    "    env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "    env = pad_observations_v0(env)\n",
    "    env = pad_action_space_v0(env)\n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "register_env(\"CentralizedPowerSystemEnv\", env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 300000},\n",
    "    config={\n",
    "        \"env\": \"CentralizedPowerSystemEnv\",\n",
    "        \"environment\": {\n",
    "            \"disable_env_checking\": True\n",
    "        },\n",
    "        \"multiagent\": {\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": policy_mapping_fn\n",
    "        },\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 19,\n",
    "        \"lr\": 3e-3,\n",
    "    },\n",
    "    local_dir=\"SAC-Test1\",\n",
    "    # callbacks=[eval_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install supergym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ae0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supersuit import pad_observations_v0, pad_action_space_v0\n",
    "from supersuit import pettingzoo_env_to_vec_env_v1\n",
    "\n",
    "env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "\n",
    "# Pad the observations to ensure they're all the same size\n",
    "# Ensure all agents have consistent observation and action spaces\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "\n",
    "# Convert to a vectorized gym environment\n",
    "gym_env = pettingzoo_env_to_vec_env_v1(padded_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cebfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SB3 expects vectorized environments, so wrap it with DummyVecEnv\n",
    "vec_env = DummyVecEnv([lambda: gym_env])\n",
    "\n",
    "# Initialize and train the PPO agent\n",
    "model = SAC(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=100000)  # Adjust timesteps as needed\n",
    "\n",
    "# Save the model\n",
    "model.save(\"power_system_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97476d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from pettingzoo.utils.conversions import from_parallel\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from pettingzoo.utils import aec_to_parallel\n",
    "\n",
    "# Your custom environment\n",
    "env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "\n",
    "# Convert your environment to parallel\n",
    "parallel_env = aec_to_parallel(env)\n",
    "\n",
    "# Make a function that produces environments\n",
    "def make_env():\n",
    "    env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "    env = aec_to_parallel(env)\n",
    "    env = from_parallel(env)  # This converts it to the format usable by Stable Baselines3\n",
    "    return env\n",
    "\n",
    "# Vectorize the environment\n",
    "vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "# If your observations are images (e.g., Atari games), Stable Baselines3 expects the channels to come first.\n",
    "# If not, you can ignore this line.\n",
    "# vec_env = VecTransposeImage(vec_env)\n",
    "\n",
    "# Initialize and train a PPO agent\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate the trained model\n",
    "obs = vec_env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render()\n",
    "\n",
    "vec_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33833306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils.conversions import from_parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517becd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Check the observation\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "# Manually step through the environment for a number of steps\n",
    "for step in range(5):\n",
    "    for agent in env.agents:\n",
    "        env.agent_selection = agent\n",
    "        random_action = env.action_space(agent).sample()  # Taking random action\n",
    "        obs, reward, done, info = env.step(random_action)\n",
    "        print(f\"Step {step + 1} - Agent {agent}: Reward = {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67503aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = env.step(random_action)\n",
    "print(result)\n",
    "print(type(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pettingzoo[test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def env_creator(env_config):\n",
    "    return  CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "\n",
    "\n",
    "register_env(\"CentralizedPowerSystemEnv\", env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "\n",
    "def env_creator(config):\n",
    "    # You might want to extract some parameters from the config here if you plan to parametrize your env\n",
    "    return CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "\n",
    "register_env('CentralizedPowerSystem', lambda config: PettingZooEnv(env_creator(config)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d09c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env import PettingZooEnv\n",
    "\n",
    "def env_creator(config):\n",
    "    raw_env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "    # Make sure to wrap your environment with the PettingZoo wrapper from RLlib\n",
    "    env = PettingZooEnv(raw_env)\n",
    "    return env\n",
    "\n",
    "register_env('CentralizedPowerSystem', env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "\n",
    "def env_creator(_):\n",
    "    env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "    env = pad_observations_v0(env)\n",
    "    env = pad_action_space_v0(env)\n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "register_env(\"CentralizedPowerSystemEnv\", env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a49594",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\",\n",
    "    stop={\"episodes_total\": 300000},\n",
    "    config={\n",
    "        \"env\": \"CentralizedPowerSystemEnv\",\n",
    "        \"environment\": {\n",
    "            \"disable_env_checking\": True\n",
    "        },\n",
    "        \n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 19,\n",
    "        \"lr\": 3e-3,\n",
    "    },\n",
    "    local_dir=\"SAC-Test\",\n",
    "    # callbacks=[eval_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install supersuit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f38332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supersuit import pettingzoo_env_to_vec_env_v1\n",
    "from pettingzoo.utils import aec_to_parallel\n",
    "\n",
    "def env_creator(_):\n",
    "    env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "    env = aec_to_parallel(env)  # Convert to parallel API\n",
    "    env = pettingzoo_env_to_vec_env_v1(env)  # Convert to vec env\n",
    "    return env\n",
    "\n",
    "register_env(\"CentralizedPowerSystem\", env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pettingzoo.utils\n",
    "print(dir(pettingzoo.utils))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb321bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supersuit import pettingzoo_env_to_vec_env_v1\n",
    "\n",
    "def env_creator(_):\n",
    "    env = CentralizedPowerSystemEnv(dss_path, dss_file, irradiance_csv_file, load_profile_file)\n",
    "    env = env.parallel_env()  # Convert AEC environment to parallel API\n",
    "    env = pettingzoo_env_to_vec_env_v1(env)  # Convert to vec env\n",
    "    return env\n",
    "\n",
    "register_env(\"CentralizedPowerSystem\", env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, action):\n",
    "        #agent_idx = self.agent_name_to_agent_num[self.agent_selection]\n",
    "        #self._take_action_for_agent(agent_idx, action)\n",
    "        for agent_name, action in actions.items():\n",
    "            agent_idx = self.agent_name_to_agent_num[agent_name]\n",
    "            self._take_action_for_agent(agent_idx, action)\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "\n",
    "        all_bus_voltages = np.clip(self.controller.circuit_all_bus_vmag_pu(), 1e-10, 10.0)\n",
    "\n",
    "        reward = self.local_reward(agent_idx, all_bus_voltages)\n",
    "        self._cumulative_rewards[self.agent_selection] += reward\n",
    "        self.rewards[self.agent_selection] = reward\n",
    "\n",
    "        for agent in self.agents:\n",
    "            observation = self.observe(agent)\n",
    "            print(f\"Observation for {agent}:\", observation)\n",
    "\n",
    "        self.control_steps += 1\n",
    "        done = self.control_steps >= self.max_control_steps\n",
    "        self.terminations = {agent: done for agent in self.agents}  # Update termination status\n",
    "\n",
    "        # Set truncateds to False for simplicity\n",
    "        truncateds = {agent: False for agent in self.agents}\n",
    "        infos = {}  # You can add additional information here if needed\n",
    "\n",
    "        # Get the observation for the agent\n",
    "        observation = self.get_agent_observation(agent_idx, all_bus_voltages)\n",
    "\n",
    "        # Since it's AECEnv, we'll handle agent iteration within the environment\n",
    "        #self.agent_selection = self._agent_selector.next()\n",
    "\n",
    "        return observation, reward, self.terminations, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ebc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "\n",
    "from dss import DSS\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "TRANSFORMER_TAPS_ACTION_LOW = 0.9\n",
    "TRANSFORMER_TAPS_ACTION_HIGH = 1.1\n",
    "\n",
    "CAPACITOR_ACTION_LOW = 0\n",
    "CAPACITOR_ACTION_HIGH = 1\n",
    "\n",
    "class PowerSystemEnv(MultiAgentEnv):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file, load_profile_file):\n",
    "        super(PowerSystemEnv, self).__init__()\n",
    "        self.home_dir = os.getcwd()\n",
    "        self.dss_file = os.path.abspath(dss_file)\n",
    "        self.controller = DSS #DSSDLL(dss_path)\n",
    "        self.controller.Text.Command = f\"compile [{self.dss_file}]\"\n",
    "        self.dssCircuit = DSS.ActiveCircuit\n",
    "        self.AllNodeNum = len(self.dssCircuit.AllNodeNames)\n",
    "        self.ranked_buses = ['57', '54', '64', '63', '55', '62',\n",
    "                             '56', '65', '60', '52', '53', '61',\n",
    "                             '66', '67', '72', '86', '87', '76',\n",
    "                             '89', '97', '36', '40', '42', '43',\n",
    "                             '44', '47', '48','49','50','51']\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated = 100\n",
    "        self.previous_reward = 0.0\n",
    "        self.alpha = 0.1  #\n",
    "        # Apply actions to PV systems and batteries\n",
    "\n",
    "        if self.home_dir != os.getcwd():\n",
    "            os.chdir(self.home_dir)\n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "\n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)  # Skip the header\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "\n",
    "        self.load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "        ]\n",
    "\n",
    "        # Read CSV file into list\n",
    "        self.load_powers = pd.read_csv('.\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        self.load_dict = dict(zip(self.load_names, self.load_powers))\n",
    "\n",
    "        # Update the action and observation spaces for each agent\n",
    "\n",
    "        self.actionspace_dict = {}\n",
    "        self.actionlist = [13,7,10]     # the no of PV/BESs in each agent\n",
    "        self.agentno = len(self.actionlist)\n",
    "        self.actionspace_dict = {\n",
    "            f\"agent_{i}\": spaces.Box(\n",
    "                low=np.concatenate(([PV_KVAR_ACTION_LOW] * self.actionlist[i], [BESS_KW_ACTION_LOW] * self.actionlist[i])), # PV Q control\n",
    "                high=np.concatenate(([PV_KVAR_ACTION_HIGH] * self.actionlist[i], [BESS_KW_ACTION_HIGH] * self.actionlist[i])),  #BES P control\n",
    "                shape=(self.actionlist[i]*2,), dtype=np.float32) for i in range(self.agentno)\n",
    "        }\n",
    "\n",
    "\n",
    "        self.observation_space_dict = {}\n",
    "        for i in range(self.agentno):               # each agent has whole-system observation\n",
    "            self.observation_space_dict[f'agent_{i}'] = spaces.Box(low=0, high=2, shape=(self.AllNodeNum,), dtype=np.float32)\n",
    "\n",
    "        self.action_space = gym.spaces.Dict(self.actionspace_dict)\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space_dict)\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Initialize control step counter\n",
    "        self.control_steps = 0\n",
    "\n",
    "        # Maximum control steps allowed in one episode\n",
    "        self.max_control_steps = int(5)  # for example\n",
    "\n",
    "        # Define the agent IDs\n",
    "        self.agents = [f'agent_{i}' for i in range(self.agentno)]\n",
    "        self._agent_ids = [f'agent_{i}' for i in range(self.agentno)]\n",
    "\n",
    "    def _take_action(self, action_dict):\n",
    "        # Define total actions per agent\n",
    "\n",
    "        # Iterate through the action_dict\n",
    "        for agent_id, action in action_dict.items():\n",
    "            agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "            # Determine action segments for this agent\n",
    "            #pv_kw_actions = action[start_idx:start_idx + 6]\n",
    "            #pv_kvar_actions = action[start_idx + 6:start_idx + 12]\n",
    "            #bess_kw_actions = action[start_idx + 12:start_idx + 18]\n",
    "            # only Q control for PV\n",
    "            pv_kvar_actions = action[0:self.actionlist[agent_idx]]\n",
    "            bess_kw_actions = action[self.actionlist[agent_idx]:self.actionlist[agent_idx] + self.actionlist[agent_idx]]\n",
    "            # Index offset based on agent_idx\n",
    "            idx_offset = agent_idx * self.actionlist[agent_idx]\n",
    "\n",
    "            # Actions for PVs\n",
    "            # Q control, P will be curtailed to provide Q for Voltage control\n",
    "            self.dssCircuit.SetActiveClass('PVSystem')\n",
    "            for idx,kvar_action_value in enumerate(pv_kvar_actions):\n",
    "                # handle kW actions\n",
    "                irradiance = self.irradiance_profile[(self.current_step) % 8760]\n",
    "                # handle kVAR actions\n",
    "                pv_kvar = kvar_action_value\n",
    "                S_max = self.KWrated  # Maximum apparent power (example value)\n",
    "                p_max1 = np.sqrt(S_max ** 2 - np.power(pv_kvar, 2))\n",
    "                pv_kw = np.clip(irradiance * self.KWrated, 0, p_max1)\n",
    "                self.dssCircuit.SetActiveElement(f'PV{idx + idx_offset + 1}')\n",
    "                self.dssCircuit.ActiveElement.Properties('kvar').Val = pv_kvar\n",
    "                self.dssCircuit.ActiveElement.Properties('Pmpp').Val = pv_kw\n",
    "#                self.controller.Text.Command =f\"edit PVSystem.PV{idx + idx_offset + 1} phases=3 kV=4.16 Pmpp={scaled_pv_kw} kVAR={pv_kvar}\"\n",
    "            self.dssCircuit.SetActiveClass('Storage')\n",
    "            for idx, action_value in enumerate(bess_kw_actions):\n",
    "                self.dssCircuit.SetActiveElement(f'Battery{idx + idx_offset + 1}')\n",
    "                self.dssCircuit.ActiveElement.Properties('kVAR').Val = 0\n",
    "                self.dssCircuit.ActiveElement.Properties('kW').Val = action_value\n",
    "                #self.controller.Text.Command =f\"edit Storage.Battery{idx + idx_offset + 1} phases=3 kV=4.16 kW={action_value} kVAR=0\"\n",
    "\n",
    "            # Solve the power flow\n",
    "        self.controller.Text.Command = \"set controlmode=off\"\n",
    "        self.controller.Text.Command = \"solve\"\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # Execute the action\n",
    "        self._take_action(action_dict)\n",
    "\n",
    "        # Calculate the rewards\n",
    "        losses = sum(self.dssCircuit.AllElementLosses)\n",
    "        all_bus_voltages = self.dssCircuit.AllBusVmagPu\n",
    "        all_bus_voltages_modified = all_bus_voltages[all_bus_voltages>0.5]      ## filter out open-node voltages\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages_modified if v <= 0.95 or v >= 1.05)/len(all_bus_voltages_modified)\n",
    "\n",
    "        # Calculate the sum of squared voltage deviations from 1. This penalizes larger deviations more heavily.\n",
    "        #voltage_deviations = sum((v - 1) ** 2 for v in all_bus_voltages)\n",
    "\n",
    "        # Define penalty weights for different components\n",
    "        #w_deviation = 1.0\n",
    "        #w_violation = 5.0\n",
    "        #w_loss = 0.001\n",
    "\n",
    "        # Combine losses, violations, and deviations in the reward\n",
    "        # reward = - w_deviation * voltage_deviations - w_violation * voltage_violations\n",
    "        reward = - voltage_violations\n",
    "        # Set the reward for all agents\n",
    "        rewards = {agent_id: reward for agent_id in self._agent_ids}\n",
    "\n",
    "        # Gather observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "        # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        # Set termination status for all agents\n",
    "        terminations = {agent_id: (self.control_steps >= self.max_control_steps) or (voltage_violations == 0) for\n",
    "                        agent_id in self._agent_ids}\n",
    "        terminations['__all__'] = any(terminations.values())\n",
    "\n",
    "        # Assuming no truncation; modify as needed\n",
    "        truncateds = {agent_id: False for agent_id in self._agent_ids}\n",
    "        truncateds['__all__'] = any(truncateds.values())\n",
    "\n",
    "        # Gather additional information if required\n",
    "        infos = {agent_id: {} for agent_id in self._agent_ids}\n",
    "\n",
    "\n",
    "\n",
    "        return observations, rewards, terminations, truncateds, infos\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # You can use options to customize the reset function if needed\n",
    "        # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "        # Reset power system to initial state\n",
    "        self.controller.Text.Command = f\"compile [{self.dss_file}]\"\n",
    "\n",
    "        if self.home_dir != os.getcwd():\n",
    "            os.chdir(self.home_dir)\n",
    "\n",
    "        # self.current_step = int(np.clip(np.random.normal(loc=0, scale=8760), 0, 8760))\n",
    "        self.current_step = np.random.randint(0, 8761)\n",
    "\n",
    "        # assuming the profile has 8760 hours  #use gaussian distribution\n",
    "        self.control_steps = 0\n",
    "\n",
    "        # Load names\n",
    "\n",
    "\n",
    "        # Generate load scales using a Gaussian distribution\n",
    "        #load_scales = np.random.normal(loc=self.load_profile[self.current_step % 8760], scale=0.3, size=len(self.load_names))\n",
    "        load_scales = np.random.uniform(low=0.4,high=1.0, size=len(self.load_names))\n",
    "        self.dssCircuit.SetActiveClass('load')\n",
    "        for load_name, load_scale in zip(self.load_names, load_scales):\n",
    "            # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = self.load_dict[load_name]\n",
    "            # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "            # Use the result in your controller\n",
    "            self.dssCircuit.SetActiveElement(load_name)\n",
    "            self.dssCircuit.ActiveElement.Properties('kW').Val = result\n",
    "            #self.controller.Text.Command = f\"edit Load.{load_name} kW={result}\"\n",
    "\n",
    "#        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            # kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 8760]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 8760] * 10  # Scale Battery kW by load\n",
    "            self.dssCircuit.SetActiveClass('PVSystem')\n",
    "            self.dssCircuit.SetActiveElement(f'PV{i + 1}')\n",
    "            self.dssCircuit.ActiveElement.Properties('kvar').Val = pv_kvar\n",
    "            self.dssCircuit.ActiveElement.Properties('Pmpp').Val = pv_kw\n",
    "            self.dssCircuit.SetActiveClass('Storage')\n",
    "            self.dssCircuit.SetActiveElement(f'Battery{i + 1}')\n",
    "            self.dssCircuit.ActiveElement.Properties('kVAR').Val = 0\n",
    "            self.dssCircuit.ActiveElement.Properties('kW').Val = battery_kw\n",
    "\n",
    "            ##self.controller.Text.Command = f\"edit PVSystem.PV{i + 1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\"\n",
    "            #self.controller.Text.Command =  f\"edit Storage.Battery{i + 1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\"  # select charging or discharging\n",
    "            # make a comparison study beween different modes\n",
    "            # investigate batteries charging/dicharging pattern\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.Text.Command = \"set controlmode=off\"\n",
    "        self.controller.Text.Command = \"solve\"\n",
    "\n",
    "        # Get the observations for all agents\n",
    "        observations = {agent_id: self.get_observation(agent_id) for agent_id in self._agent_ids}\n",
    "\n",
    "        # You can include any additional information here. If there's nothing, just return an empty dictionary.\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self, agent_id):\n",
    "        # Get the voltage at all buses\n",
    "        all_bus_voltages = self.dssCircuit.AllBusVmagPu\n",
    "        # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "\n",
    "        # Extract agent index from the agent_id\n",
    "        agent_idx = int(agent_id.split('_')[1])\n",
    "\n",
    "        # Determine the start and end index for the observation chunk for this agent\n",
    "        if agent_idx < 4:\n",
    "            start_idx = agent_idx * 50\n",
    "            end_idx = start_idx + 50\n",
    "        else:  # This is agent_4\n",
    "            start_idx = 200\n",
    "            end_idx = 278  # or you could use end_idx = len(all_bus_voltages) if the size is not guaranteed to be 278\n",
    "\n",
    "        # Extract the observation chunk for this agent\n",
    "        agent_observation = all_bus_voltages#[start_idx:end_idx]\n",
    "\n",
    "        return agent_observation\n",
    "\n",
    "    def get_agent_observation(self):\n",
    "        agent_obs = {}\n",
    "        for agent_id in self.agents:\n",
    "            agent_obs[agent_id] = self.get_observation()\n",
    "        return agent_obs\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # perform a simple test for the environment here\n",
    "    dss_file = '.\\IEEE123Master.dss'\n",
    "    irradiance_csv_file = \".\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "    load_profile_file = \".\\LoadShape1.csv\"\n",
    "\n",
    "    env = PowerSystemEnv(\n",
    "        dss_path= r\" \",\n",
    "        dss_file= dss_file,\n",
    "        irradiance_csv_file=irradiance_csv_file,\n",
    "        load_profile_file=load_profile_file)\n",
    "    obs,_ = env.reset()\n",
    "    print(obs)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    #print('action: ', action)\n",
    "    obs, rewards, terminations, truncateds, infos = env.step(action)\n",
    "    print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087bd680",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show dss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11025c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Testing with partial observability of voltage violatiosn for each agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Tuple, Dict\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.sac import SAC\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.tune.registry import register_env\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "from py_dss_interface import DSSDLL\n",
    "#import stable_baselines3\n",
    "#from stable_baselines3 import SAC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from stable_baselines3 import A2C, DQN, PPO, TD3, SAC\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize OpenDSS\n",
    "dss = DSSDLL(r\"C:\\Program Files\\OpenDSS\")\n",
    "dss_file = r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\"\n",
    "dss.text(f\"compile [{dss_file}]\")\n",
    "\n",
    "# defining lengths for each segment\n",
    "PV_KVAR_ACTION_LEN = 30\n",
    "PV_KW_ACTION_LEN = 30\n",
    "BESS_KW_ACTION_LEN = 30\n",
    "TRANSFORMER_TAPS_ACTION_LEN = 1\n",
    "CAPACITOR_ACTION_LEN = 4\n",
    "\n",
    "# defining values for each segment\n",
    "PV_KVAR_ACTION_LOW = -80\n",
    "PV_KVAR_ACTION_HIGH = 80\n",
    "\n",
    "PV_KW_ACTION_LOW = 0\n",
    "PV_KW_ACTION_HIGH = 100\n",
    "\n",
    "BESS_KW_ACTION_LOW = 0\n",
    "BESS_KW_ACTION_HIGH = 100\n",
    "\n",
    "TRANSFORMER_TAPS_ACTION_LOW = 0.9\n",
    "TRANSFORMER_TAPS_ACTION_HIGH = 1.1\n",
    "\n",
    "CAPACITOR_ACTION_LOW = 0\n",
    "CAPACITOR_ACTION_HIGH = 1\n",
    "\n",
    "\n",
    "class PowerSystemEnv(gym.Env):\n",
    "    def __init__(self, dss_path, dss_file, irradiance_csv_file,load_profile_file):\n",
    "        super(PowerSystemEnv, self).__init__()\n",
    "        self.controller = DSSDLL(dss_path)\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        self.ranked_buses = [\n",
    "                                '1', '7', '8', '13', '21', '23', '29', '250', '35', '40', '42', '55',\n",
    "                                '56', '65', '76', '78', '66', '79', '81', '83', '91', '95', '100',\n",
    "                                '197', '300', '110', '135', '160', '152', '610'\n",
    "                            ]\n",
    "        self.capacitor_names = [\"C83\", \"C88a\", \"C90b\", \"C92c\"]\n",
    "        self.KWrated=100\n",
    "        \n",
    "        # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            self.controller.text(f\"new PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR=0 KVA=100 Pmpp=80\")\n",
    "            self.controller.text(f\"new Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW=100 kVAR=0\")\n",
    "        \n",
    "        with open(irradiance_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            self.irradiance_profile = [float(row[0]) for row in reader]\n",
    "    \n",
    "        with open(load_profile_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)  # Skip the header\n",
    "            self.load_profile = [float(row[0]) for row in reader]\n",
    "            \n",
    "        \n",
    "\n",
    "        # Define action and observation space\n",
    "        # Actions are continuous values for pv_kvar, pv_kW, battery_kw, transformer_tap and capacitor_states\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array(\n",
    "                [PV_KVAR_ACTION_LOW]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_LOW]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_LOW]*BESS_KW_ACTION_LEN +\n",
    "                [TRANSFORMER_TAPS_ACTION_LOW]*TRANSFORMER_TAPS_ACTION_LEN +\n",
    "                [CAPACITOR_ACTION_LOW]*CAPACITOR_ACTION_LEN\n",
    "            ),\n",
    "            high=np.array(\n",
    "                [PV_KVAR_ACTION_HIGH]*PV_KVAR_ACTION_LEN +\n",
    "                [PV_KW_ACTION_HIGH]*PV_KW_ACTION_LEN +\n",
    "                [BESS_KW_ACTION_HIGH]*BESS_KW_ACTION_LEN +\n",
    "                [TRANSFORMER_TAPS_ACTION_HIGH]*TRANSFORMER_TAPS_ACTION_LEN +\n",
    "                [CAPACITOR_ACTION_HIGH]*CAPACITOR_ACTION_LEN\n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # Assuming observation space is the voltage at each bus\n",
    "        self.observation_space = spaces.Box(low=0, high=100000, shape=(278,), dtype=np.float32)  # Modified shape\n",
    "        \n",
    "        self.current_step = 0\n",
    "\n",
    "        \n",
    "         # Initialize control step counter\n",
    "        self.control_steps = 0\n",
    "\n",
    "        # Maximum control steps allowed in one episode\n",
    "        self.max_control_steps =10  # for example\n",
    "        self.hourly_violations_count = []\n",
    "\n",
    "\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        \n",
    "        pv_kvar = action[:30].copy()\n",
    "        pv_kw = action[30:60].copy()\n",
    "        battery_kw = action[60:90].copy()\n",
    "        transformer_tap = action[90].copy()\n",
    "        capacitor_states = action[91:].copy()\n",
    "        capacitor_states = np.round(capacitor_states)\n",
    "\n",
    "        \n",
    "        \n",
    "        pv_kw_limits = [0, 100]  # Limits for PV kW (example values)\n",
    "        q_max1_limits = [-80, 80]  # Limits for maximum reactive power (example values)\n",
    "    \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        for z in range (0,30):\n",
    "            if  pv_kw[z] <= irradiance* self.KWrated:\n",
    "            # no clipping\n",
    "                pv_kw[z] = pv_kw[z]\n",
    "                #pv_kvar[z]  = np.sqrt(100**2 - np.power(pv_kw[z], 2))\n",
    "            else:\n",
    "                pv_kw[z] = irradiance* self.KWrated\n",
    "     \n",
    "        #pv_kw = np.clip(action[30:60], pv_kw_limits[0], pv_kw_limits[1])\n",
    "    \n",
    "        S_max = 100  # Maximum apparent power (example value)\n",
    "\n",
    "        q_max1 = np.sqrt(S_max**2 - np.power(pv_kw, 2))\n",
    "        \n",
    "        #pv_kvar= 0.8 ** pv_kw\n",
    "        #add checking condition on action space, or you can use PU. system or use clip action (make a constrined actio space)\n",
    "           \n",
    "    # Make pv_kvar controlled based on q_max1\n",
    "        pv_kvar = np.clip(pv_kvar, -q_max1, q_max1)\n",
    "        \n",
    "         \n",
    "        \n",
    "       \n",
    "        # Apply actions to PV systems and batteries\n",
    "        for i in range(30):\n",
    "            bus = self.ranked_buses[i]\n",
    "            scaled_pv_kw = pv_kw[i] \n",
    "            #* irradiance\n",
    "          \n",
    "            \n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar[i]} Pmpp={scaled_pv_kw}\")\n",
    "            # need to add KVA in OpenDSS model\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw[i]} kVAR=0\")\n",
    "\n",
    "        # Apply transformer control\n",
    "        self.controller.text(f\"edit Transformer.reg1a taps={transformer_tap}\")\n",
    "\n",
    "        # Apply capacitor control\n",
    "        for i, cap_name in enumerate(self.capacitor_names):\n",
    "            self.controller.text(f\"edit Capacitor.{cap_name} states={int(capacitor_states[i])}\")\n",
    "        #add control mode = off\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\") \n",
    "        #print(f'dss.circuit_losses(): {dss.circuit_losses()}')\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "    # Execute the action\n",
    "        self._take_action(action)\n",
    "\n",
    "    # Calculate the rewards\n",
    "        losses = sum(self.controller.circuit_losses())\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "        voltage_violations = sum(1 for v in all_bus_voltages if v <= 0.95 or v >= 1.05 and v != 0)\n",
    "\n",
    "    # Calculate the sum of squared voltage deviations from 1. This penalizes larger deviations more heavily.\n",
    "        voltage_deviations = sum((v - 1)**2 for v in all_bus_voltages)\n",
    "\n",
    "    # Define penalty weights for different components\n",
    "        w_deviation = 1.0\n",
    "        w_violation = 5.0\n",
    "        w_loss = 0.001\n",
    "\n",
    "    # Combine losses, violations, and deviations in the reward\n",
    "        reward = -voltage_violations \n",
    "        self.hourly_violations_count.append(voltage_violations)\n",
    "        print(f\"violations count: {self.hourly_violations_count}\")\n",
    "\n",
    "\n",
    "    # Gather observations\n",
    "        observation = self.get_observation()\n",
    "\n",
    "        termination = (self.control_steps >= self.max_control_steps) or (voltage_violations == 0)\n",
    "\n",
    "    # After taking action, increment control steps\n",
    "        self.control_steps += 1\n",
    "\n",
    "        return observation, reward, termination, False, {}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "    # If a seed is provided, set the random seed for numpy\n",
    "        if seed is not None:\n",
    "            seed = 1 \n",
    "            np.random.seed(seed)\n",
    "\n",
    "    # You can use options to customize the reset function if needed\n",
    "    # For now, we'll just print the options\n",
    "        if options is not None:\n",
    "            print(f\"Reset options: {options}\")\n",
    "        # Reset power system to initial state\n",
    "        self.controller.text(f\"compile [{dss_file}]\")\n",
    "        \n",
    "        #self.current_step = int(np.clip(np.random.normal(loc=0, scale=8760), 0, 8760))\n",
    "        self.current_step = np.random.randint(0, 8761)\n",
    "       \n",
    "\n",
    " # assuming the profile has 8760 hours  #use gaussian distribution\n",
    "        self.control_steps = 0\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "         # Load names\n",
    "        load_names = [\n",
    "            \"S1a\", \"S2b\", \"S4c\", \"S5c\", \"S6c\", \"S7a\", \"S9a\", \"S10a\", \"S11a\", \"S12b\",\n",
    "            \"S16c\", \"S17c\", \"S19a\", \"S20a\", \"S22b\", \"S24c\", \"S28a\", \"S29a\", \"S30c\", \"S31c\",\n",
    "            \"S32c\", \"S33a\", \"S34c\", \"S35a\", \"S37a\", \"S38b\", \"S39b\", \"S41c\", \"S42a\", \"S43b\",\n",
    "            \"S45a\", \"S46a\", \"S47\", \"S48\", \"S49a\", \"S49b\", \"S49c\", \"S50c\", \"S51a\", \"S52a\",\n",
    "            \"S53a\", \"S55a\", \"S56b\", \"S58b\", \"S59b\", \"S60a\", \"S62c\", \"S63a\", \"S64b\", \"S65a\",\n",
    "            \"S65b\", \"S65c\", \"S66c\", \"S68a\", \"S69a\", \"S70a\", \"S71a\", \"S73c\", \"S74c\", \"S75c\",\n",
    "            \"S76a\", \"S76b\", \"S76c\", \"S77b\", \"S79a\", \"S80b\", \"S82a\", \"S83c\", \"S84c\", \"S85c\",\n",
    "            \"S86b\"\n",
    "            ]\n",
    "  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Read CSV file into list\n",
    "        #load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).tolist()\n",
    "        # Convert the first column to a list\n",
    "        load_powers = pd.read_csv('D:\\Alaa_Selim\\Loadpowers.csv', header=None).iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "        # Create a dictionary with load names as keys and load powers as values\n",
    "        load_dict = dict(zip(load_names, load_powers))\n",
    "\n",
    "# Generate load scales using a Gaussian distribution\n",
    "        load_scales = np.random.normal(loc=self.load_profile[self.current_step % 8760], scale=0.001, size=len(load_names))\n",
    "\n",
    "        for load_name, load_scale in zip(load_names, load_scales):\n",
    "    # Get the load power corresponding to load_name from the dictionary\n",
    "            load_power = load_dict[load_name]\n",
    "    \n",
    "    # Multiply load power by load_scale\n",
    "            result = load_power * load_scale\n",
    "    \n",
    "    # Use the result in your controller\n",
    "            self.controller.text(f\"edit Load.{load_name} kW={result}\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # Load the irradiance for the current hour\n",
    "        \n",
    "        \n",
    "        irradiance = self.irradiance_profile[self.current_step % 8760]\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        # Initialize random states for controlled devices\n",
    "        for i, bus in enumerate(self.ranked_buses):\n",
    "            # Randomly initialize PV systems and batteries\n",
    "            #kvar should be set to very small\n",
    "            fixed_power_factor = 0.9  # Set a fixed power factor value (e.g., 0.9)\n",
    "            pv_kw = 100 * self.irradiance_profile[self.current_step % 8760]  # Scale PV kW by irradiance\n",
    "            power_factor_angle = np.arccos(fixed_power_factor)  # Calculate the angle corresponding to the power factor\n",
    "            pv_kvar = pv_kw * np.tan(power_factor_angle)  # Calculate reactive power (kVAR) based on kW and power factor\n",
    "            battery_kw = self.load_profile[self.current_step % 8760]*100  # Scale Battery kW by load\n",
    "            self.controller.text(f\"edit PVSystem.PV{i+1} phases=3 bus1={bus} kV=4.16 kVAR={pv_kvar} Pmpp={pv_kw}\")\n",
    "            self.controller.text(f\"edit Storage.Battery{i+1} phases=3 bus1={bus} kV=4.16 kW={battery_kw} kVAR=0\") #select charging or discharging\n",
    "            #make a comparison study beween different modes\n",
    "            #investigate batteries charging/dicharging pattern\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solve the power flow\n",
    "        self.controller.text(\"set controlmode=off\")\n",
    "        self.controller.text(\"solve\")\n",
    "        \n",
    "        # Get the observations for all agents\n",
    "        observations = self.get_observation()\n",
    "\n",
    "    # You can include any additional information here. If there's nothing, just return an empty dictionary.\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def get_observation(self):\n",
    "    # Get the voltage at all buses\n",
    "        all_bus_voltages = self.controller.circuit_all_bus_vmag_pu()\n",
    "    \n",
    "    # Flatten the list of bus voltages\n",
    "        all_bus_voltages = np.array(all_bus_voltages).flatten()\n",
    "    \n",
    "    # As there's only one agent, you should decide the observation length. \n",
    "    # If you'd like to keep the previous structure, the agent might observe the first 50, or all of them.\n",
    "    # Here, I'm assuming the agent observes all of them:\n",
    "        agent_observation = all_bus_voltages\n",
    "\n",
    "        return agent_observation\n",
    "\n",
    "    def _end_of_episode(self):\n",
    "        return self.current_step >= len(self.irradiance_profile)\n",
    "\n",
    "    \n",
    "\n",
    "irradiance_csv_file = r\"D:\\Alaa_Selim\\Irradiance_Profile_Santa_Clara.csv\"\n",
    "load_profile_file = r\"D:\\Alaa_Selim\\LoadShape1.csv\"\n",
    "\n",
    "    \n",
    "    \n",
    "original_env = PowerSystemEnv(\n",
    "    dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "    dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "    irradiance_csv_file=irradiance_csv_file,\n",
    "    load_profile_file=load_profile_file\n",
    ")\n",
    "\n",
    "\n",
    "#from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "#env = MultiAgentEnvCompatibility(original_env)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PowerSystemEnv(\n",
    "                            dss_path=r\"C:\\Program Files\\OpenDSS\",\n",
    "                            dss_file=r\"D:\\Alaa_Selim\\123Bus\\IEEE123Master.dss\",\n",
    "                            irradiance_csv_file=irradiance_csv_file,\n",
    "                            load_profile_file=load_profile_file\n",
    "                            )\n",
    "\n",
    "register_env(\"PowerSystemEnv\", env_creator)\n",
    "\n",
    "\n",
    "\n",
    "# Suppose you have N agents\n",
    "N = 5  # adjust this as per your actual number of agents\n",
    "\n",
    "# Only one policy for the single agent\n",
    "#policies = {\n",
    "  #  \"single_policy\": (None, \n",
    "        #              original_env.observation_space, \n",
    "           #           original_env.action_space, \n",
    "                #      {})\n",
    "#}#\n",
    "\n",
    "#def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "  #  return \"single_policy\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb8945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f5e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
